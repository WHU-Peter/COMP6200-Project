{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg_model_benchmark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEpws0CVlcHHIEyMJRyIIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHU-Peter/COMP6200-Project/blob/main/vgg_model_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZduTHahSHt",
        "outputId": "e9173339-006d-49a3-9db8-09e38b866708"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KuXXQLqyGR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8LYseck5ixy",
        "outputId": "f09151bb-ff25-43a7-cb74-59bf7ddb65f3"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True)\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=1,\n",
        "                                         shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPLbo3Pn1Z4s"
      },
      "source": [
        "# baseline_model = models.vgg16(pretrained=True).to(device)  # 使用VGG16 网络预训练好的模型\n",
        "\n",
        "# epoch = 30  #Training times\n",
        "# learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(baseline_model.parameters(), lr=learning_rate)\n",
        "# Loss_Baseline_Model = []\n",
        "\n",
        "# print('Baseline_Model Started Training')\n",
        "\n",
        "# for epoch in range(epoch):    #Iteration\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         inputs, labels = data\n",
        "#                  #Initialize gradient\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = baseline_model(inputs.to(device))\n",
        "#         loss = criterion(outputs, labels.to(device))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Print loss\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "#             print('[%d, %5d] loss: %.5f' %\n",
        "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
        "#             Loss_Baseline_Model.append(running_loss)\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print('Baseline_Model Finished Training')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb6nXBKFU8FH"
      },
      "source": [
        "# correct = 0\n",
        "# total = 0\n",
        "# baseline_model.eval()\n",
        "# for data in testloader:\n",
        "#     images, labels = data\n",
        "#     images, labels = images.to(device), labels.to(device)\n",
        "#     outputs = baseline_model(images)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
        "#     100 * correct / total))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU-gOa4fcak4",
        "outputId": "eebb0a1d-56e5-4893-c4a1-e1e943e5edc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build the model \n",
        "baseline_model = models.vgg16(pretrained=True).to(device)  # 使用VGG16 网络预训练好的模型\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(baseline_model.parameters(), lr=learning_rate)\n",
        "\n",
        "def baseline_model_train(inputs, labels):\n",
        "  baseline_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = baseline_model(inputs)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def baseline_model_inference(inputs):\n",
        "  baseline_model.eval()\n",
        "  outputs = baseline_model(inputs)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_baseline_train = timeit.Timer(\n",
        "    stmt='baseline_model_train(x, y)', \n",
        "    setup='from __main__ import baseline_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_baseline_inference = timeit.Timer(\n",
        "    stmt='baseline_model_inference(x)', \n",
        "    setup='from __main__ import baseline_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'baseline_model_train(x, y):  {t_baseline_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'baseline_model_inference(x):  {t_baseline_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "baseline_model_train(x, y):  1905.3 ms\n",
            "baseline_model_inference(x):   72.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GNeu-VqX-V"
      },
      "source": [
        "def softmax_temperature(logits, temperature):\n",
        "    pro = F.softmax(logits / temperature, dim=-1)\n",
        "    return pro;"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkWst_c2qbnq"
      },
      "source": [
        "class LearnableLookUpTable(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, init_temperature: float):\n",
        "        super(LearnableLookUpTable, self).__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.temperature = nn.Parameter(torch.tensor(init_temperature, requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # if self.training:\n",
        "          x = softmax_temperature(x, self.temperature)\n",
        "          return x @ self.emb.weight\n",
        "        # else:\n",
        "        #   x = softmax_temperature(x, 0.00001)\n",
        "        #   x = mapping_onehot_vector(x)\n",
        "        #   return self.emb(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZV7QMU0gg5_"
      },
      "source": [
        "class Learnable_VGG_LookUpTable(nn.Module):\n",
        "  def __init__(self, originalModel, init_temperature):\n",
        "    super(Learnable_VGG_LookUpTable, self).__init__()\n",
        "    # self.features = originalModel.features\n",
        "    # self.avgpool = originalModel.avgpool\n",
        "    self.vgg = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "    # self.classifier = originalModel.classifier\n",
        "    self.look_up_table = LearnableLookUpTable(25088, 4096, init_temperature)\n",
        "    self.classifier = nn.Sequential(*list(originalModel.classifier.children())[1:])\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # x = self.features(x)\n",
        "    # x = self.avgpool(x)\n",
        "    x = self.vgg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.look_up_table(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFiS0_T5DYdr"
      },
      "source": [
        "# pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "# for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "#     parma.requires_grad = True\n",
        "# learnable_lut_model = Learnable_VGG_LookUpTable(pre_model, 1.).to(device)\n",
        "\n",
        "# epoches = 30  #Training times\n",
        "# learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(learnable_lut_model.parameters(), lr=learning_rate)\n",
        "# Loss_Learnable_Model = []\n",
        "# Temperature = []\n",
        "\n",
        "# print('Learnable_lut_model Started Training')\n",
        "# for epoch in range(epoches):    #Iteration\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         inputs, labels = data\n",
        "#                  #Initialize gradient\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = learnable_lut_model(inputs.to(device))\n",
        "#         loss = criterion(outputs, labels.to(device))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # for parameter in optimizer.param_groups[0]['params']:\n",
        "#         #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "#         # Print loss\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "#             print('[%d, %5d] loss: %.5f' %\n",
        "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
        "#             Loss_Learnable_Model.append(running_loss)\n",
        "#             running_loss = 0.0\n",
        "\n",
        "#             for parameter in learnable_lut_model.named_parameters():\n",
        "#               if parameter[0] == 'look_up_table.temperature':\n",
        "#                 print('[%d, %5d] temperature: %.5f' %\n",
        "#                   (epoch + 1, i + 1, parameter[1].data.detach().cpu().numpy()))\n",
        "#                 Temperature.append(parameter[1].data.detach().cpu().numpy())\n",
        "\n",
        "# print('Learnable_lut_model Finished Training')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwrqQrHWVlYR"
      },
      "source": [
        "# correct = 0\n",
        "# total = 0\n",
        "# learnable_lut_model.eval()\n",
        "# for data in testloader:\n",
        "#     images, labels = data\n",
        "#     images, labels = images.to(device), labels.to(device)\n",
        "#     outputs = learnable_lut_model(images)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
        "#     100 * correct / total))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wD9RCrbeLH8",
        "outputId": "6b3e06d2-910d-477b-f17b-2e09777a1f9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build the model \n",
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "learnable_lut_model = Learnable_VGG_LookUpTable(pre_model, 1.).to(device)\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(learnable_lut_model.parameters(), lr=learning_rate)\n",
        "\n",
        "def learnable_model_train(inputs, labels):\n",
        "  learnable_lut_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = learnable_lut_model(inputs)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def learnable_model_inference(inputs):\n",
        "  learnable_lut_model.eval()\n",
        "  outputs = learnable_lut_model(inputs)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_learnable_train = timeit.Timer(\n",
        "    stmt='learnable_model_train(x, y)', \n",
        "    setup='from __main__ import learnable_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_learnable_inference = timeit.Timer(\n",
        "    stmt='learnable_model_inference(x)', \n",
        "    setup='from __main__ import learnable_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'learnable_model_train(x, y):  {t_learnable_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'learnable_model_inference(x):  {t_learnable_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learnable_model_train(x, y):  1908.4 ms\n",
            "learnable_model_inference(x):   66.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl7e1OTmrj0v"
      },
      "source": [
        "class AnnealingLookUpTable(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        super(AnnealingLookUpTable, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "    def forward(self, x, temperature):\n",
        "        if self.training:\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          return x @ self.emb.weight\n",
        "        else:\n",
        "          # x = softmax_temperature(x, 0.00001)\n",
        "          # x = mapping_onehot_vector(x)\n",
        "          # return self.emb(x)\n",
        "\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          nozero = torch.nonzero(x);\n",
        "          out = np.zeros((x.shape[0], self.embedding_dim))\n",
        "          out = torch.tensor(out).to(device)\n",
        "          # print(np.array(nozero).shape[1])\n",
        "          for i in range(x.shape[0]):\n",
        "            idx = torch.where(nozero[:,0]==i)[0]\n",
        "            rows = nozero[idx, 1].long()\n",
        "            out[i] = torch.mean(self.emb(rows), axis=0)\n",
        "          return out.float()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5R4HiLbrzG3"
      },
      "source": [
        "class Annealing_VGG_LookUpTable(nn.Module):\n",
        "  def __init__(self, originalModel):\n",
        "    super(Annealing_VGG_LookUpTable, self).__init__()\n",
        "    # self.features = originalModel.features\n",
        "    # self.avgpool = originalModel.avgpool\n",
        "    self.vgg = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "    # self.classifier = originalModel.classifier\n",
        "    self.look_up_table = AnnealingLookUpTable(25088, 4096)\n",
        "    self.classifier = nn.Sequential(*list(originalModel.classifier.children())[1:])\n",
        "    \n",
        "  def forward(self, x, temperature):\n",
        "    # x = self.features(x)\n",
        "    # x = self.avgpool(x)\n",
        "    x = self.vgg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.look_up_table(x, temperature)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM2zbdVmsBbj"
      },
      "source": [
        "# pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "# for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "#     parma.requires_grad = True\n",
        "# Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "# epoches = 30  #Training times\n",
        "# learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "# Loss_Annealing_Model = []\n",
        "# idx = 0\n",
        "\n",
        "# print('Loss_Annealing_Model Started Training')\n",
        "# for epoch in range(epoches):    #Iteration\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         idx = idx + 1\n",
        "#         inputs, labels = data\n",
        "#                  #Initialize gradient\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, math.exp(-1.4 * math.pow(10, -5) * idx)), device= device))\n",
        "#         loss = criterion(outputs, labels.to(device))\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # for parameter in optimizer.param_groups[0]['params']:\n",
        "#         #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "#         # Print loss\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "#             print('[%d, %5d] loss: %.5f' %\n",
        "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
        "#             Loss_Annealing_Model.append(running_loss)\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uym5WF5yV0Qu"
      },
      "source": [
        "# correct = 0\n",
        "# total = 0\n",
        "# Annealing_lut_model.eval()\n",
        "# for data in testloader:\n",
        "#     images, labels = data\n",
        "#     images, labels = images.to(device), labels.to(device)\n",
        "#     outputs = Annealing_lut_model(images, 0.0001)\n",
        "#     _, predicted = torch.max(outputs.data, 1)\n",
        "#     total += labels.size(0)\n",
        "#     correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print('Accuracy of the network on the 10000 test images: %.2f %%' % (\n",
        "#     100 * correct / total))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXHDxuwGe5zt",
        "outputId": "1b0ff8eb-c2e3-4553-e2ba-83af7e3d67d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build the model \n",
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "\n",
        "def annealing_model_train(inputs, labels):\n",
        "  Annealing_lut_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = Annealing_lut_model(inputs, 0.001)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def annealing_model_inference(inputs):\n",
        "  Annealing_lut_model.eval()\n",
        "  outputs = Annealing_lut_model(inputs, 0.001)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_annealing_train = timeit.Timer(\n",
        "    stmt='annealing_model_train(x, y)', \n",
        "    setup='from __main__ import annealing_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_annealing_inference = timeit.Timer(\n",
        "    stmt='annealing_model_inference(x)', \n",
        "    setup='from __main__ import annealing_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'annealing_model_train(x, y):  {t_annealing_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'annealing_model_inference(x):  {t_annealing_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "annealing_model_train(x, y):  1820.5 ms\n",
            "annealing_model_inference(x):   29.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}