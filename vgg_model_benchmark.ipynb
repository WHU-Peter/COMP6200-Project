{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg_model_benchmark.ipynb",
      "provenance": [],
      "mount_file_id": "https://gist.github.com/WHU-Peter/4f6e4daa3fe2f60b53d026ce3abd5e1b#file-project-vgg-ipynb",
      "authorship_tag": "ABX9TyPC/7TxZMR2+Ei+xmRtBxAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30054a0bbbd04187b1e04b967c061fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_73985e2ad8eb448691513b22acd39b3f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2869680b6d2340bcba29e91aba4f0adc",
              "IPY_MODEL_2d54dd66166748759b787ea7fd49f819",
              "IPY_MODEL_1ced282ddc1f42dfb948e9fe014bc084"
            ]
          }
        },
        "73985e2ad8eb448691513b22acd39b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2869680b6d2340bcba29e91aba4f0adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14e5008cebea43cdaacc3d6f65e7060b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8fc398321422486eb32be0f93810fa6a"
          }
        },
        "2d54dd66166748759b787ea7fd49f819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c04fc2727cb940ea8ca11b8a0e84b384",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_375d2386bce64c47b5a0b28e0b0b0fdf"
          }
        },
        "1ced282ddc1f42dfb948e9fe014bc084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0044d8d74ca34483b5eebffa2b327512",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:11&lt;00:00, 16693947.79it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51f646695bab4586a8f70a6b98ed3688"
          }
        },
        "14e5008cebea43cdaacc3d6f65e7060b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8fc398321422486eb32be0f93810fa6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c04fc2727cb940ea8ca11b8a0e84b384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "375d2386bce64c47b5a0b28e0b0b0fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0044d8d74ca34483b5eebffa2b327512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51f646695bab4586a8f70a6b98ed3688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHU-Peter/COMP6200-Project/blob/main/vgg_model_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZduTHahSHt",
        "outputId": "e9173339-006d-49a3-9db8-09e38b866708"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KuXXQLqyGR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import timeit\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "30054a0bbbd04187b1e04b967c061fdd",
            "73985e2ad8eb448691513b22acd39b3f",
            "2869680b6d2340bcba29e91aba4f0adc",
            "2d54dd66166748759b787ea7fd49f819",
            "1ced282ddc1f42dfb948e9fe014bc084",
            "14e5008cebea43cdaacc3d6f65e7060b",
            "8fc398321422486eb32be0f93810fa6a",
            "c04fc2727cb940ea8ca11b8a0e84b384",
            "375d2386bce64c47b5a0b28e0b0b0fdf",
            "0044d8d74ca34483b5eebffa2b327512",
            "51f646695bab4586a8f70a6b98ed3688"
          ]
        },
        "id": "g8LYseck5ixy",
        "outputId": "69c94b78-514b-4006-adca-8c46fb4b9f0a"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True)\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=1,\n",
        "                                         shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30054a0bbbd04187b1e04b967c061fdd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU-gOa4fcak4",
        "outputId": "eebb0a1d-56e5-4893-c4a1-e1e943e5edc8"
      },
      "source": [
        "# build the model \n",
        "baseline_model = models.vgg16(pretrained=True).to(device)  # 使用VGG16 网络预训练好的模型\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(baseline_model.parameters(), lr=learning_rate)\n",
        "\n",
        "def baseline_model_train(inputs, labels):\n",
        "  baseline_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = baseline_model(inputs)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def baseline_model_inference(inputs):\n",
        "  baseline_model.eval()\n",
        "  outputs = baseline_model(inputs)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_baseline_train = timeit.Timer(\n",
        "    stmt='baseline_model_train(x, y)', \n",
        "    setup='from __main__ import baseline_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_baseline_inference = timeit.Timer(\n",
        "    stmt='baseline_model_inference(x)', \n",
        "    setup='from __main__ import baseline_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'baseline_model_train(x, y):  {t_baseline_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'baseline_model_inference(x):  {t_baseline_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "baseline_model_train(x, y):  1905.3 ms\n",
            "baseline_model_inference(x):   72.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GNeu-VqX-V"
      },
      "source": [
        "def softmax_temperature(logits, temperature):\n",
        "    pro = F.softmax(logits / temperature, dim=-1)\n",
        "    return pro;"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkWst_c2qbnq"
      },
      "source": [
        "class LearnableLookUpTable(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, init_temperature: float):\n",
        "        super(LearnableLookUpTable, self).__init__()\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.temperature = nn.Parameter(torch.tensor(init_temperature, requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # if self.training:\n",
        "          x = softmax_temperature(x, self.temperature)\n",
        "          return x @ self.emb.weight\n",
        "        # else:\n",
        "        #   x = softmax_temperature(x, 0.00001)\n",
        "        #   x = mapping_onehot_vector(x)\n",
        "        #   return self.emb(x)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZV7QMU0gg5_"
      },
      "source": [
        "class Learnable_VGG_LookUpTable(nn.Module):\n",
        "  def __init__(self, originalModel, init_temperature):\n",
        "    super(Learnable_VGG_LookUpTable, self).__init__()\n",
        "    # self.features = originalModel.features\n",
        "    # self.avgpool = originalModel.avgpool\n",
        "    self.vgg = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "    # self.classifier = originalModel.classifier\n",
        "    self.look_up_table = LearnableLookUpTable(25088, 4096, init_temperature)\n",
        "    self.classifier = nn.Sequential(*list(originalModel.classifier.children())[1:])\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # x = self.features(x)\n",
        "    # x = self.avgpool(x)\n",
        "    x = self.vgg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.look_up_table(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wD9RCrbeLH8",
        "outputId": "c33f8812-c512-4aa4-f54a-0511ce1d5ed0"
      },
      "source": [
        "# build the model \n",
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "learnable_lut_model = Learnable_VGG_LookUpTable(pre_model, 1.).to(device)\n",
        "learnable_lut_model.load_state_dict(torch.load('/content/drive/MyDrive/learnable_lut_model.weights', map_location=torch.device(device)))\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(learnable_lut_model.parameters(), lr=learning_rate)\n",
        "\n",
        "def learnable_model_train(inputs, labels):\n",
        "  learnable_lut_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = learnable_lut_model(inputs)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def learnable_model_inference(inputs):\n",
        "  learnable_lut_model.eval()\n",
        "  outputs = learnable_lut_model(inputs)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_learnable_train = timeit.Timer(\n",
        "    stmt='learnable_model_train(x, y)', \n",
        "    setup='from __main__ import learnable_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_learnable_inference = timeit.Timer(\n",
        "    stmt='learnable_model_inference(x)', \n",
        "    setup='from __main__ import learnable_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'learnable_model_train(x, y):  {t_learnable_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'learnable_model_inference(x):  {t_learnable_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learnable_model_train(x, y):  2944.0 ms\n",
            "learnable_model_inference(x):   68.7 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl7e1OTmrj0v"
      },
      "source": [
        "class AnnealingLookUpTable(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        super(AnnealingLookUpTable, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "    def forward(self, x, temperature):\n",
        "        if self.training:\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          return x @ self.emb.weight\n",
        "        else:\n",
        "          # x = softmax_temperature(x, 0.00001)\n",
        "          # x = mapping_onehot_vector(x)\n",
        "          # return self.emb(x)\n",
        "\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          nozero = torch.nonzero(x);\n",
        "          out = np.zeros((x.shape[0], self.embedding_dim))\n",
        "          out = torch.tensor(out).to(device)\n",
        "          # print(np.array(nozero).shape[1])\n",
        "          for i in range(x.shape[0]):\n",
        "            idx = torch.where(nozero[:,0]==i)[0]\n",
        "            rows = nozero[idx, 1].long()\n",
        "            out[i] = torch.mean(self.emb(rows), axis=0)\n",
        "          return out.float()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5R4HiLbrzG3"
      },
      "source": [
        "class Annealing_VGG_LookUpTable(nn.Module):\n",
        "  def __init__(self, originalModel):\n",
        "    super(Annealing_VGG_LookUpTable, self).__init__()\n",
        "    # self.features = originalModel.features\n",
        "    # self.avgpool = originalModel.avgpool\n",
        "    self.vgg = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "    # self.classifier = originalModel.classifier\n",
        "    self.look_up_table = AnnealingLookUpTable(25088, 4096)\n",
        "    self.classifier = nn.Sequential(*list(originalModel.classifier.children())[1:])\n",
        "    \n",
        "  def forward(self, x, temperature):\n",
        "    # x = self.features(x)\n",
        "    # x = self.avgpool(x)\n",
        "    x = self.vgg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.look_up_table(x, temperature)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXHDxuwGe5zt",
        "outputId": "dffbd0dc-10ba-44de-c11a-b9ad31f4d857"
      },
      "source": [
        "# build the model \n",
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "Annealing_lut_model.load_state_dict(torch.load('/content/drive/MyDrive/Annealing_lut_model.weights', map_location=torch.device(device)))\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "\n",
        "def annealing_model_train(inputs, labels):\n",
        "  Annealing_lut_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimizer.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = Annealing_lut_model(inputs, 0.0001)\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "def annealing_model_inference(inputs):\n",
        "  Annealing_lut_model.eval()\n",
        "  outputs = Annealing_lut_model(inputs, 0.0001)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_annealing_train = timeit.Timer(\n",
        "    stmt='annealing_model_train(x, y)', \n",
        "    setup='from __main__ import annealing_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_annealing_inference = timeit.Timer(\n",
        "    stmt='annealing_model_inference(x)', \n",
        "    setup='from __main__ import annealing_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'annealing_model_train(x, y):  {t_annealing_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'annealing_model_inference(x):  {t_annealing_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "annealing_model_train(x, y):  1920.9 ms\n",
            "annealing_model_inference(x):   31.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}