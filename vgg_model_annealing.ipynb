{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vgg_model_annealing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQwyDd1q85UP37lcIB3FWZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHU-Peter/COMP6200-Project/blob/main/vgg_model_annealing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZduTHahSHt",
        "outputId": "a5115e5f-3c25-4e8f-9555-7398d15dacc0"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug 21 07:07:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    54W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KuXXQLqyGR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8LYseck5ixy",
        "outputId": "d9be9a01-a320-4bf2-ac1e-e87de2a238b9"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True)\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GNeu-VqX-V"
      },
      "source": [
        "def softmax_temperature(logits, temperature):\n",
        "    pro = F.softmax(logits / temperature, dim=-1)\n",
        "    return pro;"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkWst_c2qbnq"
      },
      "source": [
        "class AnnealingLookUpTable(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        super(AnnealingLookUpTable, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "    def forward(self, x, temperature):\n",
        "        if self.training:\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          return x @ self.emb.weight\n",
        "        else:\n",
        "          # x = softmax_temperature(x, 0.00001)\n",
        "          # x = mapping_onehot_vector(x)\n",
        "          # return self.emb(x)\n",
        "\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          nozero = torch.nonzero(x);\n",
        "          out = np.zeros((x.shape[0], self.embedding_dim))\n",
        "          out = torch.tensor(out).to(device)\n",
        "          # print(np.array(nozero).shape[1])\n",
        "          for i in range(x.shape[0]):\n",
        "            idx = torch.where(nozero[:,0]==i)[0]\n",
        "            rows = nozero[idx, 1].long()\n",
        "            out[i] = torch.mean(self.emb(rows), axis=0)\n",
        "          return out.float()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZV7QMU0gg5_"
      },
      "source": [
        "class Annealing_VGG_LookUpTable(nn.Module):\n",
        "  def __init__(self, originalModel):\n",
        "    super(Annealing_VGG_LookUpTable, self).__init__()\n",
        "    # self.features = originalModel.features\n",
        "    # self.avgpool = originalModel.avgpool\n",
        "    self.vgg = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "    # self.classifier = originalModel.classifier\n",
        "    self.look_up_table = AnnealingLookUpTable(25088, 4096)\n",
        "    self.classifier = nn.Sequential(*list(originalModel.classifier.children())[1:])\n",
        "    \n",
        "  def forward(self, x, temperature):\n",
        "    # x = self.features(x)\n",
        "    # x = self.avgpool(x)\n",
        "    x = self.vgg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.look_up_table(x, temperature)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vnrDliORL8T",
        "outputId": "8761a235-1833-4bb1-88ef-420a20586775"
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_exp = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, math.exp(-1.4 * math.pow(10, -5) * idx)), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            Loss_Annealing_Model_exp.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 3.14771\n",
            "[1,  4000] loss: 2.34918\n",
            "[1,  6000] loss: 2.34055\n",
            "[1,  8000] loss: 2.33426\n",
            "[1, 10000] loss: 2.33270\n",
            "[1, 12000] loss: 2.32458\n",
            "[2,  2000] loss: 2.32285\n",
            "[2,  4000] loss: 2.32143\n",
            "[2,  6000] loss: 2.32133\n",
            "[2,  8000] loss: 2.31889\n",
            "[2, 10000] loss: 2.31556\n",
            "[2, 12000] loss: 2.31515\n",
            "[3,  2000] loss: 2.31481\n",
            "[3,  4000] loss: 2.31354\n",
            "[3,  6000] loss: 2.31333\n",
            "[3,  8000] loss: 2.31175\n",
            "[3, 10000] loss: 2.31311\n",
            "[3, 12000] loss: 2.31000\n",
            "[4,  2000] loss: 2.30645\n",
            "[4,  4000] loss: 2.31018\n",
            "[4,  6000] loss: 2.30920\n",
            "[4,  8000] loss: 2.30883\n",
            "[4, 10000] loss: 2.30924\n",
            "[4, 12000] loss: 2.30744\n",
            "[5,  2000] loss: 2.30837\n",
            "[5,  4000] loss: 2.30736\n",
            "[5,  6000] loss: 2.17031\n",
            "[5,  8000] loss: 1.70402\n",
            "[5, 10000] loss: 1.43958\n",
            "[5, 12000] loss: 1.20283\n",
            "[6,  2000] loss: 1.06267\n",
            "[6,  4000] loss: 1.00648\n",
            "[6,  6000] loss: 0.93727\n",
            "[6,  8000] loss: 0.85255\n",
            "[6, 10000] loss: 0.73028\n",
            "[6, 12000] loss: 0.65753\n",
            "[7,  2000] loss: 0.49543\n",
            "[7,  4000] loss: 0.48776\n",
            "[7,  6000] loss: 0.47412\n",
            "[7,  8000] loss: 0.45261\n",
            "[7, 10000] loss: 0.42338\n",
            "[7, 12000] loss: 0.42928\n",
            "[8,  2000] loss: 0.28663\n",
            "[8,  4000] loss: 0.29418\n",
            "[8,  6000] loss: 0.27105\n",
            "[8,  8000] loss: 0.27416\n",
            "[8, 10000] loss: 0.27917\n",
            "[8, 12000] loss: 0.27838\n",
            "[9,  2000] loss: 0.15551\n",
            "[9,  4000] loss: 0.16458\n",
            "[9,  6000] loss: 0.16815\n",
            "[9,  8000] loss: 0.18282\n",
            "[9, 10000] loss: 0.17885\n",
            "[9, 12000] loss: 0.18761\n",
            "[10,  2000] loss: 0.09848\n",
            "[10,  4000] loss: 0.09353\n",
            "[10,  6000] loss: 0.11200\n",
            "[10,  8000] loss: 0.11383\n",
            "[10, 10000] loss: 0.12538\n",
            "[10, 12000] loss: 0.12250\n",
            "[11,  2000] loss: 0.07363\n",
            "[11,  4000] loss: 0.07780\n",
            "[11,  6000] loss: 0.07150\n",
            "[11,  8000] loss: 0.09052\n",
            "[11, 10000] loss: 0.08911\n",
            "[11, 12000] loss: 0.08243\n",
            "[12,  2000] loss: 0.05779\n",
            "[12,  4000] loss: 0.06441\n",
            "[12,  6000] loss: 0.06753\n",
            "[12,  8000] loss: 0.05803\n",
            "[12, 10000] loss: 0.07258\n",
            "[12, 12000] loss: 0.06971\n",
            "[13,  2000] loss: 0.04093\n",
            "[13,  4000] loss: 0.05925\n",
            "[13,  6000] loss: 0.05913\n",
            "[13,  8000] loss: 0.05603\n",
            "[13, 10000] loss: 0.06373\n",
            "[13, 12000] loss: 0.04908\n",
            "[14,  2000] loss: 0.04481\n",
            "[14,  4000] loss: 0.04525\n",
            "[14,  6000] loss: 0.05936\n",
            "[14,  8000] loss: 0.05200\n",
            "[14, 10000] loss: 0.05563\n",
            "[14, 12000] loss: 0.05640\n",
            "[15,  2000] loss: 0.04542\n",
            "[15,  4000] loss: 0.04969\n",
            "[15,  6000] loss: 0.04719\n",
            "[15,  8000] loss: 0.04928\n",
            "[15, 10000] loss: 0.05011\n",
            "[15, 12000] loss: 0.05640\n",
            "[16,  2000] loss: 0.03715\n",
            "[16,  4000] loss: 0.05121\n",
            "[16,  6000] loss: 0.05006\n",
            "[16,  8000] loss: 0.04664\n",
            "[16, 10000] loss: 0.05237\n",
            "[16, 12000] loss: 0.05032\n",
            "[17,  2000] loss: 0.05668\n",
            "[17,  4000] loss: 0.04315\n",
            "[17,  6000] loss: 0.05124\n",
            "[17,  8000] loss: 0.04669\n",
            "[17, 10000] loss: 0.06429\n",
            "[17, 12000] loss: 0.04170\n",
            "[18,  2000] loss: 0.03671\n",
            "[18,  4000] loss: 0.05108\n",
            "[18,  6000] loss: 0.04173\n",
            "[18,  8000] loss: 0.05636\n",
            "[18, 10000] loss: 0.04257\n",
            "[18, 12000] loss: 0.05765\n",
            "[19,  2000] loss: 0.04866\n",
            "[19,  4000] loss: 0.05575\n",
            "[19,  6000] loss: 0.04480\n",
            "[19,  8000] loss: 0.05848\n",
            "[19, 10000] loss: 0.04792\n",
            "[19, 12000] loss: 0.05148\n",
            "[20,  2000] loss: 0.04262\n",
            "[20,  4000] loss: 0.05712\n",
            "[20,  6000] loss: 0.05461\n",
            "[20,  8000] loss: 0.05561\n",
            "[20, 10000] loss: 0.03552\n",
            "[20, 12000] loss: 0.06092\n",
            "[21,  2000] loss: 0.03295\n",
            "[21,  4000] loss: 0.05742\n",
            "[21,  6000] loss: 0.05444\n",
            "[21,  8000] loss: 0.04413\n",
            "[21, 10000] loss: 0.05973\n",
            "[21, 12000] loss: 0.04918\n",
            "[22,  2000] loss: 0.04177\n",
            "[22,  4000] loss: 0.06180\n",
            "[22,  6000] loss: 0.05844\n",
            "[22,  8000] loss: 0.04958\n",
            "[22, 10000] loss: 0.05711\n",
            "[22, 12000] loss: 0.05269\n",
            "[23,  2000] loss: 0.05253\n",
            "[23,  4000] loss: 0.06666\n",
            "[23,  6000] loss: 0.06838\n",
            "[23,  8000] loss: 0.06369\n",
            "[23, 10000] loss: 0.04951\n",
            "[23, 12000] loss: 0.05849\n",
            "[24,  2000] loss: 0.05533\n",
            "[24,  4000] loss: 0.05854\n",
            "[24,  6000] loss: 0.05089\n",
            "[24,  8000] loss: 0.06776\n",
            "[24, 10000] loss: 0.07840\n",
            "[24, 12000] loss: 0.06408\n",
            "[25,  2000] loss: 0.06820\n",
            "[25,  4000] loss: 0.06267\n",
            "[25,  6000] loss: 0.06360\n",
            "[25,  8000] loss: 0.08542\n",
            "[25, 10000] loss: 0.07095\n",
            "[25, 12000] loss: 0.08654\n",
            "[26,  2000] loss: 0.05385\n",
            "[26,  4000] loss: 0.08652\n",
            "[26,  6000] loss: 0.05483\n",
            "[26,  8000] loss: 0.05863\n",
            "[26, 10000] loss: 0.07425\n",
            "[26, 12000] loss: 0.07130\n",
            "[27,  2000] loss: 0.05923\n",
            "[27,  4000] loss: 0.07778\n",
            "[27,  6000] loss: 0.07477\n",
            "[27,  8000] loss: 0.07269\n",
            "[27, 10000] loss: 0.08954\n",
            "[27, 12000] loss: 0.07232\n",
            "[28,  2000] loss: 0.08907\n",
            "[28,  4000] loss: 0.06909\n",
            "[28,  6000] loss: 0.07912\n",
            "[28,  8000] loss: 0.09647\n",
            "[28, 10000] loss: 0.09673\n",
            "[28, 12000] loss: 0.07590\n",
            "[29,  2000] loss: 0.09120\n",
            "[29,  4000] loss: 0.09542\n",
            "[29,  6000] loss: 0.10282\n",
            "[29,  8000] loss: 0.08976\n",
            "[29, 10000] loss: 0.10647\n",
            "[29, 12000] loss: 0.11948\n",
            "[30,  2000] loss: 0.11870\n",
            "[30,  4000] loss: 0.12486\n",
            "[30,  6000] loss: 0.11957\n",
            "[30,  8000] loss: 0.09517\n",
            "[30, 10000] loss: 0.11591\n",
            "[30, 12000] loss: 0.12402\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_EzoAd4iJJ0"
      },
      "source": [
        "torch.save(Annealing_lut_model.state_dict(), \"./Annealing_lut_model.weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw-D7WrHgrsB",
        "outputId": "33264020-8469-46e7-e331-706c823027b2"
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_Prop = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, max(0.0001, math.pow(0.999859, idx/10))), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            Loss_Annealing_Model_Prop.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n",
            "[1,  2000] loss: 3.11043\n",
            "[1,  4000] loss: 2.17876\n",
            "[1,  6000] loss: 1.78036\n",
            "[1,  8000] loss: 1.62934\n",
            "[1, 10000] loss: 1.37215\n",
            "[1, 12000] loss: 1.22172\n",
            "[2,  2000] loss: 1.03193\n",
            "[2,  4000] loss: 1.00406\n",
            "[2,  6000] loss: 0.91328\n",
            "[2,  8000] loss: 0.72977\n",
            "[2, 10000] loss: 0.65891\n",
            "[2, 12000] loss: 0.59684\n",
            "[3,  2000] loss: 0.45673\n",
            "[3,  4000] loss: 0.44245\n",
            "[3,  6000] loss: 0.41886\n",
            "[3,  8000] loss: 0.42798\n",
            "[3, 10000] loss: 0.42486\n",
            "[3, 12000] loss: 0.41588\n",
            "[4,  2000] loss: 0.27830\n",
            "[4,  4000] loss: 0.27619\n",
            "[4,  6000] loss: 0.28058\n",
            "[4,  8000] loss: 0.27823\n",
            "[4, 10000] loss: 0.28670\n",
            "[4, 12000] loss: 0.26150\n",
            "[5,  2000] loss: 0.16712\n",
            "[5,  4000] loss: 0.19075\n",
            "[5,  6000] loss: 0.17574\n",
            "[5,  8000] loss: 0.18277\n",
            "[5, 10000] loss: 0.18618\n",
            "[5, 12000] loss: 0.19093\n",
            "[6,  2000] loss: 0.10355\n",
            "[6,  4000] loss: 0.11813\n",
            "[6,  6000] loss: 0.12705\n",
            "[6,  8000] loss: 0.12749\n",
            "[6, 10000] loss: 0.13698\n",
            "[6, 12000] loss: 0.11473\n",
            "[7,  2000] loss: 0.07560\n",
            "[7,  4000] loss: 0.08635\n",
            "[7,  6000] loss: 0.08136\n",
            "[7,  8000] loss: 0.09175\n",
            "[7, 10000] loss: 0.08691\n",
            "[7, 12000] loss: 0.09782\n",
            "[8,  2000] loss: 0.05597\n",
            "[8,  4000] loss: 0.06764\n",
            "[8,  6000] loss: 0.07298\n",
            "[8,  8000] loss: 0.07069\n",
            "[8, 10000] loss: 0.06792\n",
            "[8, 12000] loss: 0.06543\n",
            "[9,  2000] loss: 0.06169\n",
            "[9,  4000] loss: 0.05398\n",
            "[9,  6000] loss: 0.05080\n",
            "[9,  8000] loss: 0.05401\n",
            "[9, 10000] loss: 0.05471\n",
            "[9, 12000] loss: 0.06132\n",
            "[10,  2000] loss: 0.03915\n",
            "[10,  4000] loss: 0.05510\n",
            "[10,  6000] loss: 0.04904\n",
            "[10,  8000] loss: 0.05279\n",
            "[10, 10000] loss: 0.05733\n",
            "[10, 12000] loss: 0.05495\n",
            "[11,  2000] loss: 0.03742\n",
            "[11,  4000] loss: 0.04426\n",
            "[11,  6000] loss: 0.04215\n",
            "[11,  8000] loss: 0.04698\n",
            "[11, 10000] loss: 0.05629\n",
            "[11, 12000] loss: 0.04356\n",
            "[12,  2000] loss: 0.04014\n",
            "[12,  4000] loss: 0.03910\n",
            "[12,  6000] loss: 0.04293\n",
            "[12,  8000] loss: 0.04601\n",
            "[12, 10000] loss: 0.04940\n",
            "[12, 12000] loss: 0.04629\n",
            "[13,  2000] loss: 0.03589\n",
            "[13,  4000] loss: 0.03883\n",
            "[13,  6000] loss: 0.04145\n",
            "[13,  8000] loss: 0.04604\n",
            "[13, 10000] loss: 0.04918\n",
            "[13, 12000] loss: 0.03737\n",
            "[14,  2000] loss: 0.02548\n",
            "[14,  4000] loss: 0.04285\n",
            "[14,  6000] loss: 0.03665\n",
            "[14,  8000] loss: 0.04103\n",
            "[14, 10000] loss: 0.04625\n",
            "[14, 12000] loss: 0.04589\n",
            "[15,  2000] loss: 0.02520\n",
            "[15,  4000] loss: 0.04621\n",
            "[15,  6000] loss: 0.04731\n",
            "[15,  8000] loss: 0.03724\n",
            "[15, 10000] loss: 0.03925\n",
            "[15, 12000] loss: 0.04286\n",
            "[16,  2000] loss: 0.03360\n",
            "[16,  4000] loss: 0.03282\n",
            "[16,  6000] loss: 0.04901\n",
            "[16,  8000] loss: 0.03907\n",
            "[16, 10000] loss: 0.03352\n",
            "[16, 12000] loss: 0.04217\n",
            "[17,  2000] loss: 0.02903\n",
            "[17,  4000] loss: 0.04532\n",
            "[17,  6000] loss: 0.03575\n",
            "[17,  8000] loss: 0.04501\n",
            "[17, 10000] loss: 0.04175\n",
            "[17, 12000] loss: 0.03597\n",
            "[18,  2000] loss: 0.03472\n",
            "[18,  4000] loss: 0.03823\n",
            "[18,  6000] loss: 0.04272\n",
            "[18,  8000] loss: 0.05431\n",
            "[18, 10000] loss: 0.03338\n",
            "[18, 12000] loss: 0.03954\n",
            "[19,  2000] loss: 0.03033\n",
            "[19,  4000] loss: 0.03691\n",
            "[19,  6000] loss: 0.03351\n",
            "[19,  8000] loss: 0.04100\n",
            "[19, 10000] loss: 0.04109\n",
            "[19, 12000] loss: 0.04948\n",
            "[20,  2000] loss: 0.02674\n",
            "[20,  4000] loss: 0.04063\n",
            "[20,  6000] loss: 0.03509\n",
            "[20,  8000] loss: 0.03784\n",
            "[20, 10000] loss: 0.04492\n",
            "[20, 12000] loss: 0.04611\n",
            "[21,  2000] loss: 0.03665\n",
            "[21,  4000] loss: 0.04201\n",
            "[21,  6000] loss: 0.04585\n",
            "[21,  8000] loss: 0.03666\n",
            "[21, 10000] loss: 0.04139\n",
            "[21, 12000] loss: 0.05864\n",
            "[22,  2000] loss: 0.03409\n",
            "[22,  4000] loss: 0.04388\n",
            "[22,  6000] loss: 0.05609\n",
            "[22,  8000] loss: 0.05361\n",
            "[22, 10000] loss: 0.04282\n",
            "[22, 12000] loss: 0.04937\n",
            "[23,  2000] loss: 0.02451\n",
            "[23,  4000] loss: 0.04234\n",
            "[23,  6000] loss: 0.03889\n",
            "[23,  8000] loss: 0.04379\n",
            "[23, 10000] loss: 0.05265\n",
            "[23, 12000] loss: 0.04990\n",
            "[24,  2000] loss: 0.03733\n",
            "[24,  4000] loss: 0.04477\n",
            "[24,  6000] loss: 0.05186\n",
            "[24,  8000] loss: 0.05915\n",
            "[24, 10000] loss: 0.04571\n",
            "[24, 12000] loss: 0.05786\n",
            "[25,  2000] loss: 0.05717\n",
            "[25,  4000] loss: 0.04300\n",
            "[25,  6000] loss: 0.05544\n",
            "[25,  8000] loss: 0.05091\n",
            "[25, 10000] loss: 0.04119\n",
            "[25, 12000] loss: 0.04816\n",
            "[26,  2000] loss: 0.05714\n",
            "[26,  4000] loss: 0.04406\n",
            "[26,  6000] loss: 0.05737\n",
            "[26,  8000] loss: 0.05998\n",
            "[26, 10000] loss: 0.05606\n",
            "[26, 12000] loss: 0.05293\n",
            "[27,  2000] loss: 0.06809\n",
            "[27,  4000] loss: 0.04969\n",
            "[27,  6000] loss: 0.06087\n",
            "[27,  8000] loss: 0.06546\n",
            "[27, 10000] loss: 0.04434\n",
            "[27, 12000] loss: 0.07800\n",
            "[28,  2000] loss: 0.06844\n",
            "[28,  4000] loss: 0.05222\n",
            "[28,  6000] loss: 0.06621\n",
            "[28,  8000] loss: 0.06574\n",
            "[28, 10000] loss: 0.06141\n",
            "[28, 12000] loss: 0.07515\n",
            "[29,  2000] loss: 0.05695\n",
            "[29,  4000] loss: 0.07661\n",
            "[29,  6000] loss: 0.07335\n",
            "[29,  8000] loss: 0.08282\n",
            "[29, 10000] loss: 0.08054\n",
            "[29, 12000] loss: 0.07324\n",
            "[30,  2000] loss: 0.08976\n",
            "[30,  4000] loss: 0.08629\n",
            "[30,  6000] loss: 0.12210\n",
            "[30,  8000] loss: 0.07980\n",
            "[30, 10000] loss: 0.09921\n",
            "[30, 12000] loss: 0.11144\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx8D7n6Rgwj6",
        "outputId": "beeefa27-880f-4705-9ad0-e84e506bf0cb"
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_linear = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, 0.9999 - 0.033*epoch), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            print('[%d, %5d] temperature: %.5f' %\n",
        "                  (epoch + 1, i + 1, max(0.0001, 0.9999 - 0.033*epoch)))\n",
        "            Loss_Annealing_Model_linear.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n",
            "[1,  2000] loss: 3.08882\n",
            "[1,  2000] temperature: 0.99990\n",
            "[1,  4000] loss: 2.34408\n",
            "[1,  4000] temperature: 0.99990\n",
            "[1,  6000] loss: 2.34030\n",
            "[1,  6000] temperature: 0.99990\n",
            "[1,  8000] loss: 2.33999\n",
            "[1,  8000] temperature: 0.99990\n",
            "[1, 10000] loss: 2.32942\n",
            "[1, 10000] temperature: 0.99990\n",
            "[1, 12000] loss: 2.32902\n",
            "[1, 12000] temperature: 0.99990\n",
            "[2,  2000] loss: 2.32802\n",
            "[2,  2000] temperature: 0.96690\n",
            "[2,  4000] loss: 2.32182\n",
            "[2,  4000] temperature: 0.96690\n",
            "[2,  6000] loss: 2.15810\n",
            "[2,  6000] temperature: 0.96690\n",
            "[2,  8000] loss: 1.69851\n",
            "[2,  8000] temperature: 0.96690\n",
            "[2, 10000] loss: 1.34304\n",
            "[2, 10000] temperature: 0.96690\n",
            "[2, 12000] loss: 1.18133\n",
            "[2, 12000] temperature: 0.96690\n",
            "[3,  2000] loss: 1.02457\n",
            "[3,  2000] temperature: 0.93390\n",
            "[3,  4000] loss: 0.98088\n",
            "[3,  4000] temperature: 0.93390\n",
            "[3,  6000] loss: 0.91810\n",
            "[3,  6000] temperature: 0.93390\n",
            "[3,  8000] loss: 0.85310\n",
            "[3,  8000] temperature: 0.93390\n",
            "[3, 10000] loss: 0.70510\n",
            "[3, 10000] temperature: 0.93390\n",
            "[3, 12000] loss: 0.64725\n",
            "[3, 12000] temperature: 0.93390\n",
            "[4,  2000] loss: 0.46790\n",
            "[4,  2000] temperature: 0.90090\n",
            "[4,  4000] loss: 0.46762\n",
            "[4,  4000] temperature: 0.90090\n",
            "[4,  6000] loss: 0.45853\n",
            "[4,  6000] temperature: 0.90090\n",
            "[4,  8000] loss: 0.44053\n",
            "[4,  8000] temperature: 0.90090\n",
            "[4, 10000] loss: 0.42143\n",
            "[4, 10000] temperature: 0.90090\n",
            "[4, 12000] loss: 0.42993\n",
            "[4, 12000] temperature: 0.90090\n",
            "[5,  2000] loss: 0.28647\n",
            "[5,  2000] temperature: 0.86790\n",
            "[5,  4000] loss: 0.29447\n",
            "[5,  4000] temperature: 0.86790\n",
            "[5,  6000] loss: 0.28755\n",
            "[5,  6000] temperature: 0.86790\n",
            "[5,  8000] loss: 0.28561\n",
            "[5,  8000] temperature: 0.86790\n",
            "[5, 10000] loss: 0.29532\n",
            "[5, 10000] temperature: 0.86790\n",
            "[5, 12000] loss: 0.28192\n",
            "[5, 12000] temperature: 0.86790\n",
            "[6,  2000] loss: 0.17247\n",
            "[6,  2000] temperature: 0.83490\n",
            "[6,  4000] loss: 0.18347\n",
            "[6,  4000] temperature: 0.83490\n",
            "[6,  6000] loss: 0.17552\n",
            "[6,  6000] temperature: 0.83490\n",
            "[6,  8000] loss: 0.17549\n",
            "[6,  8000] temperature: 0.83490\n",
            "[6, 10000] loss: 0.18405\n",
            "[6, 10000] temperature: 0.83490\n",
            "[6, 12000] loss: 0.18587\n",
            "[6, 12000] temperature: 0.83490\n",
            "[7,  2000] loss: 0.10863\n",
            "[7,  2000] temperature: 0.80190\n",
            "[7,  4000] loss: 0.11897\n",
            "[7,  4000] temperature: 0.80190\n",
            "[7,  6000] loss: 0.11284\n",
            "[7,  6000] temperature: 0.80190\n",
            "[7,  8000] loss: 0.11499\n",
            "[7,  8000] temperature: 0.80190\n",
            "[7, 10000] loss: 0.11319\n",
            "[7, 10000] temperature: 0.80190\n",
            "[7, 12000] loss: 0.12765\n",
            "[7, 12000] temperature: 0.80190\n",
            "[8,  2000] loss: 0.07907\n",
            "[8,  2000] temperature: 0.76890\n",
            "[8,  4000] loss: 0.07604\n",
            "[8,  4000] temperature: 0.76890\n",
            "[8,  6000] loss: 0.07826\n",
            "[8,  6000] temperature: 0.76890\n",
            "[8,  8000] loss: 0.07911\n",
            "[8,  8000] temperature: 0.76890\n",
            "[8, 10000] loss: 0.07139\n",
            "[8, 10000] temperature: 0.76890\n",
            "[8, 12000] loss: 0.09805\n",
            "[8, 12000] temperature: 0.76890\n",
            "[9,  2000] loss: 0.05060\n",
            "[9,  2000] temperature: 0.73590\n",
            "[9,  4000] loss: 0.05924\n",
            "[9,  4000] temperature: 0.73590\n",
            "[9,  6000] loss: 0.06382\n",
            "[9,  6000] temperature: 0.73590\n",
            "[9,  8000] loss: 0.06928\n",
            "[9,  8000] temperature: 0.73590\n",
            "[9, 10000] loss: 0.05997\n",
            "[9, 10000] temperature: 0.73590\n",
            "[9, 12000] loss: 0.06917\n",
            "[9, 12000] temperature: 0.73590\n",
            "[10,  2000] loss: 0.04578\n",
            "[10,  2000] temperature: 0.70290\n",
            "[10,  4000] loss: 0.04620\n",
            "[10,  4000] temperature: 0.70290\n",
            "[10,  6000] loss: 0.04838\n",
            "[10,  6000] temperature: 0.70290\n",
            "[10,  8000] loss: 0.05520\n",
            "[10,  8000] temperature: 0.70290\n",
            "[10, 10000] loss: 0.05443\n",
            "[10, 10000] temperature: 0.70290\n",
            "[10, 12000] loss: 0.04920\n",
            "[10, 12000] temperature: 0.70290\n",
            "[11,  2000] loss: 0.04942\n",
            "[11,  2000] temperature: 0.66990\n",
            "[11,  4000] loss: 0.04378\n",
            "[11,  4000] temperature: 0.66990\n",
            "[11,  6000] loss: 0.05042\n",
            "[11,  6000] temperature: 0.66990\n",
            "[11,  8000] loss: 0.05443\n",
            "[11,  8000] temperature: 0.66990\n",
            "[11, 10000] loss: 0.03743\n",
            "[11, 10000] temperature: 0.66990\n",
            "[11, 12000] loss: 0.04633\n",
            "[11, 12000] temperature: 0.66990\n",
            "[12,  2000] loss: 0.02882\n",
            "[12,  2000] temperature: 0.63690\n",
            "[12,  4000] loss: 0.02942\n",
            "[12,  4000] temperature: 0.63690\n",
            "[12,  6000] loss: 0.04517\n",
            "[12,  6000] temperature: 0.63690\n",
            "[12,  8000] loss: 0.04926\n",
            "[12,  8000] temperature: 0.63690\n",
            "[12, 10000] loss: 0.03868\n",
            "[12, 10000] temperature: 0.63690\n",
            "[12, 12000] loss: 0.04457\n",
            "[12, 12000] temperature: 0.63690\n",
            "[13,  2000] loss: 0.03745\n",
            "[13,  2000] temperature: 0.60390\n",
            "[13,  4000] loss: 0.03299\n",
            "[13,  4000] temperature: 0.60390\n",
            "[13,  6000] loss: 0.03308\n",
            "[13,  6000] temperature: 0.60390\n",
            "[13,  8000] loss: 0.03118\n",
            "[13,  8000] temperature: 0.60390\n",
            "[13, 10000] loss: 0.03956\n",
            "[13, 10000] temperature: 0.60390\n",
            "[13, 12000] loss: 0.03643\n",
            "[13, 12000] temperature: 0.60390\n",
            "[14,  2000] loss: 0.03079\n",
            "[14,  2000] temperature: 0.57090\n",
            "[14,  4000] loss: 0.04095\n",
            "[14,  4000] temperature: 0.57090\n",
            "[14,  6000] loss: 0.03595\n",
            "[14,  6000] temperature: 0.57090\n",
            "[14,  8000] loss: 0.02379\n",
            "[14,  8000] temperature: 0.57090\n",
            "[14, 10000] loss: 0.03902\n",
            "[14, 10000] temperature: 0.57090\n",
            "[14, 12000] loss: 0.03312\n",
            "[14, 12000] temperature: 0.57090\n",
            "[15,  2000] loss: 0.02161\n",
            "[15,  2000] temperature: 0.53790\n",
            "[15,  4000] loss: 0.03998\n",
            "[15,  4000] temperature: 0.53790\n",
            "[15,  6000] loss: 0.02704\n",
            "[15,  6000] temperature: 0.53790\n",
            "[15,  8000] loss: 0.03216\n",
            "[15,  8000] temperature: 0.53790\n",
            "[15, 10000] loss: 0.02259\n",
            "[15, 10000] temperature: 0.53790\n",
            "[15, 12000] loss: 0.03543\n",
            "[15, 12000] temperature: 0.53790\n",
            "[16,  2000] loss: 0.02854\n",
            "[16,  2000] temperature: 0.50490\n",
            "[16,  4000] loss: 0.02667\n",
            "[16,  4000] temperature: 0.50490\n",
            "[16,  6000] loss: 0.02935\n",
            "[16,  6000] temperature: 0.50490\n",
            "[16,  8000] loss: 0.02786\n",
            "[16,  8000] temperature: 0.50490\n",
            "[16, 10000] loss: 0.04252\n",
            "[16, 10000] temperature: 0.50490\n",
            "[16, 12000] loss: 0.02068\n",
            "[16, 12000] temperature: 0.50490\n",
            "[17,  2000] loss: 0.02212\n",
            "[17,  2000] temperature: 0.47190\n",
            "[17,  4000] loss: 0.03218\n",
            "[17,  4000] temperature: 0.47190\n",
            "[17,  6000] loss: 0.02763\n",
            "[17,  6000] temperature: 0.47190\n",
            "[17,  8000] loss: 0.02426\n",
            "[17,  8000] temperature: 0.47190\n",
            "[17, 10000] loss: 0.03245\n",
            "[17, 10000] temperature: 0.47190\n",
            "[17, 12000] loss: 0.02698\n",
            "[17, 12000] temperature: 0.47190\n",
            "[18,  2000] loss: 0.02477\n",
            "[18,  2000] temperature: 0.43890\n",
            "[18,  4000] loss: 0.02978\n",
            "[18,  4000] temperature: 0.43890\n",
            "[18,  6000] loss: 0.02847\n",
            "[18,  6000] temperature: 0.43890\n",
            "[18,  8000] loss: 0.02551\n",
            "[18,  8000] temperature: 0.43890\n",
            "[18, 10000] loss: 0.03550\n",
            "[18, 10000] temperature: 0.43890\n",
            "[18, 12000] loss: 0.02511\n",
            "[18, 12000] temperature: 0.43890\n",
            "[19,  2000] loss: 0.02338\n",
            "[19,  2000] temperature: 0.40590\n",
            "[19,  4000] loss: 0.02714\n",
            "[19,  4000] temperature: 0.40590\n",
            "[19,  6000] loss: 0.01832\n",
            "[19,  6000] temperature: 0.40590\n",
            "[19,  8000] loss: 0.03403\n",
            "[19,  8000] temperature: 0.40590\n",
            "[19, 10000] loss: 0.02178\n",
            "[19, 10000] temperature: 0.40590\n",
            "[19, 12000] loss: 0.01940\n",
            "[19, 12000] temperature: 0.40590\n",
            "[20,  2000] loss: 0.03114\n",
            "[20,  2000] temperature: 0.37290\n",
            "[20,  4000] loss: 0.02868\n",
            "[20,  4000] temperature: 0.37290\n",
            "[20,  6000] loss: 0.01927\n",
            "[20,  6000] temperature: 0.37290\n",
            "[20,  8000] loss: 0.02805\n",
            "[20,  8000] temperature: 0.37290\n",
            "[20, 10000] loss: 0.02298\n",
            "[20, 10000] temperature: 0.37290\n",
            "[20, 12000] loss: 0.02006\n",
            "[20, 12000] temperature: 0.37290\n",
            "[21,  2000] loss: 0.01878\n",
            "[21,  2000] temperature: 0.33990\n",
            "[21,  4000] loss: 0.02330\n",
            "[21,  4000] temperature: 0.33990\n",
            "[21,  6000] loss: 0.02169\n",
            "[21,  6000] temperature: 0.33990\n",
            "[21,  8000] loss: 0.02857\n",
            "[21,  8000] temperature: 0.33990\n",
            "[21, 10000] loss: 0.02885\n",
            "[21, 10000] temperature: 0.33990\n",
            "[21, 12000] loss: 0.01883\n",
            "[21, 12000] temperature: 0.33990\n",
            "[22,  2000] loss: 0.02546\n",
            "[22,  2000] temperature: 0.30690\n",
            "[22,  4000] loss: 0.02046\n",
            "[22,  4000] temperature: 0.30690\n",
            "[22,  6000] loss: 0.02390\n",
            "[22,  6000] temperature: 0.30690\n",
            "[22,  8000] loss: 0.01778\n",
            "[22,  8000] temperature: 0.30690\n",
            "[22, 10000] loss: 0.03113\n",
            "[22, 10000] temperature: 0.30690\n",
            "[22, 12000] loss: 0.03015\n",
            "[22, 12000] temperature: 0.30690\n",
            "[23,  2000] loss: 0.02695\n",
            "[23,  2000] temperature: 0.27390\n",
            "[23,  4000] loss: 0.01766\n",
            "[23,  4000] temperature: 0.27390\n",
            "[23,  6000] loss: 0.02299\n",
            "[23,  6000] temperature: 0.27390\n",
            "[23,  8000] loss: 0.02640\n",
            "[23,  8000] temperature: 0.27390\n",
            "[23, 10000] loss: 0.02207\n",
            "[23, 10000] temperature: 0.27390\n",
            "[23, 12000] loss: 0.01812\n",
            "[23, 12000] temperature: 0.27390\n",
            "[24,  2000] loss: 0.02308\n",
            "[24,  2000] temperature: 0.24090\n",
            "[24,  4000] loss: 0.02618\n",
            "[24,  4000] temperature: 0.24090\n",
            "[24,  6000] loss: 0.03280\n",
            "[24,  6000] temperature: 0.24090\n",
            "[24,  8000] loss: 0.01918\n",
            "[24,  8000] temperature: 0.24090\n",
            "[24, 10000] loss: 0.02601\n",
            "[24, 10000] temperature: 0.24090\n",
            "[24, 12000] loss: 0.02782\n",
            "[24, 12000] temperature: 0.24090\n",
            "[25,  2000] loss: 0.02149\n",
            "[25,  2000] temperature: 0.20790\n",
            "[25,  4000] loss: 0.02424\n",
            "[25,  4000] temperature: 0.20790\n",
            "[25,  6000] loss: 0.02876\n",
            "[25,  6000] temperature: 0.20790\n",
            "[25,  8000] loss: 0.01528\n",
            "[25,  8000] temperature: 0.20790\n",
            "[25, 10000] loss: 0.03431\n",
            "[25, 10000] temperature: 0.20790\n",
            "[25, 12000] loss: 0.02101\n",
            "[25, 12000] temperature: 0.20790\n",
            "[26,  2000] loss: 0.02687\n",
            "[26,  2000] temperature: 0.17490\n",
            "[26,  4000] loss: 0.02523\n",
            "[26,  4000] temperature: 0.17490\n",
            "[26,  6000] loss: 0.02377\n",
            "[26,  6000] temperature: 0.17490\n",
            "[26,  8000] loss: 0.02963\n",
            "[26,  8000] temperature: 0.17490\n",
            "[26, 10000] loss: 0.02386\n",
            "[26, 10000] temperature: 0.17490\n",
            "[26, 12000] loss: 0.02760\n",
            "[26, 12000] temperature: 0.17490\n",
            "[27,  2000] loss: 0.02702\n",
            "[27,  2000] temperature: 0.14190\n",
            "[27,  4000] loss: 0.02669\n",
            "[27,  4000] temperature: 0.14190\n",
            "[27,  6000] loss: 0.03172\n",
            "[27,  6000] temperature: 0.14190\n",
            "[27,  8000] loss: 0.02814\n",
            "[27,  8000] temperature: 0.14190\n",
            "[27, 10000] loss: 0.02075\n",
            "[27, 10000] temperature: 0.14190\n",
            "[27, 12000] loss: 0.02505\n",
            "[27, 12000] temperature: 0.14190\n",
            "[28,  2000] loss: 0.02631\n",
            "[28,  2000] temperature: 0.10890\n",
            "[28,  4000] loss: 0.02263\n",
            "[28,  4000] temperature: 0.10890\n",
            "[28,  6000] loss: 0.03723\n",
            "[28,  6000] temperature: 0.10890\n",
            "[28,  8000] loss: 0.02026\n",
            "[28,  8000] temperature: 0.10890\n",
            "[28, 10000] loss: 0.02016\n",
            "[28, 10000] temperature: 0.10890\n",
            "[28, 12000] loss: 0.03076\n",
            "[28, 12000] temperature: 0.10890\n",
            "[29,  2000] loss: 0.04590\n",
            "[29,  2000] temperature: 0.07590\n",
            "[29,  4000] loss: 0.03735\n",
            "[29,  4000] temperature: 0.07590\n",
            "[29,  6000] loss: 0.02743\n",
            "[29,  6000] temperature: 0.07590\n",
            "[29,  8000] loss: 0.03088\n",
            "[29,  8000] temperature: 0.07590\n",
            "[29, 10000] loss: 0.02969\n",
            "[29, 10000] temperature: 0.07590\n",
            "[29, 12000] loss: 0.03948\n",
            "[29, 12000] temperature: 0.07590\n",
            "[30,  2000] loss: 0.04514\n",
            "[30,  2000] temperature: 0.04290\n",
            "[30,  4000] loss: 0.04960\n",
            "[30,  4000] temperature: 0.04290\n",
            "[30,  6000] loss: 0.04379\n",
            "[30,  6000] temperature: 0.04290\n",
            "[30,  8000] loss: 0.03973\n",
            "[30,  8000] temperature: 0.04290\n",
            "[30, 10000] loss: 0.04191\n",
            "[30, 10000] temperature: 0.04290\n",
            "[30, 12000] loss: 0.04005\n",
            "[30, 12000] temperature: 0.04290\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivk559GX2dzL"
      },
      "source": [
        "Loss_Annealing_Model_exp = [6295.4160088300705, 4698.362445831299, 4681.099160313606, 4668.512912392616, 4665.40352332592, 4649.154500126839, 4645.708291292191, 4642.8637989759445, 4642.6537890434265, 4637.786241769791, 4631.127181529999, 4630.308316707611, 4629.626549243927, 4627.073973894119, 4626.6579077243805, 4623.503995895386, 4626.226335763931, 4620.009803056717, 4612.908445119858, 4620.35947227478, 4618.407818555832, 4617.662576675415, 4618.4861171245575, 4614.872433662415, 4616.73054933548, 4614.7221603393555, 4340.615003347397, 3408.0363912582397, 2879.1572725772858, 2405.6561209112406, 2125.3465134873986, 2012.9659889303148, 1874.5371485576034, 1705.1083010323346, 1460.5514042954892, 1315.0550528094172, 990.863434761297, 975.5274964037817, 948.2394827473909, 905.2119477665983, 846.7528409359511, 858.550159299979, 573.2552340372931, 588.3645126987249, 542.1010585740441, 548.3139305345248, 558.3395072179846, 556.7611744028982, 311.01018921949435, 329.15448177506914, 336.3068705199985, 365.6474192829337, 357.70949062216096, 375.2174963091966, 196.9598004668369, 187.06422230505268, 224.00901033121045, 227.65594794074423, 250.7698380239308, 245.0096245467139, 147.25303144182544, 155.60782268331968, 142.99294923098932, 181.03793473975384, 178.21686264319578, 164.86853500454163, 115.57596279666905, 128.8182194550027, 135.05731756526802, 116.05888023180887, 145.1599851091887, 139.41539937615016, 81.8629353906872, 118.50191818606982, 118.26940735863172, 112.05066671759414, 127.45522794957651, 98.15993556015019, 89.61265879383427, 90.49866250083869, 118.72730098076863, 104.00022450851247, 111.25868048205302, 112.79869797622814, 90.83161698708864, 99.37500475882553, 94.37667373841396, 98.55551795671636, 100.21072030838695, 112.79142068477813, 74.3039433184822, 102.42721597936907, 100.11288451219298, 93.28121933261718, 104.74826638246304, 100.6466061032479, 113.36124050460785, 86.30087341940089, 102.47195718255534, 93.3793903534388, 128.5826018464577, 83.39713205981388, 73.42698555495008, 102.15758710772207, 83.4576257881854, 112.72878997067164, 85.13436931811157, 115.29547659990203, 97.31187044503167, 111.49026581554244, 89.59351160014921, 116.95810813056596, 95.83408886080724, 102.96631332362449, 85.24763971059292, 114.24020509213733, 109.21131833456457, 111.2107011096814, 71.04304717631021, 121.83521597171784, 65.90878196049016, 114.84838727378519, 108.88343652935873, 88.26369838722894, 119.4624904114171, 98.35616062146437, 83.53538319135987, 123.60227493316052, 116.87819167286216, 99.15412604712219, 114.21798348141601, 105.37669312005164, 105.05660838962649, 133.31881659918872, 136.75471474180813, 127.38706888750312, 99.01469031575834, 116.97607605610392, 110.65479330093876, 117.08963824689272, 101.77839259279426, 135.51604385842802, 156.79343406611588, 128.15946231316775, 136.40030639198085, 125.34101394368918, 127.2026952409069, 170.8479464748525, 141.8942141032894, 173.07016420154832, 107.7095875258965, 173.03868169133784, 109.6556740895976, 117.25597191319684, 148.50370185245993, 142.59542406987748, 118.46621643722756, 155.5620129633462, 149.54364278697176, 145.37301812914666, 179.08255185134476, 144.63176771771396, 178.14770285843406, 138.18062005139655, 158.23695968009997, 192.9404472723254, 193.46677175047807, 151.80927048111334, 182.40204298659228, 190.83327328407904, 205.64918641687836, 179.51875323971035, 212.93038592400262, 238.95440808020066, 237.40040828776546, 249.72854166512843, 239.13739011983853, 190.33303778641857, 231.81306141149253, 248.04296878050081]\n",
        "Loss_Annealing_Model_Prop = [6220.854539632797, 4357.512228369713, 3560.7143238782883, 3258.6882833838463, 2744.3019897937775, 2443.440160661936, 2063.850209519267, 2008.1231438145041, 1826.5542283058167, 1459.5433863922954, 1317.8277175985277, 1193.670773955062, 913.462082920596, 884.8907753033563, 837.7196136526763, 855.9593238416128, 849.7270208965056, 831.7540355953388, 556.6059640741441, 552.3761584348977, 561.1651882429142, 556.4671341502108, 573.4008204466663, 522.9952387192752, 334.23353291267995, 381.49829956085887, 351.4878318324336, 365.54082414496224, 372.3614286106895, 381.86965760361636, 207.0965942578623, 236.2556466513779, 254.10093355245772, 254.98677541621146, 273.9518919768743, 229.46912388467172, 151.20215218178055, 172.70516892365413, 162.72984167112736, 183.50582504263002, 173.82667379720078, 195.64218519820133, 111.94436782556295, 135.27907845918526, 145.95666329300911, 141.37535232941445, 135.83692460513703, 130.86519827920347, 123.38699456820177, 107.96427598726041, 101.59184595567785, 108.01149839731625, 109.41993228774118, 122.64215594829511, 78.30377630580551, 110.19233638581136, 98.08623543788417, 105.57204419384107, 114.65146975519383, 109.89437316254407, 74.83768240616337, 88.51907770286925, 84.29513670922506, 93.96637365380593, 112.57356677966982, 87.11818689660686, 80.28151254779277, 78.20246818179658, 85.85371770399706, 92.02220574674357, 98.80801807821354, 92.57607371475024, 71.7700077086356, 77.65279292068954, 82.89064036734999, 92.08988713461986, 98.3666644701425, 74.73861632251464, 50.95140570908188, 85.70787763495264, 73.30436932802422, 82.06438989797061, 92.50244645905696, 91.77993676969072, 50.3933720754012, 92.41109845755818, 94.62121629791, 74.47531171015726, 78.49797643410784, 85.72634399271192, 67.20536332319898, 65.63931976171261, 98.01541782727145, 78.14115873169521, 67.03880823390045, 84.33143507710702, 58.05519674954303, 90.63472934488163, 71.504201746935, 90.01595893313697, 83.49910275244656, 71.9475056617066, 69.43028580918872, 76.45588561195655, 85.44832379454783, 108.62902272275733, 66.75146675873293, 79.08281219650235, 60.65163255714833, 73.82813994305434, 67.01901052105404, 82.00114565494187, 82.18534113478745, 98.95208498062675, 53.470144948047164, 81.25247265269354, 70.18512464572814, 75.67947529074445, 89.84495142823289, 92.21660705854447, 73.29837861466785, 84.02562494479025, 91.70471884665221, 73.31107895772493, 82.77153714711676, 117.2709182898925, 68.188322080754, 87.76216780617506, 112.1836749116028, 107.22456512407189, 85.63151786475464, 98.73239184534759, 49.014715941603754, 84.68163477782355, 77.77314811387623, 87.57295956649614, 105.3085109849053, 99.80937882158105, 74.66674253613564, 89.54717735111262, 103.72654193900598, 118.30979176298206, 91.41453034950973, 115.7262379896456, 114.34129017288069, 85.9996365209081, 110.88514298873088, 101.81968516592678, 82.38071673901504, 96.31443170999046, 114.27044929664407, 88.11190371192788, 114.7419846110206, 119.96375622509004, 112.11399588132917, 105.8645587853116, 136.17484177384904, 99.37712556691986, 121.7416490604628, 130.92985379737365, 88.67763160055983, 156.00151549652583, 136.88786180966417, 104.44043026469444, 132.41246499778936, 131.48437380525866, 122.81701343458553, 150.30743624575916, 113.90362379302678, 153.22035992786186, 146.70739963180677, 165.63859604339814, 161.08452643285273, 146.47826912646997, 179.51025893667247, 172.573260606674, 244.20462181983748, 159.60718980350066, 198.42604974567075, 222.87538916931953]\n",
        "Loss_Annealing_Model_linear = [6177.63513982296, 4688.154862642288, 4680.595834970474, 4679.982576608658, 4658.844668149948, 4658.0486072301865, 4656.042840242386, 4643.63847887516, 4316.2015463113785, 3397.0152685046196, 2686.084766983986, 2362.6502960920334, 2049.1474898308516, 1961.7500022128224, 1836.1954694539309, 1706.1916274949908, 1410.197340078652, 1294.495565244928, 935.8000734616071, 935.2391139315441, 917.0666223624721, 881.0626139689703, 842.8628254611976, 859.8692125482485, 572.9436194796581, 588.9477559641236, 575.0998206199147, 571.2292809489882, 590.6300078561762, 563.8484839322045, 344.9473996211309, 366.9364054800826, 351.0408669330063, 350.98355356484535, 368.0944739432307, 371.7355560786091, 217.26770937783294, 237.94814313817187, 225.6872198242927, 229.97781399221276, 226.38184007268865, 255.30620852761422, 158.13169128586014, 152.08564721266885, 156.5181927769081, 158.22156277913746, 142.7848219147927, 196.10055373955765, 101.20282580216008, 118.48277791491637, 127.64937190130513, 138.5585107445586, 119.94276411071041, 138.33026749570308, 91.55920823532142, 92.40652372951445, 96.75875900189385, 110.39135252587221, 108.85871113340545, 98.39381905483788, 98.84404215017639, 87.5679754872217, 100.84720365230896, 108.86526115816196, 74.86446841432189, 92.66226976450162, 57.64340366607303, 58.83306118502378, 90.3379619954544, 98.52031646377418, 77.36405532985918, 89.14934793126986, 74.8988736879046, 65.97922516124618, 66.1665620106246, 62.35047348273065, 79.12087732289592, 72.85881813954097, 61.58828256889683, 81.90510997325441, 71.90153529435156, 47.58087429436682, 78.0397236905419, 66.24192191632073, 43.215270672468705, 79.95293436771863, 54.079227125409886, 64.31783788733583, 45.189542166744445, 70.86203764913421, 57.07770600523645, 53.33786260183928, 58.705637161491836, 55.72556871767142, 85.03063382554734, 41.35735962847096, 44.24429358986359, 64.36356889785372, 55.250241861407744, 48.51156683492218, 64.89965613474362, 53.96448373774069, 49.54399233186007, 59.56849943415051, 56.944738901401934, 51.015272398006005, 70.99246688405688, 50.21085888625265, 46.75048479580755, 54.2827229914007, 36.639378122871754, 68.05356553156841, 43.56316417413052, 38.80045056096367, 62.276073484283955, 57.357326631311025, 38.54367123651751, 56.09508805914635, 45.95696317807554, 40.12403512090032, 37.552757121144865, 46.606351209616605, 43.38335970711903, 57.1403346020862, 57.69798320438849, 37.65050443240108, 50.91794669749464, 40.91329874218684, 47.80590700862838, 35.56011181286689, 62.26911950186067, 60.296114558525005, 53.90827183946806, 35.32124669200397, 45.97817203102146, 52.79760190794855, 44.14802972919429, 36.24118357505637, 46.158250670759735, 52.35076473416137, 65.59874504221717, 38.36403506422826, 52.026226129651434, 55.63734285998555, 42.98443995714682, 48.47104126951373, 57.5269945333753, 30.55949777343774, 68.62660051074175, 42.020057480523576, 53.74721183322811, 50.46618256578725, 47.54393276658186, 59.25862434238397, 47.72942800103719, 55.1973986099635, 54.047229241600434, 53.378003295923214, 63.44727377089839, 56.275737586254905, 41.492042740063724, 50.092185558889746, 52.61138360103709, 45.263400583539095, 74.45151781298333, 40.51303343696617, 40.31838699915379, 61.516358665445466, 91.80051654547991, 74.70624295410455, 54.85348572814631, 61.75123809206548, 59.388727995377735, 78.9581865802405, 90.28083369474689, 99.2082248194613, 87.57844982204324, 79.46288096128512, 83.82231957863922, 80.10519370284533]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "441xtpHFqB9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "1e84e299-6a86-4ad1-98e6-164b8253576e"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(6, 4))\n",
        "ax.plot(Loss_Annealing_Model_exp, Color='red', label='exponential_e')\n",
        "ax.plot(Loss_Annealing_Model_Prop, Color='green', label='exponential')\n",
        "ax.plot(Loss_Annealing_Model_linear, Color='blue', label='linear')\n",
        "ax.set_xlabel(\"Iteration\", fontsize=16)\n",
        "ax.set_ylabel(\"Loss\", fontsize=16)\n",
        "# ax.set_title(\"Linear Model\", fontsize=16)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "# ax.yaxis.set_major_locator(y_major_locator)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6A-mCyNRdM9",
        "outputId": "1a17fbc7-9a1b-4693-e5b4-8cd445c505ee"
      },
      "source": [
        "# print(Loss_Annealing_Model_exp)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6295.4160088300705, 4698.362445831299, 4681.099160313606, 4668.512912392616, 4665.40352332592, 4649.154500126839, 4645.708291292191, 4642.8637989759445, 4642.6537890434265, 4637.786241769791, 4631.127181529999, 4630.308316707611, 4629.626549243927, 4627.073973894119, 4626.6579077243805, 4623.503995895386, 4626.226335763931, 4620.009803056717, 4612.908445119858, 4620.35947227478, 4618.407818555832, 4617.662576675415, 4618.4861171245575, 4614.872433662415, 4616.73054933548, 4614.7221603393555, 4340.615003347397, 3408.0363912582397, 2879.1572725772858, 2405.6561209112406, 2125.3465134873986, 2012.9659889303148, 1874.5371485576034, 1705.1083010323346, 1460.5514042954892, 1315.0550528094172, 990.863434761297, 975.5274964037817, 948.2394827473909, 905.2119477665983, 846.7528409359511, 858.550159299979, 573.2552340372931, 588.3645126987249, 542.1010585740441, 548.3139305345248, 558.3395072179846, 556.7611744028982, 311.01018921949435, 329.15448177506914, 336.3068705199985, 365.6474192829337, 357.70949062216096, 375.2174963091966, 196.9598004668369, 187.06422230505268, 224.00901033121045, 227.65594794074423, 250.7698380239308, 245.0096245467139, 147.25303144182544, 155.60782268331968, 142.99294923098932, 181.03793473975384, 178.21686264319578, 164.86853500454163, 115.57596279666905, 128.8182194550027, 135.05731756526802, 116.05888023180887, 145.1599851091887, 139.41539937615016, 81.8629353906872, 118.50191818606982, 118.26940735863172, 112.05066671759414, 127.45522794957651, 98.15993556015019, 89.61265879383427, 90.49866250083869, 118.72730098076863, 104.00022450851247, 111.25868048205302, 112.79869797622814, 90.83161698708864, 99.37500475882553, 94.37667373841396, 98.55551795671636, 100.21072030838695, 112.79142068477813, 74.3039433184822, 102.42721597936907, 100.11288451219298, 93.28121933261718, 104.74826638246304, 100.6466061032479, 113.36124050460785, 86.30087341940089, 102.47195718255534, 93.3793903534388, 128.5826018464577, 83.39713205981388, 73.42698555495008, 102.15758710772207, 83.4576257881854, 112.72878997067164, 85.13436931811157, 115.29547659990203, 97.31187044503167, 111.49026581554244, 89.59351160014921, 116.95810813056596, 95.83408886080724, 102.96631332362449, 85.24763971059292, 114.24020509213733, 109.21131833456457, 111.2107011096814, 71.04304717631021, 121.83521597171784, 65.90878196049016, 114.84838727378519, 108.88343652935873, 88.26369838722894, 119.4624904114171, 98.35616062146437, 83.53538319135987, 123.60227493316052, 116.87819167286216, 99.15412604712219, 114.21798348141601, 105.37669312005164, 105.05660838962649, 133.31881659918872, 136.75471474180813, 127.38706888750312, 99.01469031575834, 116.97607605610392, 110.65479330093876, 117.08963824689272, 101.77839259279426, 135.51604385842802, 156.79343406611588, 128.15946231316775, 136.40030639198085, 125.34101394368918, 127.2026952409069, 170.8479464748525, 141.8942141032894, 173.07016420154832, 107.7095875258965, 173.03868169133784, 109.6556740895976, 117.25597191319684, 148.50370185245993, 142.59542406987748, 118.46621643722756, 155.5620129633462, 149.54364278697176, 145.37301812914666, 179.08255185134476, 144.63176771771396, 178.14770285843406, 138.18062005139655, 158.23695968009997, 192.9404472723254, 193.46677175047807, 151.80927048111334, 182.40204298659228, 190.83327328407904, 205.64918641687836, 179.51875323971035, 212.93038592400262, 238.95440808020066, 237.40040828776546, 249.72854166512843, 239.13739011983853, 190.33303778641857, 231.81306141149253, 248.04296878050081]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVeCsCYdfVA3",
        "outputId": "96c50f63-2800-47b5-e8a2-68a0e56c9e20"
      },
      "source": [
        "# print(Loss_Annealing_Model_Prop)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6220.854539632797, 4357.512228369713, 3560.7143238782883, 3258.6882833838463, 2744.3019897937775, 2443.440160661936, 2063.850209519267, 2008.1231438145041, 1826.5542283058167, 1459.5433863922954, 1317.8277175985277, 1193.670773955062, 913.462082920596, 884.8907753033563, 837.7196136526763, 855.9593238416128, 849.7270208965056, 831.7540355953388, 556.6059640741441, 552.3761584348977, 561.1651882429142, 556.4671341502108, 573.4008204466663, 522.9952387192752, 334.23353291267995, 381.49829956085887, 351.4878318324336, 365.54082414496224, 372.3614286106895, 381.86965760361636, 207.0965942578623, 236.2556466513779, 254.10093355245772, 254.98677541621146, 273.9518919768743, 229.46912388467172, 151.20215218178055, 172.70516892365413, 162.72984167112736, 183.50582504263002, 173.82667379720078, 195.64218519820133, 111.94436782556295, 135.27907845918526, 145.95666329300911, 141.37535232941445, 135.83692460513703, 130.86519827920347, 123.38699456820177, 107.96427598726041, 101.59184595567785, 108.01149839731625, 109.41993228774118, 122.64215594829511, 78.30377630580551, 110.19233638581136, 98.08623543788417, 105.57204419384107, 114.65146975519383, 109.89437316254407, 74.83768240616337, 88.51907770286925, 84.29513670922506, 93.96637365380593, 112.57356677966982, 87.11818689660686, 80.28151254779277, 78.20246818179658, 85.85371770399706, 92.02220574674357, 98.80801807821354, 92.57607371475024, 71.7700077086356, 77.65279292068954, 82.89064036734999, 92.08988713461986, 98.3666644701425, 74.73861632251464, 50.95140570908188, 85.70787763495264, 73.30436932802422, 82.06438989797061, 92.50244645905696, 91.77993676969072, 50.3933720754012, 92.41109845755818, 94.62121629791, 74.47531171015726, 78.49797643410784, 85.72634399271192, 67.20536332319898, 65.63931976171261, 98.01541782727145, 78.14115873169521, 67.03880823390045, 84.33143507710702, 58.05519674954303, 90.63472934488163, 71.504201746935, 90.01595893313697, 83.49910275244656, 71.9475056617066, 69.43028580918872, 76.45588561195655, 85.44832379454783, 108.62902272275733, 66.75146675873293, 79.08281219650235, 60.65163255714833, 73.82813994305434, 67.01901052105404, 82.00114565494187, 82.18534113478745, 98.95208498062675, 53.470144948047164, 81.25247265269354, 70.18512464572814, 75.67947529074445, 89.84495142823289, 92.21660705854447, 73.29837861466785, 84.02562494479025, 91.70471884665221, 73.31107895772493, 82.77153714711676, 117.2709182898925, 68.188322080754, 87.76216780617506, 112.1836749116028, 107.22456512407189, 85.63151786475464, 98.73239184534759, 49.014715941603754, 84.68163477782355, 77.77314811387623, 87.57295956649614, 105.3085109849053, 99.80937882158105, 74.66674253613564, 89.54717735111262, 103.72654193900598, 118.30979176298206, 91.41453034950973, 115.7262379896456, 114.34129017288069, 85.9996365209081, 110.88514298873088, 101.81968516592678, 82.38071673901504, 96.31443170999046, 114.27044929664407, 88.11190371192788, 114.7419846110206, 119.96375622509004, 112.11399588132917, 105.8645587853116, 136.17484177384904, 99.37712556691986, 121.7416490604628, 130.92985379737365, 88.67763160055983, 156.00151549652583, 136.88786180966417, 104.44043026469444, 132.41246499778936, 131.48437380525866, 122.81701343458553, 150.30743624575916, 113.90362379302678, 153.22035992786186, 146.70739963180677, 165.63859604339814, 161.08452643285273, 146.47826912646997, 179.51025893667247, 172.573260606674, 244.20462181983748, 159.60718980350066, 198.42604974567075, 222.87538916931953]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uat6usrfRig9",
        "outputId": "20dc6346-7a9b-428a-ba53-671cef472fc6"
      },
      "source": [
        "# print(Loss_Annealing_Model_linear)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6177.63513982296, 4688.154862642288, 4680.595834970474, 4679.982576608658, 4658.844668149948, 4658.0486072301865, 4656.042840242386, 4643.63847887516, 4316.2015463113785, 3397.0152685046196, 2686.084766983986, 2362.6502960920334, 2049.1474898308516, 1961.7500022128224, 1836.1954694539309, 1706.1916274949908, 1410.197340078652, 1294.495565244928, 935.8000734616071, 935.2391139315441, 917.0666223624721, 881.0626139689703, 842.8628254611976, 859.8692125482485, 572.9436194796581, 588.9477559641236, 575.0998206199147, 571.2292809489882, 590.6300078561762, 563.8484839322045, 344.9473996211309, 366.9364054800826, 351.0408669330063, 350.98355356484535, 368.0944739432307, 371.7355560786091, 217.26770937783294, 237.94814313817187, 225.6872198242927, 229.97781399221276, 226.38184007268865, 255.30620852761422, 158.13169128586014, 152.08564721266885, 156.5181927769081, 158.22156277913746, 142.7848219147927, 196.10055373955765, 101.20282580216008, 118.48277791491637, 127.64937190130513, 138.5585107445586, 119.94276411071041, 138.33026749570308, 91.55920823532142, 92.40652372951445, 96.75875900189385, 110.39135252587221, 108.85871113340545, 98.39381905483788, 98.84404215017639, 87.5679754872217, 100.84720365230896, 108.86526115816196, 74.86446841432189, 92.66226976450162, 57.64340366607303, 58.83306118502378, 90.3379619954544, 98.52031646377418, 77.36405532985918, 89.14934793126986, 74.8988736879046, 65.97922516124618, 66.1665620106246, 62.35047348273065, 79.12087732289592, 72.85881813954097, 61.58828256889683, 81.90510997325441, 71.90153529435156, 47.58087429436682, 78.0397236905419, 66.24192191632073, 43.215270672468705, 79.95293436771863, 54.079227125409886, 64.31783788733583, 45.189542166744445, 70.86203764913421, 57.07770600523645, 53.33786260183928, 58.705637161491836, 55.72556871767142, 85.03063382554734, 41.35735962847096, 44.24429358986359, 64.36356889785372, 55.250241861407744, 48.51156683492218, 64.89965613474362, 53.96448373774069, 49.54399233186007, 59.56849943415051, 56.944738901401934, 51.015272398006005, 70.99246688405688, 50.21085888625265, 46.75048479580755, 54.2827229914007, 36.639378122871754, 68.05356553156841, 43.56316417413052, 38.80045056096367, 62.276073484283955, 57.357326631311025, 38.54367123651751, 56.09508805914635, 45.95696317807554, 40.12403512090032, 37.552757121144865, 46.606351209616605, 43.38335970711903, 57.1403346020862, 57.69798320438849, 37.65050443240108, 50.91794669749464, 40.91329874218684, 47.80590700862838, 35.56011181286689, 62.26911950186067, 60.296114558525005, 53.90827183946806, 35.32124669200397, 45.97817203102146, 52.79760190794855, 44.14802972919429, 36.24118357505637, 46.158250670759735, 52.35076473416137, 65.59874504221717, 38.36403506422826, 52.026226129651434, 55.63734285998555, 42.98443995714682, 48.47104126951373, 57.5269945333753, 30.55949777343774, 68.62660051074175, 42.020057480523576, 53.74721183322811, 50.46618256578725, 47.54393276658186, 59.25862434238397, 47.72942800103719, 55.1973986099635, 54.047229241600434, 53.378003295923214, 63.44727377089839, 56.275737586254905, 41.492042740063724, 50.092185558889746, 52.61138360103709, 45.263400583539095, 74.45151781298333, 40.51303343696617, 40.31838699915379, 61.516358665445466, 91.80051654547991, 74.70624295410455, 54.85348572814631, 61.75123809206548, 59.388727995377735, 78.9581865802405, 90.28083369474689, 99.2082248194613, 87.57844982204324, 79.46288096128512, 83.82231957863922, 80.10519370284533]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}