{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vgg_model_annealing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjtukAPSRxIUVqI65E+Zzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d8e64caa434470d877adef5d03b0b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7afc35d34354d64ae15ba6e42d519bc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_db6cb912d5084100a0fc37322364a3d7",
              "IPY_MODEL_42d029b4ff68481793d080e08e232de9",
              "IPY_MODEL_42df01fdeff34b4fb48ffb080380492a"
            ]
          }
        },
        "a7afc35d34354d64ae15ba6e42d519bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db6cb912d5084100a0fc37322364a3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_66df62e39630430bab7e13ebaa1179c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d00c6e9e8da43319ed504b28e5913ac"
          }
        },
        "42d029b4ff68481793d080e08e232de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a8e5eb7fc0b54d96ba2d50561f16577a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ac43b66de1d4508aac5ee3b07fd6b6d"
          }
        },
        "42df01fdeff34b4fb48ffb080380492a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b45c13473d664536a1ce4a9e960fc28e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 53407497.07it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df9a89efdb034d21a055476816bb0750"
          }
        },
        "66df62e39630430bab7e13ebaa1179c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d00c6e9e8da43319ed504b28e5913ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8e5eb7fc0b54d96ba2d50561f16577a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ac43b66de1d4508aac5ee3b07fd6b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b45c13473d664536a1ce4a9e960fc28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df9a89efdb034d21a055476816bb0750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHU-Peter/COMP6200-Project/blob/main/vgg_model_annealing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZduTHahSHt",
        "outputId": "b42c7920-a35b-4bd3-b667-0742145953ce"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep  2 08:18:23 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KuXXQLqyGR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "0d8e64caa434470d877adef5d03b0b3e",
            "a7afc35d34354d64ae15ba6e42d519bc",
            "db6cb912d5084100a0fc37322364a3d7",
            "42d029b4ff68481793d080e08e232de9",
            "42df01fdeff34b4fb48ffb080380492a",
            "66df62e39630430bab7e13ebaa1179c6",
            "2d00c6e9e8da43319ed504b28e5913ac",
            "a8e5eb7fc0b54d96ba2d50561f16577a",
            "7ac43b66de1d4508aac5ee3b07fd6b6d",
            "b45c13473d664536a1ce4a9e960fc28e",
            "df9a89efdb034d21a055476816bb0750"
          ]
        },
        "id": "g8LYseck5ixy",
        "outputId": "03bdf093-fa2d-43ee-e456-9353498689fd"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True)\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d8e64caa434470d877adef5d03b0b3e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GNeu-VqX-V"
      },
      "source": [
        "def softmax_temperature(logits, temperature):\n",
        "    pro = F.softmax(logits / temperature, dim=-1)\n",
        "    return pro;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkWst_c2qbnq"
      },
      "source": [
        "class AnnealingLookUpTable(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        super(AnnealingLookUpTable, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "    def forward(self, x, temperature):\n",
        "        if self.training:\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          return x @ self.emb.weight\n",
        "        else:\n",
        "          # x = softmax_temperature(x, 0.00001)\n",
        "          # x = mapping_onehot_vector(x)\n",
        "          # return self.emb(x)\n",
        "\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          nozero = torch.nonzero(x);\n",
        "          out = np.zeros((x.shape[0], self.embedding_dim))\n",
        "          out = torch.tensor(out).to(device)\n",
        "          # print(np.array(nozero).shape[1])\n",
        "          for i in range(x.shape[0]):\n",
        "            idx = torch.where(nozero[:,0]==i)[0]\n",
        "            rows = nozero[idx, 1].long()\n",
        "            out[i] = torch.mean(self.emb(rows), axis=0)\n",
        "          return out.float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZV7QMU0gg5_"
      },
      "source": [
        "class Annealing_VGG_LookUpTable(nn.Module):\n",
        "  def __init__(self, originalModel):\n",
        "    super(Annealing_VGG_LookUpTable, self).__init__()\n",
        "    # self.features = originalModel.features\n",
        "    # self.avgpool = originalModel.avgpool\n",
        "    self.vgg = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "    # self.classifier = originalModel.classifier\n",
        "    self.look_up_table = AnnealingLookUpTable(25088, 4096)\n",
        "    self.classifier = nn.Sequential(*list(originalModel.classifier.children())[1:])\n",
        "    \n",
        "  def forward(self, x, temperature):\n",
        "    # x = self.features(x)\n",
        "    # x = self.avgpool(x)\n",
        "    x = self.vgg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.look_up_table(x, temperature)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vnrDliORL8T",
        "outputId": "8761a235-1833-4bb1-88ef-420a20586775"
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_exp = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, math.exp(-1.4 * math.pow(10, -5) * idx)), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            Loss_Annealing_Model_exp.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 3.14771\n",
            "[1,  4000] loss: 2.34918\n",
            "[1,  6000] loss: 2.34055\n",
            "[1,  8000] loss: 2.33426\n",
            "[1, 10000] loss: 2.33270\n",
            "[1, 12000] loss: 2.32458\n",
            "[2,  2000] loss: 2.32285\n",
            "[2,  4000] loss: 2.32143\n",
            "[2,  6000] loss: 2.32133\n",
            "[2,  8000] loss: 2.31889\n",
            "[2, 10000] loss: 2.31556\n",
            "[2, 12000] loss: 2.31515\n",
            "[3,  2000] loss: 2.31481\n",
            "[3,  4000] loss: 2.31354\n",
            "[3,  6000] loss: 2.31333\n",
            "[3,  8000] loss: 2.31175\n",
            "[3, 10000] loss: 2.31311\n",
            "[3, 12000] loss: 2.31000\n",
            "[4,  2000] loss: 2.30645\n",
            "[4,  4000] loss: 2.31018\n",
            "[4,  6000] loss: 2.30920\n",
            "[4,  8000] loss: 2.30883\n",
            "[4, 10000] loss: 2.30924\n",
            "[4, 12000] loss: 2.30744\n",
            "[5,  2000] loss: 2.30837\n",
            "[5,  4000] loss: 2.30736\n",
            "[5,  6000] loss: 2.17031\n",
            "[5,  8000] loss: 1.70402\n",
            "[5, 10000] loss: 1.43958\n",
            "[5, 12000] loss: 1.20283\n",
            "[6,  2000] loss: 1.06267\n",
            "[6,  4000] loss: 1.00648\n",
            "[6,  6000] loss: 0.93727\n",
            "[6,  8000] loss: 0.85255\n",
            "[6, 10000] loss: 0.73028\n",
            "[6, 12000] loss: 0.65753\n",
            "[7,  2000] loss: 0.49543\n",
            "[7,  4000] loss: 0.48776\n",
            "[7,  6000] loss: 0.47412\n",
            "[7,  8000] loss: 0.45261\n",
            "[7, 10000] loss: 0.42338\n",
            "[7, 12000] loss: 0.42928\n",
            "[8,  2000] loss: 0.28663\n",
            "[8,  4000] loss: 0.29418\n",
            "[8,  6000] loss: 0.27105\n",
            "[8,  8000] loss: 0.27416\n",
            "[8, 10000] loss: 0.27917\n",
            "[8, 12000] loss: 0.27838\n",
            "[9,  2000] loss: 0.15551\n",
            "[9,  4000] loss: 0.16458\n",
            "[9,  6000] loss: 0.16815\n",
            "[9,  8000] loss: 0.18282\n",
            "[9, 10000] loss: 0.17885\n",
            "[9, 12000] loss: 0.18761\n",
            "[10,  2000] loss: 0.09848\n",
            "[10,  4000] loss: 0.09353\n",
            "[10,  6000] loss: 0.11200\n",
            "[10,  8000] loss: 0.11383\n",
            "[10, 10000] loss: 0.12538\n",
            "[10, 12000] loss: 0.12250\n",
            "[11,  2000] loss: 0.07363\n",
            "[11,  4000] loss: 0.07780\n",
            "[11,  6000] loss: 0.07150\n",
            "[11,  8000] loss: 0.09052\n",
            "[11, 10000] loss: 0.08911\n",
            "[11, 12000] loss: 0.08243\n",
            "[12,  2000] loss: 0.05779\n",
            "[12,  4000] loss: 0.06441\n",
            "[12,  6000] loss: 0.06753\n",
            "[12,  8000] loss: 0.05803\n",
            "[12, 10000] loss: 0.07258\n",
            "[12, 12000] loss: 0.06971\n",
            "[13,  2000] loss: 0.04093\n",
            "[13,  4000] loss: 0.05925\n",
            "[13,  6000] loss: 0.05913\n",
            "[13,  8000] loss: 0.05603\n",
            "[13, 10000] loss: 0.06373\n",
            "[13, 12000] loss: 0.04908\n",
            "[14,  2000] loss: 0.04481\n",
            "[14,  4000] loss: 0.04525\n",
            "[14,  6000] loss: 0.05936\n",
            "[14,  8000] loss: 0.05200\n",
            "[14, 10000] loss: 0.05563\n",
            "[14, 12000] loss: 0.05640\n",
            "[15,  2000] loss: 0.04542\n",
            "[15,  4000] loss: 0.04969\n",
            "[15,  6000] loss: 0.04719\n",
            "[15,  8000] loss: 0.04928\n",
            "[15, 10000] loss: 0.05011\n",
            "[15, 12000] loss: 0.05640\n",
            "[16,  2000] loss: 0.03715\n",
            "[16,  4000] loss: 0.05121\n",
            "[16,  6000] loss: 0.05006\n",
            "[16,  8000] loss: 0.04664\n",
            "[16, 10000] loss: 0.05237\n",
            "[16, 12000] loss: 0.05032\n",
            "[17,  2000] loss: 0.05668\n",
            "[17,  4000] loss: 0.04315\n",
            "[17,  6000] loss: 0.05124\n",
            "[17,  8000] loss: 0.04669\n",
            "[17, 10000] loss: 0.06429\n",
            "[17, 12000] loss: 0.04170\n",
            "[18,  2000] loss: 0.03671\n",
            "[18,  4000] loss: 0.05108\n",
            "[18,  6000] loss: 0.04173\n",
            "[18,  8000] loss: 0.05636\n",
            "[18, 10000] loss: 0.04257\n",
            "[18, 12000] loss: 0.05765\n",
            "[19,  2000] loss: 0.04866\n",
            "[19,  4000] loss: 0.05575\n",
            "[19,  6000] loss: 0.04480\n",
            "[19,  8000] loss: 0.05848\n",
            "[19, 10000] loss: 0.04792\n",
            "[19, 12000] loss: 0.05148\n",
            "[20,  2000] loss: 0.04262\n",
            "[20,  4000] loss: 0.05712\n",
            "[20,  6000] loss: 0.05461\n",
            "[20,  8000] loss: 0.05561\n",
            "[20, 10000] loss: 0.03552\n",
            "[20, 12000] loss: 0.06092\n",
            "[21,  2000] loss: 0.03295\n",
            "[21,  4000] loss: 0.05742\n",
            "[21,  6000] loss: 0.05444\n",
            "[21,  8000] loss: 0.04413\n",
            "[21, 10000] loss: 0.05973\n",
            "[21, 12000] loss: 0.04918\n",
            "[22,  2000] loss: 0.04177\n",
            "[22,  4000] loss: 0.06180\n",
            "[22,  6000] loss: 0.05844\n",
            "[22,  8000] loss: 0.04958\n",
            "[22, 10000] loss: 0.05711\n",
            "[22, 12000] loss: 0.05269\n",
            "[23,  2000] loss: 0.05253\n",
            "[23,  4000] loss: 0.06666\n",
            "[23,  6000] loss: 0.06838\n",
            "[23,  8000] loss: 0.06369\n",
            "[23, 10000] loss: 0.04951\n",
            "[23, 12000] loss: 0.05849\n",
            "[24,  2000] loss: 0.05533\n",
            "[24,  4000] loss: 0.05854\n",
            "[24,  6000] loss: 0.05089\n",
            "[24,  8000] loss: 0.06776\n",
            "[24, 10000] loss: 0.07840\n",
            "[24, 12000] loss: 0.06408\n",
            "[25,  2000] loss: 0.06820\n",
            "[25,  4000] loss: 0.06267\n",
            "[25,  6000] loss: 0.06360\n",
            "[25,  8000] loss: 0.08542\n",
            "[25, 10000] loss: 0.07095\n",
            "[25, 12000] loss: 0.08654\n",
            "[26,  2000] loss: 0.05385\n",
            "[26,  4000] loss: 0.08652\n",
            "[26,  6000] loss: 0.05483\n",
            "[26,  8000] loss: 0.05863\n",
            "[26, 10000] loss: 0.07425\n",
            "[26, 12000] loss: 0.07130\n",
            "[27,  2000] loss: 0.05923\n",
            "[27,  4000] loss: 0.07778\n",
            "[27,  6000] loss: 0.07477\n",
            "[27,  8000] loss: 0.07269\n",
            "[27, 10000] loss: 0.08954\n",
            "[27, 12000] loss: 0.07232\n",
            "[28,  2000] loss: 0.08907\n",
            "[28,  4000] loss: 0.06909\n",
            "[28,  6000] loss: 0.07912\n",
            "[28,  8000] loss: 0.09647\n",
            "[28, 10000] loss: 0.09673\n",
            "[28, 12000] loss: 0.07590\n",
            "[29,  2000] loss: 0.09120\n",
            "[29,  4000] loss: 0.09542\n",
            "[29,  6000] loss: 0.10282\n",
            "[29,  8000] loss: 0.08976\n",
            "[29, 10000] loss: 0.10647\n",
            "[29, 12000] loss: 0.11948\n",
            "[30,  2000] loss: 0.11870\n",
            "[30,  4000] loss: 0.12486\n",
            "[30,  6000] loss: 0.11957\n",
            "[30,  8000] loss: 0.09517\n",
            "[30, 10000] loss: 0.11591\n",
            "[30, 12000] loss: 0.12402\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_EzoAd4iJJ0"
      },
      "source": [
        "torch.save(Annealing_lut_model.state_dict(), \"./Annealing_lut_model.weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw-D7WrHgrsB",
        "outputId": "33264020-8469-46e7-e331-706c823027b2"
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_Prop = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, max(0.0001, math.pow(0.999859, idx/10))), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            Loss_Annealing_Model_Prop.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n",
            "[1,  2000] loss: 3.11043\n",
            "[1,  4000] loss: 2.17876\n",
            "[1,  6000] loss: 1.78036\n",
            "[1,  8000] loss: 1.62934\n",
            "[1, 10000] loss: 1.37215\n",
            "[1, 12000] loss: 1.22172\n",
            "[2,  2000] loss: 1.03193\n",
            "[2,  4000] loss: 1.00406\n",
            "[2,  6000] loss: 0.91328\n",
            "[2,  8000] loss: 0.72977\n",
            "[2, 10000] loss: 0.65891\n",
            "[2, 12000] loss: 0.59684\n",
            "[3,  2000] loss: 0.45673\n",
            "[3,  4000] loss: 0.44245\n",
            "[3,  6000] loss: 0.41886\n",
            "[3,  8000] loss: 0.42798\n",
            "[3, 10000] loss: 0.42486\n",
            "[3, 12000] loss: 0.41588\n",
            "[4,  2000] loss: 0.27830\n",
            "[4,  4000] loss: 0.27619\n",
            "[4,  6000] loss: 0.28058\n",
            "[4,  8000] loss: 0.27823\n",
            "[4, 10000] loss: 0.28670\n",
            "[4, 12000] loss: 0.26150\n",
            "[5,  2000] loss: 0.16712\n",
            "[5,  4000] loss: 0.19075\n",
            "[5,  6000] loss: 0.17574\n",
            "[5,  8000] loss: 0.18277\n",
            "[5, 10000] loss: 0.18618\n",
            "[5, 12000] loss: 0.19093\n",
            "[6,  2000] loss: 0.10355\n",
            "[6,  4000] loss: 0.11813\n",
            "[6,  6000] loss: 0.12705\n",
            "[6,  8000] loss: 0.12749\n",
            "[6, 10000] loss: 0.13698\n",
            "[6, 12000] loss: 0.11473\n",
            "[7,  2000] loss: 0.07560\n",
            "[7,  4000] loss: 0.08635\n",
            "[7,  6000] loss: 0.08136\n",
            "[7,  8000] loss: 0.09175\n",
            "[7, 10000] loss: 0.08691\n",
            "[7, 12000] loss: 0.09782\n",
            "[8,  2000] loss: 0.05597\n",
            "[8,  4000] loss: 0.06764\n",
            "[8,  6000] loss: 0.07298\n",
            "[8,  8000] loss: 0.07069\n",
            "[8, 10000] loss: 0.06792\n",
            "[8, 12000] loss: 0.06543\n",
            "[9,  2000] loss: 0.06169\n",
            "[9,  4000] loss: 0.05398\n",
            "[9,  6000] loss: 0.05080\n",
            "[9,  8000] loss: 0.05401\n",
            "[9, 10000] loss: 0.05471\n",
            "[9, 12000] loss: 0.06132\n",
            "[10,  2000] loss: 0.03915\n",
            "[10,  4000] loss: 0.05510\n",
            "[10,  6000] loss: 0.04904\n",
            "[10,  8000] loss: 0.05279\n",
            "[10, 10000] loss: 0.05733\n",
            "[10, 12000] loss: 0.05495\n",
            "[11,  2000] loss: 0.03742\n",
            "[11,  4000] loss: 0.04426\n",
            "[11,  6000] loss: 0.04215\n",
            "[11,  8000] loss: 0.04698\n",
            "[11, 10000] loss: 0.05629\n",
            "[11, 12000] loss: 0.04356\n",
            "[12,  2000] loss: 0.04014\n",
            "[12,  4000] loss: 0.03910\n",
            "[12,  6000] loss: 0.04293\n",
            "[12,  8000] loss: 0.04601\n",
            "[12, 10000] loss: 0.04940\n",
            "[12, 12000] loss: 0.04629\n",
            "[13,  2000] loss: 0.03589\n",
            "[13,  4000] loss: 0.03883\n",
            "[13,  6000] loss: 0.04145\n",
            "[13,  8000] loss: 0.04604\n",
            "[13, 10000] loss: 0.04918\n",
            "[13, 12000] loss: 0.03737\n",
            "[14,  2000] loss: 0.02548\n",
            "[14,  4000] loss: 0.04285\n",
            "[14,  6000] loss: 0.03665\n",
            "[14,  8000] loss: 0.04103\n",
            "[14, 10000] loss: 0.04625\n",
            "[14, 12000] loss: 0.04589\n",
            "[15,  2000] loss: 0.02520\n",
            "[15,  4000] loss: 0.04621\n",
            "[15,  6000] loss: 0.04731\n",
            "[15,  8000] loss: 0.03724\n",
            "[15, 10000] loss: 0.03925\n",
            "[15, 12000] loss: 0.04286\n",
            "[16,  2000] loss: 0.03360\n",
            "[16,  4000] loss: 0.03282\n",
            "[16,  6000] loss: 0.04901\n",
            "[16,  8000] loss: 0.03907\n",
            "[16, 10000] loss: 0.03352\n",
            "[16, 12000] loss: 0.04217\n",
            "[17,  2000] loss: 0.02903\n",
            "[17,  4000] loss: 0.04532\n",
            "[17,  6000] loss: 0.03575\n",
            "[17,  8000] loss: 0.04501\n",
            "[17, 10000] loss: 0.04175\n",
            "[17, 12000] loss: 0.03597\n",
            "[18,  2000] loss: 0.03472\n",
            "[18,  4000] loss: 0.03823\n",
            "[18,  6000] loss: 0.04272\n",
            "[18,  8000] loss: 0.05431\n",
            "[18, 10000] loss: 0.03338\n",
            "[18, 12000] loss: 0.03954\n",
            "[19,  2000] loss: 0.03033\n",
            "[19,  4000] loss: 0.03691\n",
            "[19,  6000] loss: 0.03351\n",
            "[19,  8000] loss: 0.04100\n",
            "[19, 10000] loss: 0.04109\n",
            "[19, 12000] loss: 0.04948\n",
            "[20,  2000] loss: 0.02674\n",
            "[20,  4000] loss: 0.04063\n",
            "[20,  6000] loss: 0.03509\n",
            "[20,  8000] loss: 0.03784\n",
            "[20, 10000] loss: 0.04492\n",
            "[20, 12000] loss: 0.04611\n",
            "[21,  2000] loss: 0.03665\n",
            "[21,  4000] loss: 0.04201\n",
            "[21,  6000] loss: 0.04585\n",
            "[21,  8000] loss: 0.03666\n",
            "[21, 10000] loss: 0.04139\n",
            "[21, 12000] loss: 0.05864\n",
            "[22,  2000] loss: 0.03409\n",
            "[22,  4000] loss: 0.04388\n",
            "[22,  6000] loss: 0.05609\n",
            "[22,  8000] loss: 0.05361\n",
            "[22, 10000] loss: 0.04282\n",
            "[22, 12000] loss: 0.04937\n",
            "[23,  2000] loss: 0.02451\n",
            "[23,  4000] loss: 0.04234\n",
            "[23,  6000] loss: 0.03889\n",
            "[23,  8000] loss: 0.04379\n",
            "[23, 10000] loss: 0.05265\n",
            "[23, 12000] loss: 0.04990\n",
            "[24,  2000] loss: 0.03733\n",
            "[24,  4000] loss: 0.04477\n",
            "[24,  6000] loss: 0.05186\n",
            "[24,  8000] loss: 0.05915\n",
            "[24, 10000] loss: 0.04571\n",
            "[24, 12000] loss: 0.05786\n",
            "[25,  2000] loss: 0.05717\n",
            "[25,  4000] loss: 0.04300\n",
            "[25,  6000] loss: 0.05544\n",
            "[25,  8000] loss: 0.05091\n",
            "[25, 10000] loss: 0.04119\n",
            "[25, 12000] loss: 0.04816\n",
            "[26,  2000] loss: 0.05714\n",
            "[26,  4000] loss: 0.04406\n",
            "[26,  6000] loss: 0.05737\n",
            "[26,  8000] loss: 0.05998\n",
            "[26, 10000] loss: 0.05606\n",
            "[26, 12000] loss: 0.05293\n",
            "[27,  2000] loss: 0.06809\n",
            "[27,  4000] loss: 0.04969\n",
            "[27,  6000] loss: 0.06087\n",
            "[27,  8000] loss: 0.06546\n",
            "[27, 10000] loss: 0.04434\n",
            "[27, 12000] loss: 0.07800\n",
            "[28,  2000] loss: 0.06844\n",
            "[28,  4000] loss: 0.05222\n",
            "[28,  6000] loss: 0.06621\n",
            "[28,  8000] loss: 0.06574\n",
            "[28, 10000] loss: 0.06141\n",
            "[28, 12000] loss: 0.07515\n",
            "[29,  2000] loss: 0.05695\n",
            "[29,  4000] loss: 0.07661\n",
            "[29,  6000] loss: 0.07335\n",
            "[29,  8000] loss: 0.08282\n",
            "[29, 10000] loss: 0.08054\n",
            "[29, 12000] loss: 0.07324\n",
            "[30,  2000] loss: 0.08976\n",
            "[30,  4000] loss: 0.08629\n",
            "[30,  6000] loss: 0.12210\n",
            "[30,  8000] loss: 0.07980\n",
            "[30, 10000] loss: 0.09921\n",
            "[30, 12000] loss: 0.11144\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx8D7n6Rgwj6",
        "outputId": "beeefa27-880f-4705-9ad0-e84e506bf0cb"
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_linear = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, 0.9999 - 0.033*epoch), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            print('[%d, %5d] temperature: %.5f' %\n",
        "                  (epoch + 1, i + 1, max(0.0001, 0.9999 - 0.033*epoch)))\n",
        "            Loss_Annealing_Model_linear.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n",
            "[1,  2000] loss: 3.08882\n",
            "[1,  2000] temperature: 0.99990\n",
            "[1,  4000] loss: 2.34408\n",
            "[1,  4000] temperature: 0.99990\n",
            "[1,  6000] loss: 2.34030\n",
            "[1,  6000] temperature: 0.99990\n",
            "[1,  8000] loss: 2.33999\n",
            "[1,  8000] temperature: 0.99990\n",
            "[1, 10000] loss: 2.32942\n",
            "[1, 10000] temperature: 0.99990\n",
            "[1, 12000] loss: 2.32902\n",
            "[1, 12000] temperature: 0.99990\n",
            "[2,  2000] loss: 2.32802\n",
            "[2,  2000] temperature: 0.96690\n",
            "[2,  4000] loss: 2.32182\n",
            "[2,  4000] temperature: 0.96690\n",
            "[2,  6000] loss: 2.15810\n",
            "[2,  6000] temperature: 0.96690\n",
            "[2,  8000] loss: 1.69851\n",
            "[2,  8000] temperature: 0.96690\n",
            "[2, 10000] loss: 1.34304\n",
            "[2, 10000] temperature: 0.96690\n",
            "[2, 12000] loss: 1.18133\n",
            "[2, 12000] temperature: 0.96690\n",
            "[3,  2000] loss: 1.02457\n",
            "[3,  2000] temperature: 0.93390\n",
            "[3,  4000] loss: 0.98088\n",
            "[3,  4000] temperature: 0.93390\n",
            "[3,  6000] loss: 0.91810\n",
            "[3,  6000] temperature: 0.93390\n",
            "[3,  8000] loss: 0.85310\n",
            "[3,  8000] temperature: 0.93390\n",
            "[3, 10000] loss: 0.70510\n",
            "[3, 10000] temperature: 0.93390\n",
            "[3, 12000] loss: 0.64725\n",
            "[3, 12000] temperature: 0.93390\n",
            "[4,  2000] loss: 0.46790\n",
            "[4,  2000] temperature: 0.90090\n",
            "[4,  4000] loss: 0.46762\n",
            "[4,  4000] temperature: 0.90090\n",
            "[4,  6000] loss: 0.45853\n",
            "[4,  6000] temperature: 0.90090\n",
            "[4,  8000] loss: 0.44053\n",
            "[4,  8000] temperature: 0.90090\n",
            "[4, 10000] loss: 0.42143\n",
            "[4, 10000] temperature: 0.90090\n",
            "[4, 12000] loss: 0.42993\n",
            "[4, 12000] temperature: 0.90090\n",
            "[5,  2000] loss: 0.28647\n",
            "[5,  2000] temperature: 0.86790\n",
            "[5,  4000] loss: 0.29447\n",
            "[5,  4000] temperature: 0.86790\n",
            "[5,  6000] loss: 0.28755\n",
            "[5,  6000] temperature: 0.86790\n",
            "[5,  8000] loss: 0.28561\n",
            "[5,  8000] temperature: 0.86790\n",
            "[5, 10000] loss: 0.29532\n",
            "[5, 10000] temperature: 0.86790\n",
            "[5, 12000] loss: 0.28192\n",
            "[5, 12000] temperature: 0.86790\n",
            "[6,  2000] loss: 0.17247\n",
            "[6,  2000] temperature: 0.83490\n",
            "[6,  4000] loss: 0.18347\n",
            "[6,  4000] temperature: 0.83490\n",
            "[6,  6000] loss: 0.17552\n",
            "[6,  6000] temperature: 0.83490\n",
            "[6,  8000] loss: 0.17549\n",
            "[6,  8000] temperature: 0.83490\n",
            "[6, 10000] loss: 0.18405\n",
            "[6, 10000] temperature: 0.83490\n",
            "[6, 12000] loss: 0.18587\n",
            "[6, 12000] temperature: 0.83490\n",
            "[7,  2000] loss: 0.10863\n",
            "[7,  2000] temperature: 0.80190\n",
            "[7,  4000] loss: 0.11897\n",
            "[7,  4000] temperature: 0.80190\n",
            "[7,  6000] loss: 0.11284\n",
            "[7,  6000] temperature: 0.80190\n",
            "[7,  8000] loss: 0.11499\n",
            "[7,  8000] temperature: 0.80190\n",
            "[7, 10000] loss: 0.11319\n",
            "[7, 10000] temperature: 0.80190\n",
            "[7, 12000] loss: 0.12765\n",
            "[7, 12000] temperature: 0.80190\n",
            "[8,  2000] loss: 0.07907\n",
            "[8,  2000] temperature: 0.76890\n",
            "[8,  4000] loss: 0.07604\n",
            "[8,  4000] temperature: 0.76890\n",
            "[8,  6000] loss: 0.07826\n",
            "[8,  6000] temperature: 0.76890\n",
            "[8,  8000] loss: 0.07911\n",
            "[8,  8000] temperature: 0.76890\n",
            "[8, 10000] loss: 0.07139\n",
            "[8, 10000] temperature: 0.76890\n",
            "[8, 12000] loss: 0.09805\n",
            "[8, 12000] temperature: 0.76890\n",
            "[9,  2000] loss: 0.05060\n",
            "[9,  2000] temperature: 0.73590\n",
            "[9,  4000] loss: 0.05924\n",
            "[9,  4000] temperature: 0.73590\n",
            "[9,  6000] loss: 0.06382\n",
            "[9,  6000] temperature: 0.73590\n",
            "[9,  8000] loss: 0.06928\n",
            "[9,  8000] temperature: 0.73590\n",
            "[9, 10000] loss: 0.05997\n",
            "[9, 10000] temperature: 0.73590\n",
            "[9, 12000] loss: 0.06917\n",
            "[9, 12000] temperature: 0.73590\n",
            "[10,  2000] loss: 0.04578\n",
            "[10,  2000] temperature: 0.70290\n",
            "[10,  4000] loss: 0.04620\n",
            "[10,  4000] temperature: 0.70290\n",
            "[10,  6000] loss: 0.04838\n",
            "[10,  6000] temperature: 0.70290\n",
            "[10,  8000] loss: 0.05520\n",
            "[10,  8000] temperature: 0.70290\n",
            "[10, 10000] loss: 0.05443\n",
            "[10, 10000] temperature: 0.70290\n",
            "[10, 12000] loss: 0.04920\n",
            "[10, 12000] temperature: 0.70290\n",
            "[11,  2000] loss: 0.04942\n",
            "[11,  2000] temperature: 0.66990\n",
            "[11,  4000] loss: 0.04378\n",
            "[11,  4000] temperature: 0.66990\n",
            "[11,  6000] loss: 0.05042\n",
            "[11,  6000] temperature: 0.66990\n",
            "[11,  8000] loss: 0.05443\n",
            "[11,  8000] temperature: 0.66990\n",
            "[11, 10000] loss: 0.03743\n",
            "[11, 10000] temperature: 0.66990\n",
            "[11, 12000] loss: 0.04633\n",
            "[11, 12000] temperature: 0.66990\n",
            "[12,  2000] loss: 0.02882\n",
            "[12,  2000] temperature: 0.63690\n",
            "[12,  4000] loss: 0.02942\n",
            "[12,  4000] temperature: 0.63690\n",
            "[12,  6000] loss: 0.04517\n",
            "[12,  6000] temperature: 0.63690\n",
            "[12,  8000] loss: 0.04926\n",
            "[12,  8000] temperature: 0.63690\n",
            "[12, 10000] loss: 0.03868\n",
            "[12, 10000] temperature: 0.63690\n",
            "[12, 12000] loss: 0.04457\n",
            "[12, 12000] temperature: 0.63690\n",
            "[13,  2000] loss: 0.03745\n",
            "[13,  2000] temperature: 0.60390\n",
            "[13,  4000] loss: 0.03299\n",
            "[13,  4000] temperature: 0.60390\n",
            "[13,  6000] loss: 0.03308\n",
            "[13,  6000] temperature: 0.60390\n",
            "[13,  8000] loss: 0.03118\n",
            "[13,  8000] temperature: 0.60390\n",
            "[13, 10000] loss: 0.03956\n",
            "[13, 10000] temperature: 0.60390\n",
            "[13, 12000] loss: 0.03643\n",
            "[13, 12000] temperature: 0.60390\n",
            "[14,  2000] loss: 0.03079\n",
            "[14,  2000] temperature: 0.57090\n",
            "[14,  4000] loss: 0.04095\n",
            "[14,  4000] temperature: 0.57090\n",
            "[14,  6000] loss: 0.03595\n",
            "[14,  6000] temperature: 0.57090\n",
            "[14,  8000] loss: 0.02379\n",
            "[14,  8000] temperature: 0.57090\n",
            "[14, 10000] loss: 0.03902\n",
            "[14, 10000] temperature: 0.57090\n",
            "[14, 12000] loss: 0.03312\n",
            "[14, 12000] temperature: 0.57090\n",
            "[15,  2000] loss: 0.02161\n",
            "[15,  2000] temperature: 0.53790\n",
            "[15,  4000] loss: 0.03998\n",
            "[15,  4000] temperature: 0.53790\n",
            "[15,  6000] loss: 0.02704\n",
            "[15,  6000] temperature: 0.53790\n",
            "[15,  8000] loss: 0.03216\n",
            "[15,  8000] temperature: 0.53790\n",
            "[15, 10000] loss: 0.02259\n",
            "[15, 10000] temperature: 0.53790\n",
            "[15, 12000] loss: 0.03543\n",
            "[15, 12000] temperature: 0.53790\n",
            "[16,  2000] loss: 0.02854\n",
            "[16,  2000] temperature: 0.50490\n",
            "[16,  4000] loss: 0.02667\n",
            "[16,  4000] temperature: 0.50490\n",
            "[16,  6000] loss: 0.02935\n",
            "[16,  6000] temperature: 0.50490\n",
            "[16,  8000] loss: 0.02786\n",
            "[16,  8000] temperature: 0.50490\n",
            "[16, 10000] loss: 0.04252\n",
            "[16, 10000] temperature: 0.50490\n",
            "[16, 12000] loss: 0.02068\n",
            "[16, 12000] temperature: 0.50490\n",
            "[17,  2000] loss: 0.02212\n",
            "[17,  2000] temperature: 0.47190\n",
            "[17,  4000] loss: 0.03218\n",
            "[17,  4000] temperature: 0.47190\n",
            "[17,  6000] loss: 0.02763\n",
            "[17,  6000] temperature: 0.47190\n",
            "[17,  8000] loss: 0.02426\n",
            "[17,  8000] temperature: 0.47190\n",
            "[17, 10000] loss: 0.03245\n",
            "[17, 10000] temperature: 0.47190\n",
            "[17, 12000] loss: 0.02698\n",
            "[17, 12000] temperature: 0.47190\n",
            "[18,  2000] loss: 0.02477\n",
            "[18,  2000] temperature: 0.43890\n",
            "[18,  4000] loss: 0.02978\n",
            "[18,  4000] temperature: 0.43890\n",
            "[18,  6000] loss: 0.02847\n",
            "[18,  6000] temperature: 0.43890\n",
            "[18,  8000] loss: 0.02551\n",
            "[18,  8000] temperature: 0.43890\n",
            "[18, 10000] loss: 0.03550\n",
            "[18, 10000] temperature: 0.43890\n",
            "[18, 12000] loss: 0.02511\n",
            "[18, 12000] temperature: 0.43890\n",
            "[19,  2000] loss: 0.02338\n",
            "[19,  2000] temperature: 0.40590\n",
            "[19,  4000] loss: 0.02714\n",
            "[19,  4000] temperature: 0.40590\n",
            "[19,  6000] loss: 0.01832\n",
            "[19,  6000] temperature: 0.40590\n",
            "[19,  8000] loss: 0.03403\n",
            "[19,  8000] temperature: 0.40590\n",
            "[19, 10000] loss: 0.02178\n",
            "[19, 10000] temperature: 0.40590\n",
            "[19, 12000] loss: 0.01940\n",
            "[19, 12000] temperature: 0.40590\n",
            "[20,  2000] loss: 0.03114\n",
            "[20,  2000] temperature: 0.37290\n",
            "[20,  4000] loss: 0.02868\n",
            "[20,  4000] temperature: 0.37290\n",
            "[20,  6000] loss: 0.01927\n",
            "[20,  6000] temperature: 0.37290\n",
            "[20,  8000] loss: 0.02805\n",
            "[20,  8000] temperature: 0.37290\n",
            "[20, 10000] loss: 0.02298\n",
            "[20, 10000] temperature: 0.37290\n",
            "[20, 12000] loss: 0.02006\n",
            "[20, 12000] temperature: 0.37290\n",
            "[21,  2000] loss: 0.01878\n",
            "[21,  2000] temperature: 0.33990\n",
            "[21,  4000] loss: 0.02330\n",
            "[21,  4000] temperature: 0.33990\n",
            "[21,  6000] loss: 0.02169\n",
            "[21,  6000] temperature: 0.33990\n",
            "[21,  8000] loss: 0.02857\n",
            "[21,  8000] temperature: 0.33990\n",
            "[21, 10000] loss: 0.02885\n",
            "[21, 10000] temperature: 0.33990\n",
            "[21, 12000] loss: 0.01883\n",
            "[21, 12000] temperature: 0.33990\n",
            "[22,  2000] loss: 0.02546\n",
            "[22,  2000] temperature: 0.30690\n",
            "[22,  4000] loss: 0.02046\n",
            "[22,  4000] temperature: 0.30690\n",
            "[22,  6000] loss: 0.02390\n",
            "[22,  6000] temperature: 0.30690\n",
            "[22,  8000] loss: 0.01778\n",
            "[22,  8000] temperature: 0.30690\n",
            "[22, 10000] loss: 0.03113\n",
            "[22, 10000] temperature: 0.30690\n",
            "[22, 12000] loss: 0.03015\n",
            "[22, 12000] temperature: 0.30690\n",
            "[23,  2000] loss: 0.02695\n",
            "[23,  2000] temperature: 0.27390\n",
            "[23,  4000] loss: 0.01766\n",
            "[23,  4000] temperature: 0.27390\n",
            "[23,  6000] loss: 0.02299\n",
            "[23,  6000] temperature: 0.27390\n",
            "[23,  8000] loss: 0.02640\n",
            "[23,  8000] temperature: 0.27390\n",
            "[23, 10000] loss: 0.02207\n",
            "[23, 10000] temperature: 0.27390\n",
            "[23, 12000] loss: 0.01812\n",
            "[23, 12000] temperature: 0.27390\n",
            "[24,  2000] loss: 0.02308\n",
            "[24,  2000] temperature: 0.24090\n",
            "[24,  4000] loss: 0.02618\n",
            "[24,  4000] temperature: 0.24090\n",
            "[24,  6000] loss: 0.03280\n",
            "[24,  6000] temperature: 0.24090\n",
            "[24,  8000] loss: 0.01918\n",
            "[24,  8000] temperature: 0.24090\n",
            "[24, 10000] loss: 0.02601\n",
            "[24, 10000] temperature: 0.24090\n",
            "[24, 12000] loss: 0.02782\n",
            "[24, 12000] temperature: 0.24090\n",
            "[25,  2000] loss: 0.02149\n",
            "[25,  2000] temperature: 0.20790\n",
            "[25,  4000] loss: 0.02424\n",
            "[25,  4000] temperature: 0.20790\n",
            "[25,  6000] loss: 0.02876\n",
            "[25,  6000] temperature: 0.20790\n",
            "[25,  8000] loss: 0.01528\n",
            "[25,  8000] temperature: 0.20790\n",
            "[25, 10000] loss: 0.03431\n",
            "[25, 10000] temperature: 0.20790\n",
            "[25, 12000] loss: 0.02101\n",
            "[25, 12000] temperature: 0.20790\n",
            "[26,  2000] loss: 0.02687\n",
            "[26,  2000] temperature: 0.17490\n",
            "[26,  4000] loss: 0.02523\n",
            "[26,  4000] temperature: 0.17490\n",
            "[26,  6000] loss: 0.02377\n",
            "[26,  6000] temperature: 0.17490\n",
            "[26,  8000] loss: 0.02963\n",
            "[26,  8000] temperature: 0.17490\n",
            "[26, 10000] loss: 0.02386\n",
            "[26, 10000] temperature: 0.17490\n",
            "[26, 12000] loss: 0.02760\n",
            "[26, 12000] temperature: 0.17490\n",
            "[27,  2000] loss: 0.02702\n",
            "[27,  2000] temperature: 0.14190\n",
            "[27,  4000] loss: 0.02669\n",
            "[27,  4000] temperature: 0.14190\n",
            "[27,  6000] loss: 0.03172\n",
            "[27,  6000] temperature: 0.14190\n",
            "[27,  8000] loss: 0.02814\n",
            "[27,  8000] temperature: 0.14190\n",
            "[27, 10000] loss: 0.02075\n",
            "[27, 10000] temperature: 0.14190\n",
            "[27, 12000] loss: 0.02505\n",
            "[27, 12000] temperature: 0.14190\n",
            "[28,  2000] loss: 0.02631\n",
            "[28,  2000] temperature: 0.10890\n",
            "[28,  4000] loss: 0.02263\n",
            "[28,  4000] temperature: 0.10890\n",
            "[28,  6000] loss: 0.03723\n",
            "[28,  6000] temperature: 0.10890\n",
            "[28,  8000] loss: 0.02026\n",
            "[28,  8000] temperature: 0.10890\n",
            "[28, 10000] loss: 0.02016\n",
            "[28, 10000] temperature: 0.10890\n",
            "[28, 12000] loss: 0.03076\n",
            "[28, 12000] temperature: 0.10890\n",
            "[29,  2000] loss: 0.04590\n",
            "[29,  2000] temperature: 0.07590\n",
            "[29,  4000] loss: 0.03735\n",
            "[29,  4000] temperature: 0.07590\n",
            "[29,  6000] loss: 0.02743\n",
            "[29,  6000] temperature: 0.07590\n",
            "[29,  8000] loss: 0.03088\n",
            "[29,  8000] temperature: 0.07590\n",
            "[29, 10000] loss: 0.02969\n",
            "[29, 10000] temperature: 0.07590\n",
            "[29, 12000] loss: 0.03948\n",
            "[29, 12000] temperature: 0.07590\n",
            "[30,  2000] loss: 0.04514\n",
            "[30,  2000] temperature: 0.04290\n",
            "[30,  4000] loss: 0.04960\n",
            "[30,  4000] temperature: 0.04290\n",
            "[30,  6000] loss: 0.04379\n",
            "[30,  6000] temperature: 0.04290\n",
            "[30,  8000] loss: 0.03973\n",
            "[30,  8000] temperature: 0.04290\n",
            "[30, 10000] loss: 0.04191\n",
            "[30, 10000] temperature: 0.04290\n",
            "[30, 12000] loss: 0.04005\n",
            "[30, 12000] temperature: 0.04290\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivk559GX2dzL"
      },
      "source": [
        "# Loss_Annealing_Model_exp = [6295.4160088300705, 4698.362445831299, 4681.099160313606, 4668.512912392616, 4665.40352332592, 4649.154500126839, 4645.708291292191, 4642.8637989759445, 4642.6537890434265, 4637.786241769791, 4631.127181529999, 4630.308316707611, 4629.626549243927, 4627.073973894119, 4626.6579077243805, 4623.503995895386, 4626.226335763931, 4620.009803056717, 4612.908445119858, 4620.35947227478, 4618.407818555832, 4617.662576675415, 4618.4861171245575, 4614.872433662415, 4616.73054933548, 4614.7221603393555, 4340.615003347397, 3408.0363912582397, 2879.1572725772858, 2405.6561209112406, 2125.3465134873986, 2012.9659889303148, 1874.5371485576034, 1705.1083010323346, 1460.5514042954892, 1315.0550528094172, 990.863434761297, 975.5274964037817, 948.2394827473909, 905.2119477665983, 846.7528409359511, 858.550159299979, 573.2552340372931, 588.3645126987249, 542.1010585740441, 548.3139305345248, 558.3395072179846, 556.7611744028982, 311.01018921949435, 329.15448177506914, 336.3068705199985, 365.6474192829337, 357.70949062216096, 375.2174963091966, 196.9598004668369, 187.06422230505268, 224.00901033121045, 227.65594794074423, 250.7698380239308, 245.0096245467139, 147.25303144182544, 155.60782268331968, 142.99294923098932, 181.03793473975384, 178.21686264319578, 164.86853500454163, 115.57596279666905, 128.8182194550027, 135.05731756526802, 116.05888023180887, 145.1599851091887, 139.41539937615016, 81.8629353906872, 118.50191818606982, 118.26940735863172, 112.05066671759414, 127.45522794957651, 98.15993556015019, 89.61265879383427, 90.49866250083869, 118.72730098076863, 104.00022450851247, 111.25868048205302, 112.79869797622814, 90.83161698708864, 99.37500475882553, 94.37667373841396, 98.55551795671636, 100.21072030838695, 112.79142068477813, 74.3039433184822, 102.42721597936907, 100.11288451219298, 93.28121933261718, 104.74826638246304, 100.6466061032479, 113.36124050460785, 86.30087341940089, 102.47195718255534, 93.3793903534388, 128.5826018464577, 83.39713205981388, 73.42698555495008, 102.15758710772207, 83.4576257881854, 112.72878997067164, 85.13436931811157, 115.29547659990203, 97.31187044503167, 111.49026581554244, 89.59351160014921, 116.95810813056596, 95.83408886080724, 102.96631332362449, 85.24763971059292, 114.24020509213733, 109.21131833456457, 111.2107011096814, 71.04304717631021, 121.83521597171784, 65.90878196049016, 114.84838727378519, 108.88343652935873, 88.26369838722894, 119.4624904114171, 98.35616062146437, 83.53538319135987, 123.60227493316052, 116.87819167286216, 99.15412604712219, 114.21798348141601, 105.37669312005164, 105.05660838962649, 133.31881659918872, 136.75471474180813, 127.38706888750312, 99.01469031575834, 116.97607605610392, 110.65479330093876, 117.08963824689272, 101.77839259279426, 135.51604385842802, 156.79343406611588, 128.15946231316775, 136.40030639198085, 125.34101394368918, 127.2026952409069, 170.8479464748525, 141.8942141032894, 173.07016420154832, 107.7095875258965, 173.03868169133784, 109.6556740895976, 117.25597191319684, 148.50370185245993, 142.59542406987748, 118.46621643722756, 155.5620129633462, 149.54364278697176, 145.37301812914666, 179.08255185134476, 144.63176771771396, 178.14770285843406, 138.18062005139655, 158.23695968009997, 192.9404472723254, 193.46677175047807, 151.80927048111334, 182.40204298659228, 190.83327328407904, 205.64918641687836, 179.51875323971035, 212.93038592400262, 238.95440808020066, 237.40040828776546, 249.72854166512843, 239.13739011983853, 190.33303778641857, 231.81306141149253, 248.04296878050081]\n",
        "# Loss_Annealing_Model_Prop = [6220.854539632797, 4357.512228369713, 3560.7143238782883, 3258.6882833838463, 2744.3019897937775, 2443.440160661936, 2063.850209519267, 2008.1231438145041, 1826.5542283058167, 1459.5433863922954, 1317.8277175985277, 1193.670773955062, 913.462082920596, 884.8907753033563, 837.7196136526763, 855.9593238416128, 849.7270208965056, 831.7540355953388, 556.6059640741441, 552.3761584348977, 561.1651882429142, 556.4671341502108, 573.4008204466663, 522.9952387192752, 334.23353291267995, 381.49829956085887, 351.4878318324336, 365.54082414496224, 372.3614286106895, 381.86965760361636, 207.0965942578623, 236.2556466513779, 254.10093355245772, 254.98677541621146, 273.9518919768743, 229.46912388467172, 151.20215218178055, 172.70516892365413, 162.72984167112736, 183.50582504263002, 173.82667379720078, 195.64218519820133, 111.94436782556295, 135.27907845918526, 145.95666329300911, 141.37535232941445, 135.83692460513703, 130.86519827920347, 123.38699456820177, 107.96427598726041, 101.59184595567785, 108.01149839731625, 109.41993228774118, 122.64215594829511, 78.30377630580551, 110.19233638581136, 98.08623543788417, 105.57204419384107, 114.65146975519383, 109.89437316254407, 74.83768240616337, 88.51907770286925, 84.29513670922506, 93.96637365380593, 112.57356677966982, 87.11818689660686, 80.28151254779277, 78.20246818179658, 85.85371770399706, 92.02220574674357, 98.80801807821354, 92.57607371475024, 71.7700077086356, 77.65279292068954, 82.89064036734999, 92.08988713461986, 98.3666644701425, 74.73861632251464, 50.95140570908188, 85.70787763495264, 73.30436932802422, 82.06438989797061, 92.50244645905696, 91.77993676969072, 50.3933720754012, 92.41109845755818, 94.62121629791, 74.47531171015726, 78.49797643410784, 85.72634399271192, 67.20536332319898, 65.63931976171261, 98.01541782727145, 78.14115873169521, 67.03880823390045, 84.33143507710702, 58.05519674954303, 90.63472934488163, 71.504201746935, 90.01595893313697, 83.49910275244656, 71.9475056617066, 69.43028580918872, 76.45588561195655, 85.44832379454783, 108.62902272275733, 66.75146675873293, 79.08281219650235, 60.65163255714833, 73.82813994305434, 67.01901052105404, 82.00114565494187, 82.18534113478745, 98.95208498062675, 53.470144948047164, 81.25247265269354, 70.18512464572814, 75.67947529074445, 89.84495142823289, 92.21660705854447, 73.29837861466785, 84.02562494479025, 91.70471884665221, 73.31107895772493, 82.77153714711676, 117.2709182898925, 68.188322080754, 87.76216780617506, 112.1836749116028, 107.22456512407189, 85.63151786475464, 98.73239184534759, 49.014715941603754, 84.68163477782355, 77.77314811387623, 87.57295956649614, 105.3085109849053, 99.80937882158105, 74.66674253613564, 89.54717735111262, 103.72654193900598, 118.30979176298206, 91.41453034950973, 115.7262379896456, 114.34129017288069, 85.9996365209081, 110.88514298873088, 101.81968516592678, 82.38071673901504, 96.31443170999046, 114.27044929664407, 88.11190371192788, 114.7419846110206, 119.96375622509004, 112.11399588132917, 105.8645587853116, 136.17484177384904, 99.37712556691986, 121.7416490604628, 130.92985379737365, 88.67763160055983, 156.00151549652583, 136.88786180966417, 104.44043026469444, 132.41246499778936, 131.48437380525866, 122.81701343458553, 150.30743624575916, 113.90362379302678, 153.22035992786186, 146.70739963180677, 165.63859604339814, 161.08452643285273, 146.47826912646997, 179.51025893667247, 172.573260606674, 244.20462181983748, 159.60718980350066, 198.42604974567075, 222.87538916931953]\n",
        "# Loss_Annealing_Model_linear = [6177.63513982296, 4688.154862642288, 4680.595834970474, 4679.982576608658, 4658.844668149948, 4658.0486072301865, 4656.042840242386, 4643.63847887516, 4316.2015463113785, 3397.0152685046196, 2686.084766983986, 2362.6502960920334, 2049.1474898308516, 1961.7500022128224, 1836.1954694539309, 1706.1916274949908, 1410.197340078652, 1294.495565244928, 935.8000734616071, 935.2391139315441, 917.0666223624721, 881.0626139689703, 842.8628254611976, 859.8692125482485, 572.9436194796581, 588.9477559641236, 575.0998206199147, 571.2292809489882, 590.6300078561762, 563.8484839322045, 344.9473996211309, 366.9364054800826, 351.0408669330063, 350.98355356484535, 368.0944739432307, 371.7355560786091, 217.26770937783294, 237.94814313817187, 225.6872198242927, 229.97781399221276, 226.38184007268865, 255.30620852761422, 158.13169128586014, 152.08564721266885, 156.5181927769081, 158.22156277913746, 142.7848219147927, 196.10055373955765, 101.20282580216008, 118.48277791491637, 127.64937190130513, 138.5585107445586, 119.94276411071041, 138.33026749570308, 91.55920823532142, 92.40652372951445, 96.75875900189385, 110.39135252587221, 108.85871113340545, 98.39381905483788, 98.84404215017639, 87.5679754872217, 100.84720365230896, 108.86526115816196, 74.86446841432189, 92.66226976450162, 57.64340366607303, 58.83306118502378, 90.3379619954544, 98.52031646377418, 77.36405532985918, 89.14934793126986, 74.8988736879046, 65.97922516124618, 66.1665620106246, 62.35047348273065, 79.12087732289592, 72.85881813954097, 61.58828256889683, 81.90510997325441, 71.90153529435156, 47.58087429436682, 78.0397236905419, 66.24192191632073, 43.215270672468705, 79.95293436771863, 54.079227125409886, 64.31783788733583, 45.189542166744445, 70.86203764913421, 57.07770600523645, 53.33786260183928, 58.705637161491836, 55.72556871767142, 85.03063382554734, 41.35735962847096, 44.24429358986359, 64.36356889785372, 55.250241861407744, 48.51156683492218, 64.89965613474362, 53.96448373774069, 49.54399233186007, 59.56849943415051, 56.944738901401934, 51.015272398006005, 70.99246688405688, 50.21085888625265, 46.75048479580755, 54.2827229914007, 36.639378122871754, 68.05356553156841, 43.56316417413052, 38.80045056096367, 62.276073484283955, 57.357326631311025, 38.54367123651751, 56.09508805914635, 45.95696317807554, 40.12403512090032, 37.552757121144865, 46.606351209616605, 43.38335970711903, 57.1403346020862, 57.69798320438849, 37.65050443240108, 50.91794669749464, 40.91329874218684, 47.80590700862838, 35.56011181286689, 62.26911950186067, 60.296114558525005, 53.90827183946806, 35.32124669200397, 45.97817203102146, 52.79760190794855, 44.14802972919429, 36.24118357505637, 46.158250670759735, 52.35076473416137, 65.59874504221717, 38.36403506422826, 52.026226129651434, 55.63734285998555, 42.98443995714682, 48.47104126951373, 57.5269945333753, 30.55949777343774, 68.62660051074175, 42.020057480523576, 53.74721183322811, 50.46618256578725, 47.54393276658186, 59.25862434238397, 47.72942800103719, 55.1973986099635, 54.047229241600434, 53.378003295923214, 63.44727377089839, 56.275737586254905, 41.492042740063724, 50.092185558889746, 52.61138360103709, 45.263400583539095, 74.45151781298333, 40.51303343696617, 40.31838699915379, 61.516358665445466, 91.80051654547991, 74.70624295410455, 54.85348572814631, 61.75123809206548, 59.388727995377735, 78.9581865802405, 90.28083369474689, 99.2082248194613, 87.57844982204324, 79.46288096128512, 83.82231957863922, 80.10519370284533]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "441xtpHFqB9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "9c5440f5-b691-43be-83d4-ad3340bc08d0"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(6, 4))\n",
        "ax.plot(Loss_Annealing_Model_exp, Color='red', label='natural exponential')\n",
        "ax.plot(Loss_Annealing_Model_Prop, Color='green', label='exponential')\n",
        "ax.plot(Loss_Annealing_Model_linear, Color='blue', label='linear')\n",
        "ax.set_xlabel(\"Iteration\", fontsize=16)\n",
        "ax.set_ylabel(\"Loss\", fontsize=16)\n",
        "# ax.set_title(\"Linear Model\", fontsize=16)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "# ax.yaxis.set_major_locator(y_major_locator)\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6A-mCyNRdM9",
        "outputId": "1a17fbc7-9a1b-4693-e5b4-8cd445c505ee"
      },
      "source": [
        "# print(Loss_Annealing_Model_exp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6295.4160088300705, 4698.362445831299, 4681.099160313606, 4668.512912392616, 4665.40352332592, 4649.154500126839, 4645.708291292191, 4642.8637989759445, 4642.6537890434265, 4637.786241769791, 4631.127181529999, 4630.308316707611, 4629.626549243927, 4627.073973894119, 4626.6579077243805, 4623.503995895386, 4626.226335763931, 4620.009803056717, 4612.908445119858, 4620.35947227478, 4618.407818555832, 4617.662576675415, 4618.4861171245575, 4614.872433662415, 4616.73054933548, 4614.7221603393555, 4340.615003347397, 3408.0363912582397, 2879.1572725772858, 2405.6561209112406, 2125.3465134873986, 2012.9659889303148, 1874.5371485576034, 1705.1083010323346, 1460.5514042954892, 1315.0550528094172, 990.863434761297, 975.5274964037817, 948.2394827473909, 905.2119477665983, 846.7528409359511, 858.550159299979, 573.2552340372931, 588.3645126987249, 542.1010585740441, 548.3139305345248, 558.3395072179846, 556.7611744028982, 311.01018921949435, 329.15448177506914, 336.3068705199985, 365.6474192829337, 357.70949062216096, 375.2174963091966, 196.9598004668369, 187.06422230505268, 224.00901033121045, 227.65594794074423, 250.7698380239308, 245.0096245467139, 147.25303144182544, 155.60782268331968, 142.99294923098932, 181.03793473975384, 178.21686264319578, 164.86853500454163, 115.57596279666905, 128.8182194550027, 135.05731756526802, 116.05888023180887, 145.1599851091887, 139.41539937615016, 81.8629353906872, 118.50191818606982, 118.26940735863172, 112.05066671759414, 127.45522794957651, 98.15993556015019, 89.61265879383427, 90.49866250083869, 118.72730098076863, 104.00022450851247, 111.25868048205302, 112.79869797622814, 90.83161698708864, 99.37500475882553, 94.37667373841396, 98.55551795671636, 100.21072030838695, 112.79142068477813, 74.3039433184822, 102.42721597936907, 100.11288451219298, 93.28121933261718, 104.74826638246304, 100.6466061032479, 113.36124050460785, 86.30087341940089, 102.47195718255534, 93.3793903534388, 128.5826018464577, 83.39713205981388, 73.42698555495008, 102.15758710772207, 83.4576257881854, 112.72878997067164, 85.13436931811157, 115.29547659990203, 97.31187044503167, 111.49026581554244, 89.59351160014921, 116.95810813056596, 95.83408886080724, 102.96631332362449, 85.24763971059292, 114.24020509213733, 109.21131833456457, 111.2107011096814, 71.04304717631021, 121.83521597171784, 65.90878196049016, 114.84838727378519, 108.88343652935873, 88.26369838722894, 119.4624904114171, 98.35616062146437, 83.53538319135987, 123.60227493316052, 116.87819167286216, 99.15412604712219, 114.21798348141601, 105.37669312005164, 105.05660838962649, 133.31881659918872, 136.75471474180813, 127.38706888750312, 99.01469031575834, 116.97607605610392, 110.65479330093876, 117.08963824689272, 101.77839259279426, 135.51604385842802, 156.79343406611588, 128.15946231316775, 136.40030639198085, 125.34101394368918, 127.2026952409069, 170.8479464748525, 141.8942141032894, 173.07016420154832, 107.7095875258965, 173.03868169133784, 109.6556740895976, 117.25597191319684, 148.50370185245993, 142.59542406987748, 118.46621643722756, 155.5620129633462, 149.54364278697176, 145.37301812914666, 179.08255185134476, 144.63176771771396, 178.14770285843406, 138.18062005139655, 158.23695968009997, 192.9404472723254, 193.46677175047807, 151.80927048111334, 182.40204298659228, 190.83327328407904, 205.64918641687836, 179.51875323971035, 212.93038592400262, 238.95440808020066, 237.40040828776546, 249.72854166512843, 239.13739011983853, 190.33303778641857, 231.81306141149253, 248.04296878050081]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVeCsCYdfVA3",
        "outputId": "96c50f63-2800-47b5-e8a2-68a0e56c9e20"
      },
      "source": [
        "# print(Loss_Annealing_Model_Prop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6220.854539632797, 4357.512228369713, 3560.7143238782883, 3258.6882833838463, 2744.3019897937775, 2443.440160661936, 2063.850209519267, 2008.1231438145041, 1826.5542283058167, 1459.5433863922954, 1317.8277175985277, 1193.670773955062, 913.462082920596, 884.8907753033563, 837.7196136526763, 855.9593238416128, 849.7270208965056, 831.7540355953388, 556.6059640741441, 552.3761584348977, 561.1651882429142, 556.4671341502108, 573.4008204466663, 522.9952387192752, 334.23353291267995, 381.49829956085887, 351.4878318324336, 365.54082414496224, 372.3614286106895, 381.86965760361636, 207.0965942578623, 236.2556466513779, 254.10093355245772, 254.98677541621146, 273.9518919768743, 229.46912388467172, 151.20215218178055, 172.70516892365413, 162.72984167112736, 183.50582504263002, 173.82667379720078, 195.64218519820133, 111.94436782556295, 135.27907845918526, 145.95666329300911, 141.37535232941445, 135.83692460513703, 130.86519827920347, 123.38699456820177, 107.96427598726041, 101.59184595567785, 108.01149839731625, 109.41993228774118, 122.64215594829511, 78.30377630580551, 110.19233638581136, 98.08623543788417, 105.57204419384107, 114.65146975519383, 109.89437316254407, 74.83768240616337, 88.51907770286925, 84.29513670922506, 93.96637365380593, 112.57356677966982, 87.11818689660686, 80.28151254779277, 78.20246818179658, 85.85371770399706, 92.02220574674357, 98.80801807821354, 92.57607371475024, 71.7700077086356, 77.65279292068954, 82.89064036734999, 92.08988713461986, 98.3666644701425, 74.73861632251464, 50.95140570908188, 85.70787763495264, 73.30436932802422, 82.06438989797061, 92.50244645905696, 91.77993676969072, 50.3933720754012, 92.41109845755818, 94.62121629791, 74.47531171015726, 78.49797643410784, 85.72634399271192, 67.20536332319898, 65.63931976171261, 98.01541782727145, 78.14115873169521, 67.03880823390045, 84.33143507710702, 58.05519674954303, 90.63472934488163, 71.504201746935, 90.01595893313697, 83.49910275244656, 71.9475056617066, 69.43028580918872, 76.45588561195655, 85.44832379454783, 108.62902272275733, 66.75146675873293, 79.08281219650235, 60.65163255714833, 73.82813994305434, 67.01901052105404, 82.00114565494187, 82.18534113478745, 98.95208498062675, 53.470144948047164, 81.25247265269354, 70.18512464572814, 75.67947529074445, 89.84495142823289, 92.21660705854447, 73.29837861466785, 84.02562494479025, 91.70471884665221, 73.31107895772493, 82.77153714711676, 117.2709182898925, 68.188322080754, 87.76216780617506, 112.1836749116028, 107.22456512407189, 85.63151786475464, 98.73239184534759, 49.014715941603754, 84.68163477782355, 77.77314811387623, 87.57295956649614, 105.3085109849053, 99.80937882158105, 74.66674253613564, 89.54717735111262, 103.72654193900598, 118.30979176298206, 91.41453034950973, 115.7262379896456, 114.34129017288069, 85.9996365209081, 110.88514298873088, 101.81968516592678, 82.38071673901504, 96.31443170999046, 114.27044929664407, 88.11190371192788, 114.7419846110206, 119.96375622509004, 112.11399588132917, 105.8645587853116, 136.17484177384904, 99.37712556691986, 121.7416490604628, 130.92985379737365, 88.67763160055983, 156.00151549652583, 136.88786180966417, 104.44043026469444, 132.41246499778936, 131.48437380525866, 122.81701343458553, 150.30743624575916, 113.90362379302678, 153.22035992786186, 146.70739963180677, 165.63859604339814, 161.08452643285273, 146.47826912646997, 179.51025893667247, 172.573260606674, 244.20462181983748, 159.60718980350066, 198.42604974567075, 222.87538916931953]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uat6usrfRig9",
        "outputId": "20dc6346-7a9b-428a-ba53-671cef472fc6"
      },
      "source": [
        "# print(Loss_Annealing_Model_linear)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6177.63513982296, 4688.154862642288, 4680.595834970474, 4679.982576608658, 4658.844668149948, 4658.0486072301865, 4656.042840242386, 4643.63847887516, 4316.2015463113785, 3397.0152685046196, 2686.084766983986, 2362.6502960920334, 2049.1474898308516, 1961.7500022128224, 1836.1954694539309, 1706.1916274949908, 1410.197340078652, 1294.495565244928, 935.8000734616071, 935.2391139315441, 917.0666223624721, 881.0626139689703, 842.8628254611976, 859.8692125482485, 572.9436194796581, 588.9477559641236, 575.0998206199147, 571.2292809489882, 590.6300078561762, 563.8484839322045, 344.9473996211309, 366.9364054800826, 351.0408669330063, 350.98355356484535, 368.0944739432307, 371.7355560786091, 217.26770937783294, 237.94814313817187, 225.6872198242927, 229.97781399221276, 226.38184007268865, 255.30620852761422, 158.13169128586014, 152.08564721266885, 156.5181927769081, 158.22156277913746, 142.7848219147927, 196.10055373955765, 101.20282580216008, 118.48277791491637, 127.64937190130513, 138.5585107445586, 119.94276411071041, 138.33026749570308, 91.55920823532142, 92.40652372951445, 96.75875900189385, 110.39135252587221, 108.85871113340545, 98.39381905483788, 98.84404215017639, 87.5679754872217, 100.84720365230896, 108.86526115816196, 74.86446841432189, 92.66226976450162, 57.64340366607303, 58.83306118502378, 90.3379619954544, 98.52031646377418, 77.36405532985918, 89.14934793126986, 74.8988736879046, 65.97922516124618, 66.1665620106246, 62.35047348273065, 79.12087732289592, 72.85881813954097, 61.58828256889683, 81.90510997325441, 71.90153529435156, 47.58087429436682, 78.0397236905419, 66.24192191632073, 43.215270672468705, 79.95293436771863, 54.079227125409886, 64.31783788733583, 45.189542166744445, 70.86203764913421, 57.07770600523645, 53.33786260183928, 58.705637161491836, 55.72556871767142, 85.03063382554734, 41.35735962847096, 44.24429358986359, 64.36356889785372, 55.250241861407744, 48.51156683492218, 64.89965613474362, 53.96448373774069, 49.54399233186007, 59.56849943415051, 56.944738901401934, 51.015272398006005, 70.99246688405688, 50.21085888625265, 46.75048479580755, 54.2827229914007, 36.639378122871754, 68.05356553156841, 43.56316417413052, 38.80045056096367, 62.276073484283955, 57.357326631311025, 38.54367123651751, 56.09508805914635, 45.95696317807554, 40.12403512090032, 37.552757121144865, 46.606351209616605, 43.38335970711903, 57.1403346020862, 57.69798320438849, 37.65050443240108, 50.91794669749464, 40.91329874218684, 47.80590700862838, 35.56011181286689, 62.26911950186067, 60.296114558525005, 53.90827183946806, 35.32124669200397, 45.97817203102146, 52.79760190794855, 44.14802972919429, 36.24118357505637, 46.158250670759735, 52.35076473416137, 65.59874504221717, 38.36403506422826, 52.026226129651434, 55.63734285998555, 42.98443995714682, 48.47104126951373, 57.5269945333753, 30.55949777343774, 68.62660051074175, 42.020057480523576, 53.74721183322811, 50.46618256578725, 47.54393276658186, 59.25862434238397, 47.72942800103719, 55.1973986099635, 54.047229241600434, 53.378003295923214, 63.44727377089839, 56.275737586254905, 41.492042740063724, 50.092185558889746, 52.61138360103709, 45.263400583539095, 74.45151781298333, 40.51303343696617, 40.31838699915379, 61.516358665445466, 91.80051654547991, 74.70624295410455, 54.85348572814631, 61.75123809206548, 59.388727995377735, 78.9581865802405, 90.28083369474689, 99.2082248194613, 87.57844982204324, 79.46288096128512, 83.82231957863922, 80.10519370284533]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}