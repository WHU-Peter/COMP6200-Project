{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vgg_model_annealing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqnLmoloGop3Oet2DT7jtS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHU-Peter/COMP6200-Project/blob/main/vgg_model_annealing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZduTHahSHt",
        "outputId": "a5115e5f-3c25-4e8f-9555-7398d15dacc0"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug 21 07:07:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    54W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KuXXQLqyGR"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8LYseck5ixy",
        "outputId": "d9be9a01-a320-4bf2-ac1e-e87de2a238b9"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True)\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_GNeu-VqX-V"
      },
      "source": [
        "def softmax_temperature(logits, temperature):\n",
        "    pro = F.softmax(logits / temperature, dim=-1)\n",
        "    return pro;"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkWst_c2qbnq"
      },
      "source": [
        "class AnnealingLookUpTable(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        super(AnnealingLookUpTable, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "    def forward(self, x, temperature):\n",
        "        if self.training:\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          return x @ self.emb.weight\n",
        "        else:\n",
        "          # x = softmax_temperature(x, 0.00001)\n",
        "          # x = mapping_onehot_vector(x)\n",
        "          # return self.emb(x)\n",
        "\n",
        "          x = softmax_temperature(x, temperature)\n",
        "          nozero = torch.nonzero(x);\n",
        "          out = np.zeros((x.shape[0], self.embedding_dim))\n",
        "          out = torch.tensor(out).to(device)\n",
        "          # print(np.array(nozero).shape[1])\n",
        "          for i in range(x.shape[0]):\n",
        "            idx = torch.where(nozero[:,0]==i)[0]\n",
        "            rows = nozero[idx, 1].long()\n",
        "            out[i] = torch.mean(self.emb(rows), axis=0)\n",
        "          return out.float()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZV7QMU0gg5_"
      },
      "source": [
        "class Annealing_VGG_LookUpTable(nn.Module):\n",
        "  def __init__(self, originalModel):\n",
        "    super(Annealing_VGG_LookUpTable, self).__init__()\n",
        "    # self.features = originalModel.features\n",
        "    # self.avgpool = originalModel.avgpool\n",
        "    self.vgg = nn.Sequential(*list(originalModel.children())[:-1])\n",
        "    # self.classifier = originalModel.classifier\n",
        "    self.look_up_table = AnnealingLookUpTable(25088, 4096)\n",
        "    self.classifier = nn.Sequential(*list(originalModel.classifier.children())[1:])\n",
        "    \n",
        "  def forward(self, x, temperature):\n",
        "    # x = self.features(x)\n",
        "    # x = self.avgpool(x)\n",
        "    x = self.vgg(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.look_up_table(x, temperature)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vnrDliORL8T",
        "outputId": "8761a235-1833-4bb1-88ef-420a20586775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_exp = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, math.exp(-1.4 * math.pow(10, -5) * idx)), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            Loss_Annealing_Model_exp.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 3.14771\n",
            "[1,  4000] loss: 2.34918\n",
            "[1,  6000] loss: 2.34055\n",
            "[1,  8000] loss: 2.33426\n",
            "[1, 10000] loss: 2.33270\n",
            "[1, 12000] loss: 2.32458\n",
            "[2,  2000] loss: 2.32285\n",
            "[2,  4000] loss: 2.32143\n",
            "[2,  6000] loss: 2.32133\n",
            "[2,  8000] loss: 2.31889\n",
            "[2, 10000] loss: 2.31556\n",
            "[2, 12000] loss: 2.31515\n",
            "[3,  2000] loss: 2.31481\n",
            "[3,  4000] loss: 2.31354\n",
            "[3,  6000] loss: 2.31333\n",
            "[3,  8000] loss: 2.31175\n",
            "[3, 10000] loss: 2.31311\n",
            "[3, 12000] loss: 2.31000\n",
            "[4,  2000] loss: 2.30645\n",
            "[4,  4000] loss: 2.31018\n",
            "[4,  6000] loss: 2.30920\n",
            "[4,  8000] loss: 2.30883\n",
            "[4, 10000] loss: 2.30924\n",
            "[4, 12000] loss: 2.30744\n",
            "[5,  2000] loss: 2.30837\n",
            "[5,  4000] loss: 2.30736\n",
            "[5,  6000] loss: 2.17031\n",
            "[5,  8000] loss: 1.70402\n",
            "[5, 10000] loss: 1.43958\n",
            "[5, 12000] loss: 1.20283\n",
            "[6,  2000] loss: 1.06267\n",
            "[6,  4000] loss: 1.00648\n",
            "[6,  6000] loss: 0.93727\n",
            "[6,  8000] loss: 0.85255\n",
            "[6, 10000] loss: 0.73028\n",
            "[6, 12000] loss: 0.65753\n",
            "[7,  2000] loss: 0.49543\n",
            "[7,  4000] loss: 0.48776\n",
            "[7,  6000] loss: 0.47412\n",
            "[7,  8000] loss: 0.45261\n",
            "[7, 10000] loss: 0.42338\n",
            "[7, 12000] loss: 0.42928\n",
            "[8,  2000] loss: 0.28663\n",
            "[8,  4000] loss: 0.29418\n",
            "[8,  6000] loss: 0.27105\n",
            "[8,  8000] loss: 0.27416\n",
            "[8, 10000] loss: 0.27917\n",
            "[8, 12000] loss: 0.27838\n",
            "[9,  2000] loss: 0.15551\n",
            "[9,  4000] loss: 0.16458\n",
            "[9,  6000] loss: 0.16815\n",
            "[9,  8000] loss: 0.18282\n",
            "[9, 10000] loss: 0.17885\n",
            "[9, 12000] loss: 0.18761\n",
            "[10,  2000] loss: 0.09848\n",
            "[10,  4000] loss: 0.09353\n",
            "[10,  6000] loss: 0.11200\n",
            "[10,  8000] loss: 0.11383\n",
            "[10, 10000] loss: 0.12538\n",
            "[10, 12000] loss: 0.12250\n",
            "[11,  2000] loss: 0.07363\n",
            "[11,  4000] loss: 0.07780\n",
            "[11,  6000] loss: 0.07150\n",
            "[11,  8000] loss: 0.09052\n",
            "[11, 10000] loss: 0.08911\n",
            "[11, 12000] loss: 0.08243\n",
            "[12,  2000] loss: 0.05779\n",
            "[12,  4000] loss: 0.06441\n",
            "[12,  6000] loss: 0.06753\n",
            "[12,  8000] loss: 0.05803\n",
            "[12, 10000] loss: 0.07258\n",
            "[12, 12000] loss: 0.06971\n",
            "[13,  2000] loss: 0.04093\n",
            "[13,  4000] loss: 0.05925\n",
            "[13,  6000] loss: 0.05913\n",
            "[13,  8000] loss: 0.05603\n",
            "[13, 10000] loss: 0.06373\n",
            "[13, 12000] loss: 0.04908\n",
            "[14,  2000] loss: 0.04481\n",
            "[14,  4000] loss: 0.04525\n",
            "[14,  6000] loss: 0.05936\n",
            "[14,  8000] loss: 0.05200\n",
            "[14, 10000] loss: 0.05563\n",
            "[14, 12000] loss: 0.05640\n",
            "[15,  2000] loss: 0.04542\n",
            "[15,  4000] loss: 0.04969\n",
            "[15,  6000] loss: 0.04719\n",
            "[15,  8000] loss: 0.04928\n",
            "[15, 10000] loss: 0.05011\n",
            "[15, 12000] loss: 0.05640\n",
            "[16,  2000] loss: 0.03715\n",
            "[16,  4000] loss: 0.05121\n",
            "[16,  6000] loss: 0.05006\n",
            "[16,  8000] loss: 0.04664\n",
            "[16, 10000] loss: 0.05237\n",
            "[16, 12000] loss: 0.05032\n",
            "[17,  2000] loss: 0.05668\n",
            "[17,  4000] loss: 0.04315\n",
            "[17,  6000] loss: 0.05124\n",
            "[17,  8000] loss: 0.04669\n",
            "[17, 10000] loss: 0.06429\n",
            "[17, 12000] loss: 0.04170\n",
            "[18,  2000] loss: 0.03671\n",
            "[18,  4000] loss: 0.05108\n",
            "[18,  6000] loss: 0.04173\n",
            "[18,  8000] loss: 0.05636\n",
            "[18, 10000] loss: 0.04257\n",
            "[18, 12000] loss: 0.05765\n",
            "[19,  2000] loss: 0.04866\n",
            "[19,  4000] loss: 0.05575\n",
            "[19,  6000] loss: 0.04480\n",
            "[19,  8000] loss: 0.05848\n",
            "[19, 10000] loss: 0.04792\n",
            "[19, 12000] loss: 0.05148\n",
            "[20,  2000] loss: 0.04262\n",
            "[20,  4000] loss: 0.05712\n",
            "[20,  6000] loss: 0.05461\n",
            "[20,  8000] loss: 0.05561\n",
            "[20, 10000] loss: 0.03552\n",
            "[20, 12000] loss: 0.06092\n",
            "[21,  2000] loss: 0.03295\n",
            "[21,  4000] loss: 0.05742\n",
            "[21,  6000] loss: 0.05444\n",
            "[21,  8000] loss: 0.04413\n",
            "[21, 10000] loss: 0.05973\n",
            "[21, 12000] loss: 0.04918\n",
            "[22,  2000] loss: 0.04177\n",
            "[22,  4000] loss: 0.06180\n",
            "[22,  6000] loss: 0.05844\n",
            "[22,  8000] loss: 0.04958\n",
            "[22, 10000] loss: 0.05711\n",
            "[22, 12000] loss: 0.05269\n",
            "[23,  2000] loss: 0.05253\n",
            "[23,  4000] loss: 0.06666\n",
            "[23,  6000] loss: 0.06838\n",
            "[23,  8000] loss: 0.06369\n",
            "[23, 10000] loss: 0.04951\n",
            "[23, 12000] loss: 0.05849\n",
            "[24,  2000] loss: 0.05533\n",
            "[24,  4000] loss: 0.05854\n",
            "[24,  6000] loss: 0.05089\n",
            "[24,  8000] loss: 0.06776\n",
            "[24, 10000] loss: 0.07840\n",
            "[24, 12000] loss: 0.06408\n",
            "[25,  2000] loss: 0.06820\n",
            "[25,  4000] loss: 0.06267\n",
            "[25,  6000] loss: 0.06360\n",
            "[25,  8000] loss: 0.08542\n",
            "[25, 10000] loss: 0.07095\n",
            "[25, 12000] loss: 0.08654\n",
            "[26,  2000] loss: 0.05385\n",
            "[26,  4000] loss: 0.08652\n",
            "[26,  6000] loss: 0.05483\n",
            "[26,  8000] loss: 0.05863\n",
            "[26, 10000] loss: 0.07425\n",
            "[26, 12000] loss: 0.07130\n",
            "[27,  2000] loss: 0.05923\n",
            "[27,  4000] loss: 0.07778\n",
            "[27,  6000] loss: 0.07477\n",
            "[27,  8000] loss: 0.07269\n",
            "[27, 10000] loss: 0.08954\n",
            "[27, 12000] loss: 0.07232\n",
            "[28,  2000] loss: 0.08907\n",
            "[28,  4000] loss: 0.06909\n",
            "[28,  6000] loss: 0.07912\n",
            "[28,  8000] loss: 0.09647\n",
            "[28, 10000] loss: 0.09673\n",
            "[28, 12000] loss: 0.07590\n",
            "[29,  2000] loss: 0.09120\n",
            "[29,  4000] loss: 0.09542\n",
            "[29,  6000] loss: 0.10282\n",
            "[29,  8000] loss: 0.08976\n",
            "[29, 10000] loss: 0.10647\n",
            "[29, 12000] loss: 0.11948\n",
            "[30,  2000] loss: 0.11870\n",
            "[30,  4000] loss: 0.12486\n",
            "[30,  6000] loss: 0.11957\n",
            "[30,  8000] loss: 0.09517\n",
            "[30, 10000] loss: 0.11591\n",
            "[30, 12000] loss: 0.12402\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_EzoAd4iJJ0"
      },
      "source": [
        "torch.save(Annealing_lut_model.state_dict(), \"./Annealing_lut_model.weights\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw-D7WrHgrsB",
        "outputId": "33264020-8469-46e7-e331-706c823027b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_Prop = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, max(0.0001, math.pow(0.999859, idx/10))), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            Loss_Annealing_Model_Prop.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n",
            "[1,  2000] loss: 3.11043\n",
            "[1,  4000] loss: 2.17876\n",
            "[1,  6000] loss: 1.78036\n",
            "[1,  8000] loss: 1.62934\n",
            "[1, 10000] loss: 1.37215\n",
            "[1, 12000] loss: 1.22172\n",
            "[2,  2000] loss: 1.03193\n",
            "[2,  4000] loss: 1.00406\n",
            "[2,  6000] loss: 0.91328\n",
            "[2,  8000] loss: 0.72977\n",
            "[2, 10000] loss: 0.65891\n",
            "[2, 12000] loss: 0.59684\n",
            "[3,  2000] loss: 0.45673\n",
            "[3,  4000] loss: 0.44245\n",
            "[3,  6000] loss: 0.41886\n",
            "[3,  8000] loss: 0.42798\n",
            "[3, 10000] loss: 0.42486\n",
            "[3, 12000] loss: 0.41588\n",
            "[4,  2000] loss: 0.27830\n",
            "[4,  4000] loss: 0.27619\n",
            "[4,  6000] loss: 0.28058\n",
            "[4,  8000] loss: 0.27823\n",
            "[4, 10000] loss: 0.28670\n",
            "[4, 12000] loss: 0.26150\n",
            "[5,  2000] loss: 0.16712\n",
            "[5,  4000] loss: 0.19075\n",
            "[5,  6000] loss: 0.17574\n",
            "[5,  8000] loss: 0.18277\n",
            "[5, 10000] loss: 0.18618\n",
            "[5, 12000] loss: 0.19093\n",
            "[6,  2000] loss: 0.10355\n",
            "[6,  4000] loss: 0.11813\n",
            "[6,  6000] loss: 0.12705\n",
            "[6,  8000] loss: 0.12749\n",
            "[6, 10000] loss: 0.13698\n",
            "[6, 12000] loss: 0.11473\n",
            "[7,  2000] loss: 0.07560\n",
            "[7,  4000] loss: 0.08635\n",
            "[7,  6000] loss: 0.08136\n",
            "[7,  8000] loss: 0.09175\n",
            "[7, 10000] loss: 0.08691\n",
            "[7, 12000] loss: 0.09782\n",
            "[8,  2000] loss: 0.05597\n",
            "[8,  4000] loss: 0.06764\n",
            "[8,  6000] loss: 0.07298\n",
            "[8,  8000] loss: 0.07069\n",
            "[8, 10000] loss: 0.06792\n",
            "[8, 12000] loss: 0.06543\n",
            "[9,  2000] loss: 0.06169\n",
            "[9,  4000] loss: 0.05398\n",
            "[9,  6000] loss: 0.05080\n",
            "[9,  8000] loss: 0.05401\n",
            "[9, 10000] loss: 0.05471\n",
            "[9, 12000] loss: 0.06132\n",
            "[10,  2000] loss: 0.03915\n",
            "[10,  4000] loss: 0.05510\n",
            "[10,  6000] loss: 0.04904\n",
            "[10,  8000] loss: 0.05279\n",
            "[10, 10000] loss: 0.05733\n",
            "[10, 12000] loss: 0.05495\n",
            "[11,  2000] loss: 0.03742\n",
            "[11,  4000] loss: 0.04426\n",
            "[11,  6000] loss: 0.04215\n",
            "[11,  8000] loss: 0.04698\n",
            "[11, 10000] loss: 0.05629\n",
            "[11, 12000] loss: 0.04356\n",
            "[12,  2000] loss: 0.04014\n",
            "[12,  4000] loss: 0.03910\n",
            "[12,  6000] loss: 0.04293\n",
            "[12,  8000] loss: 0.04601\n",
            "[12, 10000] loss: 0.04940\n",
            "[12, 12000] loss: 0.04629\n",
            "[13,  2000] loss: 0.03589\n",
            "[13,  4000] loss: 0.03883\n",
            "[13,  6000] loss: 0.04145\n",
            "[13,  8000] loss: 0.04604\n",
            "[13, 10000] loss: 0.04918\n",
            "[13, 12000] loss: 0.03737\n",
            "[14,  2000] loss: 0.02548\n",
            "[14,  4000] loss: 0.04285\n",
            "[14,  6000] loss: 0.03665\n",
            "[14,  8000] loss: 0.04103\n",
            "[14, 10000] loss: 0.04625\n",
            "[14, 12000] loss: 0.04589\n",
            "[15,  2000] loss: 0.02520\n",
            "[15,  4000] loss: 0.04621\n",
            "[15,  6000] loss: 0.04731\n",
            "[15,  8000] loss: 0.03724\n",
            "[15, 10000] loss: 0.03925\n",
            "[15, 12000] loss: 0.04286\n",
            "[16,  2000] loss: 0.03360\n",
            "[16,  4000] loss: 0.03282\n",
            "[16,  6000] loss: 0.04901\n",
            "[16,  8000] loss: 0.03907\n",
            "[16, 10000] loss: 0.03352\n",
            "[16, 12000] loss: 0.04217\n",
            "[17,  2000] loss: 0.02903\n",
            "[17,  4000] loss: 0.04532\n",
            "[17,  6000] loss: 0.03575\n",
            "[17,  8000] loss: 0.04501\n",
            "[17, 10000] loss: 0.04175\n",
            "[17, 12000] loss: 0.03597\n",
            "[18,  2000] loss: 0.03472\n",
            "[18,  4000] loss: 0.03823\n",
            "[18,  6000] loss: 0.04272\n",
            "[18,  8000] loss: 0.05431\n",
            "[18, 10000] loss: 0.03338\n",
            "[18, 12000] loss: 0.03954\n",
            "[19,  2000] loss: 0.03033\n",
            "[19,  4000] loss: 0.03691\n",
            "[19,  6000] loss: 0.03351\n",
            "[19,  8000] loss: 0.04100\n",
            "[19, 10000] loss: 0.04109\n",
            "[19, 12000] loss: 0.04948\n",
            "[20,  2000] loss: 0.02674\n",
            "[20,  4000] loss: 0.04063\n",
            "[20,  6000] loss: 0.03509\n",
            "[20,  8000] loss: 0.03784\n",
            "[20, 10000] loss: 0.04492\n",
            "[20, 12000] loss: 0.04611\n",
            "[21,  2000] loss: 0.03665\n",
            "[21,  4000] loss: 0.04201\n",
            "[21,  6000] loss: 0.04585\n",
            "[21,  8000] loss: 0.03666\n",
            "[21, 10000] loss: 0.04139\n",
            "[21, 12000] loss: 0.05864\n",
            "[22,  2000] loss: 0.03409\n",
            "[22,  4000] loss: 0.04388\n",
            "[22,  6000] loss: 0.05609\n",
            "[22,  8000] loss: 0.05361\n",
            "[22, 10000] loss: 0.04282\n",
            "[22, 12000] loss: 0.04937\n",
            "[23,  2000] loss: 0.02451\n",
            "[23,  4000] loss: 0.04234\n",
            "[23,  6000] loss: 0.03889\n",
            "[23,  8000] loss: 0.04379\n",
            "[23, 10000] loss: 0.05265\n",
            "[23, 12000] loss: 0.04990\n",
            "[24,  2000] loss: 0.03733\n",
            "[24,  4000] loss: 0.04477\n",
            "[24,  6000] loss: 0.05186\n",
            "[24,  8000] loss: 0.05915\n",
            "[24, 10000] loss: 0.04571\n",
            "[24, 12000] loss: 0.05786\n",
            "[25,  2000] loss: 0.05717\n",
            "[25,  4000] loss: 0.04300\n",
            "[25,  6000] loss: 0.05544\n",
            "[25,  8000] loss: 0.05091\n",
            "[25, 10000] loss: 0.04119\n",
            "[25, 12000] loss: 0.04816\n",
            "[26,  2000] loss: 0.05714\n",
            "[26,  4000] loss: 0.04406\n",
            "[26,  6000] loss: 0.05737\n",
            "[26,  8000] loss: 0.05998\n",
            "[26, 10000] loss: 0.05606\n",
            "[26, 12000] loss: 0.05293\n",
            "[27,  2000] loss: 0.06809\n",
            "[27,  4000] loss: 0.04969\n",
            "[27,  6000] loss: 0.06087\n",
            "[27,  8000] loss: 0.06546\n",
            "[27, 10000] loss: 0.04434\n",
            "[27, 12000] loss: 0.07800\n",
            "[28,  2000] loss: 0.06844\n",
            "[28,  4000] loss: 0.05222\n",
            "[28,  6000] loss: 0.06621\n",
            "[28,  8000] loss: 0.06574\n",
            "[28, 10000] loss: 0.06141\n",
            "[28, 12000] loss: 0.07515\n",
            "[29,  2000] loss: 0.05695\n",
            "[29,  4000] loss: 0.07661\n",
            "[29,  6000] loss: 0.07335\n",
            "[29,  8000] loss: 0.08282\n",
            "[29, 10000] loss: 0.08054\n",
            "[29, 12000] loss: 0.07324\n",
            "[30,  2000] loss: 0.08976\n",
            "[30,  4000] loss: 0.08629\n",
            "[30,  6000] loss: 0.12210\n",
            "[30,  8000] loss: 0.07980\n",
            "[30, 10000] loss: 0.09921\n",
            "[30, 12000] loss: 0.11144\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx8D7n6Rgwj6",
        "outputId": "beeefa27-880f-4705-9ad0-e84e506bf0cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pre_model = models.vgg16(pretrained=True)  # 使用VGG16 网络预训练好的模型\n",
        "for parma in pre_model.parameters():  # 设置自动梯度为false\n",
        "    parma.requires_grad = True\n",
        "Annealing_lut_model = Annealing_VGG_LookUpTable(pre_model).to(device)\n",
        "\n",
        "epoches = 30  #Training times\n",
        "learning_rate = 1e-5  #Learning rate\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "Loss_Annealing_Model_linear = []\n",
        "idx = 0\n",
        "\n",
        "print('Loss_Annealing_Model Started Training')\n",
        "for epoch in range(epoches):    #Iteration\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        idx = idx + 1\n",
        "        inputs, labels = data\n",
        "                 #Initialize gradient\n",
        "        optimizer.zero_grad()\n",
        "        outputs = Annealing_lut_model(inputs.to(device), torch.tensor(max(0.0001, 0.9999 - 0.033*epoch), device= device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # for parameter in optimizer.param_groups[0]['params']:\n",
        "        #   print(np.any(parameter.grad.cpu().numpy()==0))\n",
        "\n",
        "        # Print loss\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            print('[%d, %5d] temperature: %.5f' %\n",
        "                  (epoch + 1, i + 1, max(0.0001, 0.9999 - 0.033*epoch)))\n",
        "            Loss_Annealing_Model_linear.append(running_loss)\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Loss_Annealing_Model Finished Training')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss_Annealing_Model Started Training\n",
            "[1,  2000] loss: 3.08882\n",
            "[1,  2000] temperature: 0.99990\n",
            "[1,  4000] loss: 2.34408\n",
            "[1,  4000] temperature: 0.99990\n",
            "[1,  6000] loss: 2.34030\n",
            "[1,  6000] temperature: 0.99990\n",
            "[1,  8000] loss: 2.33999\n",
            "[1,  8000] temperature: 0.99990\n",
            "[1, 10000] loss: 2.32942\n",
            "[1, 10000] temperature: 0.99990\n",
            "[1, 12000] loss: 2.32902\n",
            "[1, 12000] temperature: 0.99990\n",
            "[2,  2000] loss: 2.32802\n",
            "[2,  2000] temperature: 0.96690\n",
            "[2,  4000] loss: 2.32182\n",
            "[2,  4000] temperature: 0.96690\n",
            "[2,  6000] loss: 2.15810\n",
            "[2,  6000] temperature: 0.96690\n",
            "[2,  8000] loss: 1.69851\n",
            "[2,  8000] temperature: 0.96690\n",
            "[2, 10000] loss: 1.34304\n",
            "[2, 10000] temperature: 0.96690\n",
            "[2, 12000] loss: 1.18133\n",
            "[2, 12000] temperature: 0.96690\n",
            "[3,  2000] loss: 1.02457\n",
            "[3,  2000] temperature: 0.93390\n",
            "[3,  4000] loss: 0.98088\n",
            "[3,  4000] temperature: 0.93390\n",
            "[3,  6000] loss: 0.91810\n",
            "[3,  6000] temperature: 0.93390\n",
            "[3,  8000] loss: 0.85310\n",
            "[3,  8000] temperature: 0.93390\n",
            "[3, 10000] loss: 0.70510\n",
            "[3, 10000] temperature: 0.93390\n",
            "[3, 12000] loss: 0.64725\n",
            "[3, 12000] temperature: 0.93390\n",
            "[4,  2000] loss: 0.46790\n",
            "[4,  2000] temperature: 0.90090\n",
            "[4,  4000] loss: 0.46762\n",
            "[4,  4000] temperature: 0.90090\n",
            "[4,  6000] loss: 0.45853\n",
            "[4,  6000] temperature: 0.90090\n",
            "[4,  8000] loss: 0.44053\n",
            "[4,  8000] temperature: 0.90090\n",
            "[4, 10000] loss: 0.42143\n",
            "[4, 10000] temperature: 0.90090\n",
            "[4, 12000] loss: 0.42993\n",
            "[4, 12000] temperature: 0.90090\n",
            "[5,  2000] loss: 0.28647\n",
            "[5,  2000] temperature: 0.86790\n",
            "[5,  4000] loss: 0.29447\n",
            "[5,  4000] temperature: 0.86790\n",
            "[5,  6000] loss: 0.28755\n",
            "[5,  6000] temperature: 0.86790\n",
            "[5,  8000] loss: 0.28561\n",
            "[5,  8000] temperature: 0.86790\n",
            "[5, 10000] loss: 0.29532\n",
            "[5, 10000] temperature: 0.86790\n",
            "[5, 12000] loss: 0.28192\n",
            "[5, 12000] temperature: 0.86790\n",
            "[6,  2000] loss: 0.17247\n",
            "[6,  2000] temperature: 0.83490\n",
            "[6,  4000] loss: 0.18347\n",
            "[6,  4000] temperature: 0.83490\n",
            "[6,  6000] loss: 0.17552\n",
            "[6,  6000] temperature: 0.83490\n",
            "[6,  8000] loss: 0.17549\n",
            "[6,  8000] temperature: 0.83490\n",
            "[6, 10000] loss: 0.18405\n",
            "[6, 10000] temperature: 0.83490\n",
            "[6, 12000] loss: 0.18587\n",
            "[6, 12000] temperature: 0.83490\n",
            "[7,  2000] loss: 0.10863\n",
            "[7,  2000] temperature: 0.80190\n",
            "[7,  4000] loss: 0.11897\n",
            "[7,  4000] temperature: 0.80190\n",
            "[7,  6000] loss: 0.11284\n",
            "[7,  6000] temperature: 0.80190\n",
            "[7,  8000] loss: 0.11499\n",
            "[7,  8000] temperature: 0.80190\n",
            "[7, 10000] loss: 0.11319\n",
            "[7, 10000] temperature: 0.80190\n",
            "[7, 12000] loss: 0.12765\n",
            "[7, 12000] temperature: 0.80190\n",
            "[8,  2000] loss: 0.07907\n",
            "[8,  2000] temperature: 0.76890\n",
            "[8,  4000] loss: 0.07604\n",
            "[8,  4000] temperature: 0.76890\n",
            "[8,  6000] loss: 0.07826\n",
            "[8,  6000] temperature: 0.76890\n",
            "[8,  8000] loss: 0.07911\n",
            "[8,  8000] temperature: 0.76890\n",
            "[8, 10000] loss: 0.07139\n",
            "[8, 10000] temperature: 0.76890\n",
            "[8, 12000] loss: 0.09805\n",
            "[8, 12000] temperature: 0.76890\n",
            "[9,  2000] loss: 0.05060\n",
            "[9,  2000] temperature: 0.73590\n",
            "[9,  4000] loss: 0.05924\n",
            "[9,  4000] temperature: 0.73590\n",
            "[9,  6000] loss: 0.06382\n",
            "[9,  6000] temperature: 0.73590\n",
            "[9,  8000] loss: 0.06928\n",
            "[9,  8000] temperature: 0.73590\n",
            "[9, 10000] loss: 0.05997\n",
            "[9, 10000] temperature: 0.73590\n",
            "[9, 12000] loss: 0.06917\n",
            "[9, 12000] temperature: 0.73590\n",
            "[10,  2000] loss: 0.04578\n",
            "[10,  2000] temperature: 0.70290\n",
            "[10,  4000] loss: 0.04620\n",
            "[10,  4000] temperature: 0.70290\n",
            "[10,  6000] loss: 0.04838\n",
            "[10,  6000] temperature: 0.70290\n",
            "[10,  8000] loss: 0.05520\n",
            "[10,  8000] temperature: 0.70290\n",
            "[10, 10000] loss: 0.05443\n",
            "[10, 10000] temperature: 0.70290\n",
            "[10, 12000] loss: 0.04920\n",
            "[10, 12000] temperature: 0.70290\n",
            "[11,  2000] loss: 0.04942\n",
            "[11,  2000] temperature: 0.66990\n",
            "[11,  4000] loss: 0.04378\n",
            "[11,  4000] temperature: 0.66990\n",
            "[11,  6000] loss: 0.05042\n",
            "[11,  6000] temperature: 0.66990\n",
            "[11,  8000] loss: 0.05443\n",
            "[11,  8000] temperature: 0.66990\n",
            "[11, 10000] loss: 0.03743\n",
            "[11, 10000] temperature: 0.66990\n",
            "[11, 12000] loss: 0.04633\n",
            "[11, 12000] temperature: 0.66990\n",
            "[12,  2000] loss: 0.02882\n",
            "[12,  2000] temperature: 0.63690\n",
            "[12,  4000] loss: 0.02942\n",
            "[12,  4000] temperature: 0.63690\n",
            "[12,  6000] loss: 0.04517\n",
            "[12,  6000] temperature: 0.63690\n",
            "[12,  8000] loss: 0.04926\n",
            "[12,  8000] temperature: 0.63690\n",
            "[12, 10000] loss: 0.03868\n",
            "[12, 10000] temperature: 0.63690\n",
            "[12, 12000] loss: 0.04457\n",
            "[12, 12000] temperature: 0.63690\n",
            "[13,  2000] loss: 0.03745\n",
            "[13,  2000] temperature: 0.60390\n",
            "[13,  4000] loss: 0.03299\n",
            "[13,  4000] temperature: 0.60390\n",
            "[13,  6000] loss: 0.03308\n",
            "[13,  6000] temperature: 0.60390\n",
            "[13,  8000] loss: 0.03118\n",
            "[13,  8000] temperature: 0.60390\n",
            "[13, 10000] loss: 0.03956\n",
            "[13, 10000] temperature: 0.60390\n",
            "[13, 12000] loss: 0.03643\n",
            "[13, 12000] temperature: 0.60390\n",
            "[14,  2000] loss: 0.03079\n",
            "[14,  2000] temperature: 0.57090\n",
            "[14,  4000] loss: 0.04095\n",
            "[14,  4000] temperature: 0.57090\n",
            "[14,  6000] loss: 0.03595\n",
            "[14,  6000] temperature: 0.57090\n",
            "[14,  8000] loss: 0.02379\n",
            "[14,  8000] temperature: 0.57090\n",
            "[14, 10000] loss: 0.03902\n",
            "[14, 10000] temperature: 0.57090\n",
            "[14, 12000] loss: 0.03312\n",
            "[14, 12000] temperature: 0.57090\n",
            "[15,  2000] loss: 0.02161\n",
            "[15,  2000] temperature: 0.53790\n",
            "[15,  4000] loss: 0.03998\n",
            "[15,  4000] temperature: 0.53790\n",
            "[15,  6000] loss: 0.02704\n",
            "[15,  6000] temperature: 0.53790\n",
            "[15,  8000] loss: 0.03216\n",
            "[15,  8000] temperature: 0.53790\n",
            "[15, 10000] loss: 0.02259\n",
            "[15, 10000] temperature: 0.53790\n",
            "[15, 12000] loss: 0.03543\n",
            "[15, 12000] temperature: 0.53790\n",
            "[16,  2000] loss: 0.02854\n",
            "[16,  2000] temperature: 0.50490\n",
            "[16,  4000] loss: 0.02667\n",
            "[16,  4000] temperature: 0.50490\n",
            "[16,  6000] loss: 0.02935\n",
            "[16,  6000] temperature: 0.50490\n",
            "[16,  8000] loss: 0.02786\n",
            "[16,  8000] temperature: 0.50490\n",
            "[16, 10000] loss: 0.04252\n",
            "[16, 10000] temperature: 0.50490\n",
            "[16, 12000] loss: 0.02068\n",
            "[16, 12000] temperature: 0.50490\n",
            "[17,  2000] loss: 0.02212\n",
            "[17,  2000] temperature: 0.47190\n",
            "[17,  4000] loss: 0.03218\n",
            "[17,  4000] temperature: 0.47190\n",
            "[17,  6000] loss: 0.02763\n",
            "[17,  6000] temperature: 0.47190\n",
            "[17,  8000] loss: 0.02426\n",
            "[17,  8000] temperature: 0.47190\n",
            "[17, 10000] loss: 0.03245\n",
            "[17, 10000] temperature: 0.47190\n",
            "[17, 12000] loss: 0.02698\n",
            "[17, 12000] temperature: 0.47190\n",
            "[18,  2000] loss: 0.02477\n",
            "[18,  2000] temperature: 0.43890\n",
            "[18,  4000] loss: 0.02978\n",
            "[18,  4000] temperature: 0.43890\n",
            "[18,  6000] loss: 0.02847\n",
            "[18,  6000] temperature: 0.43890\n",
            "[18,  8000] loss: 0.02551\n",
            "[18,  8000] temperature: 0.43890\n",
            "[18, 10000] loss: 0.03550\n",
            "[18, 10000] temperature: 0.43890\n",
            "[18, 12000] loss: 0.02511\n",
            "[18, 12000] temperature: 0.43890\n",
            "[19,  2000] loss: 0.02338\n",
            "[19,  2000] temperature: 0.40590\n",
            "[19,  4000] loss: 0.02714\n",
            "[19,  4000] temperature: 0.40590\n",
            "[19,  6000] loss: 0.01832\n",
            "[19,  6000] temperature: 0.40590\n",
            "[19,  8000] loss: 0.03403\n",
            "[19,  8000] temperature: 0.40590\n",
            "[19, 10000] loss: 0.02178\n",
            "[19, 10000] temperature: 0.40590\n",
            "[19, 12000] loss: 0.01940\n",
            "[19, 12000] temperature: 0.40590\n",
            "[20,  2000] loss: 0.03114\n",
            "[20,  2000] temperature: 0.37290\n",
            "[20,  4000] loss: 0.02868\n",
            "[20,  4000] temperature: 0.37290\n",
            "[20,  6000] loss: 0.01927\n",
            "[20,  6000] temperature: 0.37290\n",
            "[20,  8000] loss: 0.02805\n",
            "[20,  8000] temperature: 0.37290\n",
            "[20, 10000] loss: 0.02298\n",
            "[20, 10000] temperature: 0.37290\n",
            "[20, 12000] loss: 0.02006\n",
            "[20, 12000] temperature: 0.37290\n",
            "[21,  2000] loss: 0.01878\n",
            "[21,  2000] temperature: 0.33990\n",
            "[21,  4000] loss: 0.02330\n",
            "[21,  4000] temperature: 0.33990\n",
            "[21,  6000] loss: 0.02169\n",
            "[21,  6000] temperature: 0.33990\n",
            "[21,  8000] loss: 0.02857\n",
            "[21,  8000] temperature: 0.33990\n",
            "[21, 10000] loss: 0.02885\n",
            "[21, 10000] temperature: 0.33990\n",
            "[21, 12000] loss: 0.01883\n",
            "[21, 12000] temperature: 0.33990\n",
            "[22,  2000] loss: 0.02546\n",
            "[22,  2000] temperature: 0.30690\n",
            "[22,  4000] loss: 0.02046\n",
            "[22,  4000] temperature: 0.30690\n",
            "[22,  6000] loss: 0.02390\n",
            "[22,  6000] temperature: 0.30690\n",
            "[22,  8000] loss: 0.01778\n",
            "[22,  8000] temperature: 0.30690\n",
            "[22, 10000] loss: 0.03113\n",
            "[22, 10000] temperature: 0.30690\n",
            "[22, 12000] loss: 0.03015\n",
            "[22, 12000] temperature: 0.30690\n",
            "[23,  2000] loss: 0.02695\n",
            "[23,  2000] temperature: 0.27390\n",
            "[23,  4000] loss: 0.01766\n",
            "[23,  4000] temperature: 0.27390\n",
            "[23,  6000] loss: 0.02299\n",
            "[23,  6000] temperature: 0.27390\n",
            "[23,  8000] loss: 0.02640\n",
            "[23,  8000] temperature: 0.27390\n",
            "[23, 10000] loss: 0.02207\n",
            "[23, 10000] temperature: 0.27390\n",
            "[23, 12000] loss: 0.01812\n",
            "[23, 12000] temperature: 0.27390\n",
            "[24,  2000] loss: 0.02308\n",
            "[24,  2000] temperature: 0.24090\n",
            "[24,  4000] loss: 0.02618\n",
            "[24,  4000] temperature: 0.24090\n",
            "[24,  6000] loss: 0.03280\n",
            "[24,  6000] temperature: 0.24090\n",
            "[24,  8000] loss: 0.01918\n",
            "[24,  8000] temperature: 0.24090\n",
            "[24, 10000] loss: 0.02601\n",
            "[24, 10000] temperature: 0.24090\n",
            "[24, 12000] loss: 0.02782\n",
            "[24, 12000] temperature: 0.24090\n",
            "[25,  2000] loss: 0.02149\n",
            "[25,  2000] temperature: 0.20790\n",
            "[25,  4000] loss: 0.02424\n",
            "[25,  4000] temperature: 0.20790\n",
            "[25,  6000] loss: 0.02876\n",
            "[25,  6000] temperature: 0.20790\n",
            "[25,  8000] loss: 0.01528\n",
            "[25,  8000] temperature: 0.20790\n",
            "[25, 10000] loss: 0.03431\n",
            "[25, 10000] temperature: 0.20790\n",
            "[25, 12000] loss: 0.02101\n",
            "[25, 12000] temperature: 0.20790\n",
            "[26,  2000] loss: 0.02687\n",
            "[26,  2000] temperature: 0.17490\n",
            "[26,  4000] loss: 0.02523\n",
            "[26,  4000] temperature: 0.17490\n",
            "[26,  6000] loss: 0.02377\n",
            "[26,  6000] temperature: 0.17490\n",
            "[26,  8000] loss: 0.02963\n",
            "[26,  8000] temperature: 0.17490\n",
            "[26, 10000] loss: 0.02386\n",
            "[26, 10000] temperature: 0.17490\n",
            "[26, 12000] loss: 0.02760\n",
            "[26, 12000] temperature: 0.17490\n",
            "[27,  2000] loss: 0.02702\n",
            "[27,  2000] temperature: 0.14190\n",
            "[27,  4000] loss: 0.02669\n",
            "[27,  4000] temperature: 0.14190\n",
            "[27,  6000] loss: 0.03172\n",
            "[27,  6000] temperature: 0.14190\n",
            "[27,  8000] loss: 0.02814\n",
            "[27,  8000] temperature: 0.14190\n",
            "[27, 10000] loss: 0.02075\n",
            "[27, 10000] temperature: 0.14190\n",
            "[27, 12000] loss: 0.02505\n",
            "[27, 12000] temperature: 0.14190\n",
            "[28,  2000] loss: 0.02631\n",
            "[28,  2000] temperature: 0.10890\n",
            "[28,  4000] loss: 0.02263\n",
            "[28,  4000] temperature: 0.10890\n",
            "[28,  6000] loss: 0.03723\n",
            "[28,  6000] temperature: 0.10890\n",
            "[28,  8000] loss: 0.02026\n",
            "[28,  8000] temperature: 0.10890\n",
            "[28, 10000] loss: 0.02016\n",
            "[28, 10000] temperature: 0.10890\n",
            "[28, 12000] loss: 0.03076\n",
            "[28, 12000] temperature: 0.10890\n",
            "[29,  2000] loss: 0.04590\n",
            "[29,  2000] temperature: 0.07590\n",
            "[29,  4000] loss: 0.03735\n",
            "[29,  4000] temperature: 0.07590\n",
            "[29,  6000] loss: 0.02743\n",
            "[29,  6000] temperature: 0.07590\n",
            "[29,  8000] loss: 0.03088\n",
            "[29,  8000] temperature: 0.07590\n",
            "[29, 10000] loss: 0.02969\n",
            "[29, 10000] temperature: 0.07590\n",
            "[29, 12000] loss: 0.03948\n",
            "[29, 12000] temperature: 0.07590\n",
            "[30,  2000] loss: 0.04514\n",
            "[30,  2000] temperature: 0.04290\n",
            "[30,  4000] loss: 0.04960\n",
            "[30,  4000] temperature: 0.04290\n",
            "[30,  6000] loss: 0.04379\n",
            "[30,  6000] temperature: 0.04290\n",
            "[30,  8000] loss: 0.03973\n",
            "[30,  8000] temperature: 0.04290\n",
            "[30, 10000] loss: 0.04191\n",
            "[30, 10000] temperature: 0.04290\n",
            "[30, 12000] loss: 0.04005\n",
            "[30, 12000] temperature: 0.04290\n",
            "Loss_Annealing_Model Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "441xtpHFqB9m",
        "outputId": "98d188b1-6dba-47d0-a009-09e06b73eb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(12,8))\n",
        "ax.plot(Loss_Annealing_Model_exp, Color='red', label='exponential_e', markersize=16)\n",
        "ax.plot(Loss_Annealing_Model_Prop, Color='green', label='exponential', markersize=16)\n",
        "ax.plot(Loss_Annealing_Model_linear, Color='blue', label='linear', markersize=16)\n",
        "ax.set_xlabel(\"Iteration\", fontsize=16)\n",
        "ax.set_ylabel(\"Loss\", fontsize=16)\n",
        "# ax.set_title(\"Linear Model\", fontsize=16)\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "# ax.yaxis.set_major_locator(y_major_locator)\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6A-mCyNRdM9",
        "outputId": "1a17fbc7-9a1b-4693-e5b4-8cd445c505ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Loss_Annealing_Model_exp)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6295.4160088300705, 4698.362445831299, 4681.099160313606, 4668.512912392616, 4665.40352332592, 4649.154500126839, 4645.708291292191, 4642.8637989759445, 4642.6537890434265, 4637.786241769791, 4631.127181529999, 4630.308316707611, 4629.626549243927, 4627.073973894119, 4626.6579077243805, 4623.503995895386, 4626.226335763931, 4620.009803056717, 4612.908445119858, 4620.35947227478, 4618.407818555832, 4617.662576675415, 4618.4861171245575, 4614.872433662415, 4616.73054933548, 4614.7221603393555, 4340.615003347397, 3408.0363912582397, 2879.1572725772858, 2405.6561209112406, 2125.3465134873986, 2012.9659889303148, 1874.5371485576034, 1705.1083010323346, 1460.5514042954892, 1315.0550528094172, 990.863434761297, 975.5274964037817, 948.2394827473909, 905.2119477665983, 846.7528409359511, 858.550159299979, 573.2552340372931, 588.3645126987249, 542.1010585740441, 548.3139305345248, 558.3395072179846, 556.7611744028982, 311.01018921949435, 329.15448177506914, 336.3068705199985, 365.6474192829337, 357.70949062216096, 375.2174963091966, 196.9598004668369, 187.06422230505268, 224.00901033121045, 227.65594794074423, 250.7698380239308, 245.0096245467139, 147.25303144182544, 155.60782268331968, 142.99294923098932, 181.03793473975384, 178.21686264319578, 164.86853500454163, 115.57596279666905, 128.8182194550027, 135.05731756526802, 116.05888023180887, 145.1599851091887, 139.41539937615016, 81.8629353906872, 118.50191818606982, 118.26940735863172, 112.05066671759414, 127.45522794957651, 98.15993556015019, 89.61265879383427, 90.49866250083869, 118.72730098076863, 104.00022450851247, 111.25868048205302, 112.79869797622814, 90.83161698708864, 99.37500475882553, 94.37667373841396, 98.55551795671636, 100.21072030838695, 112.79142068477813, 74.3039433184822, 102.42721597936907, 100.11288451219298, 93.28121933261718, 104.74826638246304, 100.6466061032479, 113.36124050460785, 86.30087341940089, 102.47195718255534, 93.3793903534388, 128.5826018464577, 83.39713205981388, 73.42698555495008, 102.15758710772207, 83.4576257881854, 112.72878997067164, 85.13436931811157, 115.29547659990203, 97.31187044503167, 111.49026581554244, 89.59351160014921, 116.95810813056596, 95.83408886080724, 102.96631332362449, 85.24763971059292, 114.24020509213733, 109.21131833456457, 111.2107011096814, 71.04304717631021, 121.83521597171784, 65.90878196049016, 114.84838727378519, 108.88343652935873, 88.26369838722894, 119.4624904114171, 98.35616062146437, 83.53538319135987, 123.60227493316052, 116.87819167286216, 99.15412604712219, 114.21798348141601, 105.37669312005164, 105.05660838962649, 133.31881659918872, 136.75471474180813, 127.38706888750312, 99.01469031575834, 116.97607605610392, 110.65479330093876, 117.08963824689272, 101.77839259279426, 135.51604385842802, 156.79343406611588, 128.15946231316775, 136.40030639198085, 125.34101394368918, 127.2026952409069, 170.8479464748525, 141.8942141032894, 173.07016420154832, 107.7095875258965, 173.03868169133784, 109.6556740895976, 117.25597191319684, 148.50370185245993, 142.59542406987748, 118.46621643722756, 155.5620129633462, 149.54364278697176, 145.37301812914666, 179.08255185134476, 144.63176771771396, 178.14770285843406, 138.18062005139655, 158.23695968009997, 192.9404472723254, 193.46677175047807, 151.80927048111334, 182.40204298659228, 190.83327328407904, 205.64918641687836, 179.51875323971035, 212.93038592400262, 238.95440808020066, 237.40040828776546, 249.72854166512843, 239.13739011983853, 190.33303778641857, 231.81306141149253, 248.04296878050081]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVeCsCYdfVA3",
        "outputId": "96c50f63-2800-47b5-e8a2-68a0e56c9e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Loss_Annealing_Model_Prop)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6220.854539632797, 4357.512228369713, 3560.7143238782883, 3258.6882833838463, 2744.3019897937775, 2443.440160661936, 2063.850209519267, 2008.1231438145041, 1826.5542283058167, 1459.5433863922954, 1317.8277175985277, 1193.670773955062, 913.462082920596, 884.8907753033563, 837.7196136526763, 855.9593238416128, 849.7270208965056, 831.7540355953388, 556.6059640741441, 552.3761584348977, 561.1651882429142, 556.4671341502108, 573.4008204466663, 522.9952387192752, 334.23353291267995, 381.49829956085887, 351.4878318324336, 365.54082414496224, 372.3614286106895, 381.86965760361636, 207.0965942578623, 236.2556466513779, 254.10093355245772, 254.98677541621146, 273.9518919768743, 229.46912388467172, 151.20215218178055, 172.70516892365413, 162.72984167112736, 183.50582504263002, 173.82667379720078, 195.64218519820133, 111.94436782556295, 135.27907845918526, 145.95666329300911, 141.37535232941445, 135.83692460513703, 130.86519827920347, 123.38699456820177, 107.96427598726041, 101.59184595567785, 108.01149839731625, 109.41993228774118, 122.64215594829511, 78.30377630580551, 110.19233638581136, 98.08623543788417, 105.57204419384107, 114.65146975519383, 109.89437316254407, 74.83768240616337, 88.51907770286925, 84.29513670922506, 93.96637365380593, 112.57356677966982, 87.11818689660686, 80.28151254779277, 78.20246818179658, 85.85371770399706, 92.02220574674357, 98.80801807821354, 92.57607371475024, 71.7700077086356, 77.65279292068954, 82.89064036734999, 92.08988713461986, 98.3666644701425, 74.73861632251464, 50.95140570908188, 85.70787763495264, 73.30436932802422, 82.06438989797061, 92.50244645905696, 91.77993676969072, 50.3933720754012, 92.41109845755818, 94.62121629791, 74.47531171015726, 78.49797643410784, 85.72634399271192, 67.20536332319898, 65.63931976171261, 98.01541782727145, 78.14115873169521, 67.03880823390045, 84.33143507710702, 58.05519674954303, 90.63472934488163, 71.504201746935, 90.01595893313697, 83.49910275244656, 71.9475056617066, 69.43028580918872, 76.45588561195655, 85.44832379454783, 108.62902272275733, 66.75146675873293, 79.08281219650235, 60.65163255714833, 73.82813994305434, 67.01901052105404, 82.00114565494187, 82.18534113478745, 98.95208498062675, 53.470144948047164, 81.25247265269354, 70.18512464572814, 75.67947529074445, 89.84495142823289, 92.21660705854447, 73.29837861466785, 84.02562494479025, 91.70471884665221, 73.31107895772493, 82.77153714711676, 117.2709182898925, 68.188322080754, 87.76216780617506, 112.1836749116028, 107.22456512407189, 85.63151786475464, 98.73239184534759, 49.014715941603754, 84.68163477782355, 77.77314811387623, 87.57295956649614, 105.3085109849053, 99.80937882158105, 74.66674253613564, 89.54717735111262, 103.72654193900598, 118.30979176298206, 91.41453034950973, 115.7262379896456, 114.34129017288069, 85.9996365209081, 110.88514298873088, 101.81968516592678, 82.38071673901504, 96.31443170999046, 114.27044929664407, 88.11190371192788, 114.7419846110206, 119.96375622509004, 112.11399588132917, 105.8645587853116, 136.17484177384904, 99.37712556691986, 121.7416490604628, 130.92985379737365, 88.67763160055983, 156.00151549652583, 136.88786180966417, 104.44043026469444, 132.41246499778936, 131.48437380525866, 122.81701343458553, 150.30743624575916, 113.90362379302678, 153.22035992786186, 146.70739963180677, 165.63859604339814, 161.08452643285273, 146.47826912646997, 179.51025893667247, 172.573260606674, 244.20462181983748, 159.60718980350066, 198.42604974567075, 222.87538916931953]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uat6usrfRig9",
        "outputId": "20dc6346-7a9b-428a-ba53-671cef472fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Loss_Annealing_Model_linear)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6177.63513982296, 4688.154862642288, 4680.595834970474, 4679.982576608658, 4658.844668149948, 4658.0486072301865, 4656.042840242386, 4643.63847887516, 4316.2015463113785, 3397.0152685046196, 2686.084766983986, 2362.6502960920334, 2049.1474898308516, 1961.7500022128224, 1836.1954694539309, 1706.1916274949908, 1410.197340078652, 1294.495565244928, 935.8000734616071, 935.2391139315441, 917.0666223624721, 881.0626139689703, 842.8628254611976, 859.8692125482485, 572.9436194796581, 588.9477559641236, 575.0998206199147, 571.2292809489882, 590.6300078561762, 563.8484839322045, 344.9473996211309, 366.9364054800826, 351.0408669330063, 350.98355356484535, 368.0944739432307, 371.7355560786091, 217.26770937783294, 237.94814313817187, 225.6872198242927, 229.97781399221276, 226.38184007268865, 255.30620852761422, 158.13169128586014, 152.08564721266885, 156.5181927769081, 158.22156277913746, 142.7848219147927, 196.10055373955765, 101.20282580216008, 118.48277791491637, 127.64937190130513, 138.5585107445586, 119.94276411071041, 138.33026749570308, 91.55920823532142, 92.40652372951445, 96.75875900189385, 110.39135252587221, 108.85871113340545, 98.39381905483788, 98.84404215017639, 87.5679754872217, 100.84720365230896, 108.86526115816196, 74.86446841432189, 92.66226976450162, 57.64340366607303, 58.83306118502378, 90.3379619954544, 98.52031646377418, 77.36405532985918, 89.14934793126986, 74.8988736879046, 65.97922516124618, 66.1665620106246, 62.35047348273065, 79.12087732289592, 72.85881813954097, 61.58828256889683, 81.90510997325441, 71.90153529435156, 47.58087429436682, 78.0397236905419, 66.24192191632073, 43.215270672468705, 79.95293436771863, 54.079227125409886, 64.31783788733583, 45.189542166744445, 70.86203764913421, 57.07770600523645, 53.33786260183928, 58.705637161491836, 55.72556871767142, 85.03063382554734, 41.35735962847096, 44.24429358986359, 64.36356889785372, 55.250241861407744, 48.51156683492218, 64.89965613474362, 53.96448373774069, 49.54399233186007, 59.56849943415051, 56.944738901401934, 51.015272398006005, 70.99246688405688, 50.21085888625265, 46.75048479580755, 54.2827229914007, 36.639378122871754, 68.05356553156841, 43.56316417413052, 38.80045056096367, 62.276073484283955, 57.357326631311025, 38.54367123651751, 56.09508805914635, 45.95696317807554, 40.12403512090032, 37.552757121144865, 46.606351209616605, 43.38335970711903, 57.1403346020862, 57.69798320438849, 37.65050443240108, 50.91794669749464, 40.91329874218684, 47.80590700862838, 35.56011181286689, 62.26911950186067, 60.296114558525005, 53.90827183946806, 35.32124669200397, 45.97817203102146, 52.79760190794855, 44.14802972919429, 36.24118357505637, 46.158250670759735, 52.35076473416137, 65.59874504221717, 38.36403506422826, 52.026226129651434, 55.63734285998555, 42.98443995714682, 48.47104126951373, 57.5269945333753, 30.55949777343774, 68.62660051074175, 42.020057480523576, 53.74721183322811, 50.46618256578725, 47.54393276658186, 59.25862434238397, 47.72942800103719, 55.1973986099635, 54.047229241600434, 53.378003295923214, 63.44727377089839, 56.275737586254905, 41.492042740063724, 50.092185558889746, 52.61138360103709, 45.263400583539095, 74.45151781298333, 40.51303343696617, 40.31838699915379, 61.516358665445466, 91.80051654547991, 74.70624295410455, 54.85348572814631, 61.75123809206548, 59.388727995377735, 78.9581865802405, 90.28083369474689, 99.2082248194613, 87.57844982204324, 79.46288096128512, 83.82231957863922, 80.10519370284533]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}