{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVGdciX+XuTH/SUS+6CzEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHU-Peter/COMP6200-Project/blob/main/project_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKDeAnF07V6x"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import math\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZL2hmRM8TMQ"
      },
      "source": [
        "# flatten 28*28 images to a 784 vector for each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # convert to tensor\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # flatten into vector\n",
        "])\n",
        "\n",
        "trainset = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "testset = MNIST(\".\", train=False, download=True, transform=transform)\n",
        "\n",
        "# create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=True)\n",
        "\n",
        "class_counts = torch.zeros(10, dtype=torch.int32)\n",
        "\n",
        "for (images, labels) in trainloader:\n",
        "  for label in labels:\n",
        "    class_counts[label] += 1\n",
        "\n",
        "assert class_counts.sum()==60000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLpsbgjy6f14"
      },
      "source": [
        "Baseline model, two linear layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgmENJ6YgR3K"
      },
      "source": [
        "# define baseline model\n",
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "            out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju7wMIq7gYGh",
        "outputId": "2786d7ff-852c-4d77-ae88-caeed17594af"
      },
      "source": [
        "# build the model \n",
        "model = BaselineModel(784, 784, 10)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters())\n",
        "\n",
        "# the epoch loop\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for data in trainloader:\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # forward + loss + backward + optimise (update weights)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        # keep track of the loss this epoch\n",
        "        running_loss += loss.item()\n",
        "    print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
        "print('**** Finished Training ****')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss 133.89\n",
            "Epoch 1, loss 51.05\n",
            "Epoch 2, loss 32.49\n",
            "Epoch 3, loss 23.12\n",
            "Epoch 4, loss 16.61\n",
            "Epoch 5, loss 12.50\n",
            "Epoch 6, loss 9.49\n",
            "Epoch 7, loss 6.82\n",
            "Epoch 8, loss 5.10\n",
            "Epoch 9, loss 4.38\n",
            "**** Finished Training ****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrRZ_MJ-glzX",
        "outputId": "230a9ebf-f824-4fae-b8f0-a249d0608689"
      },
      "source": [
        "# Compute the model accuracy on the test set\n",
        "class_correct = torch.zeros(10)\n",
        "class_total = torch.zeros(10)\n",
        "model.eval()\n",
        "for (images, labels) in testloader:\n",
        "\n",
        "  for label in labels:\n",
        "    class_total[label] += 1\n",
        "  \n",
        "  outputs = model(images)\n",
        "  for i in range(outputs.shape[0]):\n",
        "    _, prediction = outputs[i].max(0)\n",
        "    if prediction == labels[i]:\n",
        "      class_correct[labels[i]] += 1\n",
        "\n",
        "for i in range(10):\n",
        "    print('Class %d accuracy: %2.2f %%' % (i, 100.0*class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 0 accuracy: 99.08 %\n",
            "Class 1 accuracy: 98.94 %\n",
            "Class 2 accuracy: 97.77 %\n",
            "Class 3 accuracy: 96.34 %\n",
            "Class 4 accuracy: 97.35 %\n",
            "Class 5 accuracy: 98.21 %\n",
            "Class 6 accuracy: 98.23 %\n",
            "Class 7 accuracy: 97.76 %\n",
            "Class 8 accuracy: 98.25 %\n",
            "Class 9 accuracy: 98.51 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sK0zJMA8K25"
      },
      "source": [
        "learnable temperature parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgIQZdemuXW3"
      },
      "source": [
        "class SoftMaxWithTemperature(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftMaxWithTemperature, self).__init__()\n",
        "        self.temperature = nn.Parameter(torch.tensor(1.))\n",
        "    def forward(self, x):\n",
        "        # print(self.temperature)\n",
        "        return F.softmax(x / self.temperature, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqKL-4RvlVWx"
      },
      "source": [
        "# define LUTModel\n",
        "class LUTModelWithLearnableTemperature(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1,hidden_size2, num_classes):\n",
        "        super(LUTModelWithLearnableTemperature, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1) \n",
        "        self.softmax_temperature = SoftMaxWithTemperature();\n",
        "        self.emb = nn.Embedding(hidden_size1, hidden_size2) \n",
        "        self.fc2 = nn.Linear(hidden_size2, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.softmax_temperature(out)\n",
        "        out = out @ self.emb.weight\n",
        "        # out = self.emb(out.long())\n",
        "        # out = self.emb(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "            out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lGu1mINKhAZ",
        "outputId": "4619bdc2-fe5c-444c-efa6-c9d80542ce89"
      },
      "source": [
        "# build the model \n",
        "model = LUTModelWithLearnableTemperature(784, 200, 20, 10)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# the epoch loop\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for data in trainloader:\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # forward + loss + backward + optimise (update weights)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        # print(optimiser.param_groups[0]['params'][2])\n",
        "        # print(np.all(optimiser.param_groups[0]['params'][0].grad.numpy() == 0))\n",
        "        # keep track of the loss this epoch\n",
        "        running_loss += loss.item()\n",
        "        # break\n",
        "    print(optimiser.param_groups[0]['params'][2])\n",
        "    print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
        "print('**** Finished Training ****')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor(1.6316, requires_grad=True)\n",
            "Epoch 0, loss 169.89\n",
            "Parameter containing:\n",
            "tensor(2.1211, requires_grad=True)\n",
            "Epoch 1, loss 81.24\n",
            "Parameter containing:\n",
            "tensor(2.4600, requires_grad=True)\n",
            "Epoch 2, loss 64.32\n",
            "Parameter containing:\n",
            "tensor(2.7636, requires_grad=True)\n",
            "Epoch 3, loss 52.46\n",
            "Parameter containing:\n",
            "tensor(3.0728, requires_grad=True)\n",
            "Epoch 4, loss 45.09\n",
            "Parameter containing:\n",
            "tensor(3.2618, requires_grad=True)\n",
            "Epoch 5, loss 38.09\n",
            "Parameter containing:\n",
            "tensor(3.5440, requires_grad=True)\n",
            "Epoch 6, loss 33.64\n",
            "Parameter containing:\n",
            "tensor(3.7379, requires_grad=True)\n",
            "Epoch 7, loss 28.68\n",
            "Parameter containing:\n",
            "tensor(3.9677, requires_grad=True)\n",
            "Epoch 8, loss 24.44\n",
            "Parameter containing:\n",
            "tensor(4.1709, requires_grad=True)\n",
            "Epoch 9, loss 21.61\n",
            "**** Finished Training ****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccNHXUmNS-2H",
        "outputId": "12c46fc7-f980-458d-fab4-01b511e580d2"
      },
      "source": [
        "# Compute the model accuracy on the test set\n",
        "class_correct = torch.zeros(10)\n",
        "class_total = torch.zeros(10)\n",
        "model.eval()\n",
        "for (images, labels) in testloader:\n",
        "\n",
        "  for label in labels:\n",
        "    class_total[label] += 1\n",
        "  \n",
        "  outputs = model(images)\n",
        "  for i in range(outputs.shape[0]):\n",
        "    _, prediction = outputs[i].max(0)\n",
        "    if prediction == labels[i]:\n",
        "      class_correct[labels[i]] += 1\n",
        "\n",
        "for i in range(10):\n",
        "    print('Class %d accuracy: %2.2f %%' % (i, 100.0*class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 0 accuracy: 99.08 %\n",
            "Class 1 accuracy: 98.77 %\n",
            "Class 2 accuracy: 97.38 %\n",
            "Class 3 accuracy: 96.93 %\n",
            "Class 4 accuracy: 97.66 %\n",
            "Class 5 accuracy: 97.09 %\n",
            "Class 6 accuracy: 95.09 %\n",
            "Class 7 accuracy: 97.08 %\n",
            "Class 8 accuracy: 95.89 %\n",
            "Class 9 accuracy: 94.65 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Zi3FklRyK5",
        "outputId": "d5faf5c6-7ede-4573-9cc4-cd54696abd3f"
      },
      "source": [
        "for name,parameters in model.named_parameters():\n",
        "    print(name,':',parameters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fc1.weight : Parameter containing:\n",
            "tensor([[-0.0194,  0.0095,  0.0185,  ...,  0.0101,  0.0243, -0.0177],\n",
            "        [-0.0266, -0.0146,  0.0153,  ..., -0.0184, -0.0188,  0.0208],\n",
            "        [-0.0274, -0.0220,  0.0284,  ..., -0.0287, -0.0305,  0.0048],\n",
            "        ...,\n",
            "        [ 0.0087, -0.0059,  0.0007,  ..., -0.0066, -0.0040, -0.0352],\n",
            "        [ 0.0303, -0.0048,  0.0353,  ..., -0.0248,  0.0236, -0.0343],\n",
            "        [ 0.0236, -0.0205,  0.0348,  ...,  0.0022, -0.0086,  0.0282]],\n",
            "       requires_grad=True)\n",
            "fc1.bias : Parameter containing:\n",
            "tensor([-0.0889,  0.1299, -0.0260, -0.0790,  0.3115, -0.3797, -0.5406,  0.1640,\n",
            "         0.0039, -0.5243,  0.2748, -0.2115, -0.1084, -0.0385, -0.4883, -0.0730,\n",
            "        -0.0782, -0.7780, -0.2906, -0.0974, -0.0622, -0.4684, -0.0879, -0.0508,\n",
            "        -0.3832,  0.2421, -0.0394, -0.0385,  0.7548, -0.2019, -0.0699,  1.0311,\n",
            "        -0.0476,  0.1075, -0.0701, -0.1475,  0.0982, -0.0494,  0.4610,  0.3993,\n",
            "         0.0057,  0.5459, -0.0641, -0.1036, -0.1345,  0.2464, -0.1711,  1.0858,\n",
            "        -0.3338, -0.2880, -0.4903, -0.4840, -0.0448, -0.1980, -0.1222,  0.1090,\n",
            "         0.0069, -0.0744, -0.0791,  0.2825,  1.1406, -0.0693, -0.0909, -0.2361,\n",
            "        -0.0726, -0.1523, -0.0612,  0.0077, -0.6231, -0.1149, -0.0575, -0.1691,\n",
            "        -0.0963,  0.2650, -0.0647, -0.0636, -0.2015,  0.6214, -0.1297, -0.1306,\n",
            "        -0.1066, -0.0679, -0.4322, -0.0434, -0.3162,  0.0865, -0.0872,  0.5162,\n",
            "        -0.5331,  0.6521, -0.3783, -0.1419, -0.4907, -0.5431, -0.0341, -0.2801,\n",
            "        -0.4617, -0.2311, -0.0759,  0.2692, -0.2796, -0.0203, -0.1552, -0.5719,\n",
            "        -0.0958, -0.2696, -0.0485, -0.1890, -0.0960, -0.1785, -0.1495, -0.0101,\n",
            "        -0.1887, -0.0762, -0.3333,  0.6831, -0.0012, -0.1209,  0.4802,  0.2825,\n",
            "        -0.0794,  0.0940, -0.0465, -0.2140, -0.4909,  0.4614, -0.1582, -0.0460,\n",
            "        -0.0427, -0.0642, -0.1619,  0.4703, -0.0711,  0.5364,  0.6031,  0.0046,\n",
            "         0.8434, -0.0665, -0.0793, -0.3558, -0.1721,  0.1279,  0.0495, -0.2341,\n",
            "         0.0213, -0.2243, -0.0143, -0.0801, -0.1603, -0.0017, -0.1206, -0.0687,\n",
            "        -0.1967,  0.2689, -0.1250, -0.0381,  0.4597, -0.0968, -0.1936, -0.0451,\n",
            "         0.8593,  0.4421, -0.4596, -0.0539,  0.6606, -0.0835,  0.2575, -0.0701,\n",
            "         0.3142,  0.3496, -0.0755, -0.2602,  0.3249, -0.0608, -0.2621, -0.3504,\n",
            "        -0.5714, -0.1543,  0.6843, -0.0572, -0.4429, -0.0844, -0.1377, -0.0792,\n",
            "         0.2601,  0.6541, -0.1617, -0.0786, -0.1218, -0.3083, -0.1870, -0.1545,\n",
            "        -0.0603, -0.0684, -0.0446, -0.0621, -0.0271,  0.8048, -0.0463, -0.2159],\n",
            "       requires_grad=True)\n",
            "softmax_temperature.temperature : Parameter containing:\n",
            "tensor(4.1709, requires_grad=True)\n",
            "emb.weight : Parameter containing:\n",
            "tensor([[-0.6299,  1.3645,  2.9189,  ...,  2.9201,  2.3638,  1.4098],\n",
            "        [ 1.3598,  1.9972,  0.6288,  ...,  0.8790,  0.4220,  0.9597],\n",
            "        [ 0.3464,  0.2893,  0.6041,  ...,  3.4372,  0.2803,  0.7234],\n",
            "        ...,\n",
            "        [-4.1930, -3.3438, -2.1622,  ...,  2.3770,  0.8342, -2.1115],\n",
            "        [-1.0924,  2.4923,  5.5472,  ...,  3.4988,  1.1318,  1.1556],\n",
            "        [-0.6646,  0.3336,  0.3709,  ...,  1.6520, -0.6239,  1.7020]],\n",
            "       requires_grad=True)\n",
            "fc2.weight : Parameter containing:\n",
            "tensor([[-0.6729, -1.4916,  1.2488,  0.6050, -1.9561, -1.2335, -0.6657, -0.9437,\n",
            "          0.2788, -0.9192,  0.6262,  0.6171, -1.5525,  0.1350,  0.4461, -0.3948,\n",
            "         -0.2002,  0.6798,  0.6259, -0.9478],\n",
            "        [ 0.8838,  0.5127,  0.2398, -1.1454,  0.1312,  0.9671,  0.1088,  0.0144,\n",
            "          0.9849, -1.3790, -0.7926, -1.0456, -1.8170, -2.1236, -1.3892,  0.8199,\n",
            "         -1.1448,  0.6485, -1.3181,  1.1665],\n",
            "        [ 0.7725,  0.0128, -1.3659, -1.2108, -0.7429, -1.2297,  1.1523, -0.4716,\n",
            "          0.3109,  0.1910, -0.9266,  1.0238, -2.1456, -0.6738,  1.0446, -0.5442,\n",
            "          0.3964, -1.2238,  0.3938,  0.1714],\n",
            "        [ 1.0962, -0.3425, -0.5483, -0.8583,  1.6511, -2.2232, -0.3343,  0.4659,\n",
            "         -1.8835, -0.0124,  0.7429,  0.9006,  0.7873, -0.0212, -0.8961, -1.1260,\n",
            "          0.9030, -1.2687, -1.2938,  0.1706],\n",
            "        [-1.6195, -1.6439, -1.8805,  0.4887,  0.1697,  0.7583,  0.4875, -0.5384,\n",
            "          1.5194,  1.4373, -2.3469, -0.1671,  0.0049,  0.2699, -0.7570,  1.0993,\n",
            "         -1.2247,  0.6886,  0.4123, -1.5769],\n",
            "        [ 0.3436,  0.0415, -0.0457,  0.4826, -0.9219, -0.2363, -0.7651,  1.0568,\n",
            "         -1.7079, -1.5129,  0.6404, -0.8029,  1.2163,  1.2619,  0.3198, -1.9449,\n",
            "         -2.7750,  0.1623, -1.3971,  0.7852],\n",
            "        [-1.1324, -0.7208,  1.0175,  0.6374, -2.3545,  1.0327, -1.1388,  0.9310,\n",
            "          0.7835,  0.0530, -0.1421, -0.5623, -1.0387, -0.0098,  0.9592, -1.6213,\n",
            "         -2.5839, -2.1045,  1.0168, -1.0746],\n",
            "        [-0.7188,  0.9852, -1.4705, -2.8248, -1.1447, -1.2420,  1.1468, -0.6373,\n",
            "         -0.7640,  0.5162,  0.9580, -0.5807,  0.3974, -1.5667, -1.3419,  0.7125,\n",
            "          0.0813,  1.1581,  1.1370, -0.4066],\n",
            "        [ 0.2224,  0.8883,  0.7921,  0.9146, -1.9439,  0.6610, -1.4631, -0.5850,\n",
            "         -2.0915,  0.8703, -1.9873, -0.4685,  0.3999, -0.9412, -0.2764, -1.2245,\n",
            "          1.2132, -2.0189, -1.2983,  0.1119],\n",
            "        [-1.5577,  0.2209, -1.6288,  0.7052,  1.4567,  0.1988, -1.2531, -2.7358,\n",
            "         -1.1634, -1.4971,  0.5058, -0.8539,  0.6229,  1.2877, -1.5353,  0.9369,\n",
            "          0.8720,  0.3690, -0.4328, -0.8183]], requires_grad=True)\n",
            "fc2.bias : Parameter containing:\n",
            "tensor([-0.8354, -0.2809,  0.2633,  0.1603, -0.2948,  0.1657, -0.5023,  0.1356,\n",
            "         0.5366, -0.2676], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv90aYQ_8Yv5"
      },
      "source": [
        "def softmax_temperature(logits, temperature=0.00001):\n",
        "  pro = F.softmax(logits / temperature, dim=-1)\n",
        "  # pro = torch.matmul(pro, torch.FloatTensor(range(0, pro.shape[1])));\n",
        "  return pro;\n",
        "  # return one_hot_code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdJqT4Fu93Sz"
      },
      "source": [
        "class LUTModelWithAnnealTemperature(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1,hidden_size2, num_classes):\n",
        "        super(LUTModelWithAnnealTemperature, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.emb = nn.Embedding(hidden_size1, hidden_size2) \n",
        "        self.fc2 = nn.Linear(hidden_size2, num_classes)  \n",
        "    \n",
        "    def forward(self, x, temperature):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = softmax_temperature(out, temperature)\n",
        "        out = out @ self.emb.weight\n",
        "        # out = self.emb(out.long())\n",
        "        # out = self.emb(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "            out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1StIRCg-CeI",
        "outputId": "eecfceb9-176f-4d79-e8a6-21a398aaedee"
      },
      "source": [
        "# build the model \n",
        "model = LUTModelWithAnnealTemperature(784, 200, 20, 10)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# the epoch loop\n",
        "i = 0;\n",
        "for epoch in range(1000):\n",
        "    running_loss = 0.0\n",
        "    for data in trainloader:\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimiser.zero_grad()\n",
        "        # forward + loss + backward + optimise (update weights)\n",
        "        outputs = model(inputs, max(0.001, math.exp(-3 * math.pow(10, -5) * epoch)))\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        # print(optimiser.param_groups[0]['params'][2])\n",
        "        # print(np.all(optimiser.param_groups[0]['params'][0].grad.numpy() == 0))\n",
        "        # keep track of the loss this epoch\n",
        "        running_loss += loss.item()\n",
        "        # break\n",
        "    print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
        "print('**** Finished Training ****')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss 176.66\n",
            "Epoch 1, loss 99.29\n",
            "Epoch 2, loss 89.00\n",
            "Epoch 3, loss 79.73\n",
            "Epoch 4, loss 74.44\n",
            "Epoch 5, loss 71.47\n",
            "Epoch 6, loss 66.83\n",
            "Epoch 7, loss 63.73\n",
            "Epoch 8, loss 60.61\n",
            "Epoch 9, loss 58.66\n",
            "Epoch 10, loss 55.63\n",
            "Epoch 11, loss 55.16\n",
            "Epoch 12, loss 52.65\n",
            "Epoch 13, loss 50.11\n",
            "Epoch 14, loss 49.76\n",
            "Epoch 15, loss 48.17\n",
            "Epoch 16, loss 47.48\n",
            "Epoch 17, loss 44.40\n",
            "Epoch 18, loss 45.68\n",
            "Epoch 19, loss 45.54\n",
            "Epoch 20, loss 40.56\n",
            "Epoch 21, loss 40.36\n",
            "Epoch 22, loss 41.92\n",
            "Epoch 23, loss 39.60\n",
            "Epoch 24, loss 38.44\n",
            "Epoch 25, loss 38.38\n",
            "Epoch 26, loss 37.12\n",
            "Epoch 27, loss 35.88\n",
            "Epoch 28, loss 37.82\n",
            "Epoch 29, loss 35.65\n",
            "Epoch 30, loss 36.11\n",
            "Epoch 31, loss 36.57\n",
            "Epoch 32, loss 36.11\n",
            "Epoch 33, loss 34.79\n",
            "Epoch 34, loss 32.64\n",
            "Epoch 35, loss 32.09\n",
            "Epoch 36, loss 32.33\n",
            "Epoch 37, loss 31.19\n",
            "Epoch 38, loss 33.55\n",
            "Epoch 39, loss 31.71\n",
            "Epoch 40, loss 28.45\n",
            "Epoch 41, loss 29.45\n",
            "Epoch 42, loss 30.18\n",
            "Epoch 43, loss 30.30\n",
            "Epoch 44, loss 28.90\n",
            "Epoch 45, loss 30.54\n",
            "Epoch 46, loss 28.48\n",
            "Epoch 47, loss 28.83\n",
            "Epoch 48, loss 29.13\n",
            "Epoch 49, loss 27.55\n",
            "Epoch 50, loss 28.85\n",
            "Epoch 51, loss 29.83\n",
            "Epoch 52, loss 25.90\n",
            "Epoch 53, loss 26.55\n",
            "Epoch 54, loss 27.51\n",
            "Epoch 55, loss 27.59\n",
            "Epoch 56, loss 26.51\n",
            "Epoch 57, loss 24.94\n",
            "Epoch 58, loss 26.31\n",
            "Epoch 59, loss 26.97\n",
            "Epoch 60, loss 25.98\n",
            "Epoch 61, loss 25.01\n",
            "Epoch 62, loss 23.71\n",
            "Epoch 63, loss 24.52\n",
            "Epoch 64, loss 25.24\n",
            "Epoch 65, loss 26.22\n",
            "Epoch 66, loss 24.70\n",
            "Epoch 67, loss 24.90\n",
            "Epoch 68, loss 22.99\n",
            "Epoch 69, loss 23.51\n",
            "Epoch 70, loss 24.66\n",
            "Epoch 71, loss 23.46\n",
            "Epoch 72, loss 26.82\n",
            "Epoch 73, loss 22.29\n",
            "Epoch 74, loss 22.64\n",
            "Epoch 75, loss 23.73\n",
            "Epoch 76, loss 21.89\n",
            "Epoch 77, loss 22.08\n",
            "Epoch 78, loss 25.63\n",
            "Epoch 79, loss 24.14\n",
            "Epoch 80, loss 22.78\n",
            "Epoch 81, loss 21.76\n",
            "Epoch 82, loss 21.93\n",
            "Epoch 83, loss 23.72\n",
            "Epoch 84, loss 22.72\n",
            "Epoch 85, loss 22.27\n",
            "Epoch 86, loss 21.12\n",
            "Epoch 87, loss 20.76\n",
            "Epoch 88, loss 23.33\n",
            "Epoch 89, loss 22.37\n",
            "Epoch 90, loss 21.65\n",
            "Epoch 91, loss 21.71\n",
            "Epoch 92, loss 21.02\n",
            "Epoch 93, loss 21.02\n",
            "Epoch 94, loss 21.76\n",
            "Epoch 95, loss 20.89\n",
            "Epoch 96, loss 21.87\n",
            "Epoch 97, loss 20.56\n",
            "Epoch 98, loss 21.66\n",
            "Epoch 99, loss 21.33\n",
            "Epoch 100, loss 19.97\n",
            "Epoch 101, loss 18.82\n",
            "Epoch 102, loss 20.31\n",
            "Epoch 103, loss 20.15\n",
            "Epoch 104, loss 19.92\n",
            "Epoch 105, loss 20.18\n",
            "Epoch 106, loss 20.65\n",
            "Epoch 107, loss 18.92\n",
            "Epoch 108, loss 20.67\n",
            "Epoch 109, loss 20.74\n",
            "Epoch 110, loss 19.61\n",
            "Epoch 111, loss 20.11\n",
            "Epoch 112, loss 19.17\n",
            "Epoch 113, loss 19.67\n",
            "Epoch 114, loss 21.66\n",
            "Epoch 115, loss 19.00\n",
            "Epoch 116, loss 20.67\n",
            "Epoch 117, loss 18.98\n",
            "Epoch 118, loss 19.29\n",
            "Epoch 119, loss 18.84\n",
            "Epoch 120, loss 18.56\n",
            "Epoch 121, loss 18.87\n",
            "Epoch 122, loss 20.17\n",
            "Epoch 123, loss 19.53\n",
            "Epoch 124, loss 18.53\n",
            "Epoch 125, loss 17.55\n",
            "Epoch 126, loss 20.34\n",
            "Epoch 127, loss 18.74\n",
            "Epoch 128, loss 19.62\n",
            "Epoch 129, loss 19.55\n",
            "Epoch 130, loss 19.20\n",
            "Epoch 131, loss 19.42\n",
            "Epoch 132, loss 17.04\n",
            "Epoch 133, loss 18.75\n",
            "Epoch 134, loss 18.95\n",
            "Epoch 135, loss 18.24\n",
            "Epoch 136, loss 18.60\n",
            "Epoch 137, loss 18.49\n",
            "Epoch 138, loss 17.58\n",
            "Epoch 139, loss 19.42\n",
            "Epoch 140, loss 20.25\n",
            "Epoch 141, loss 18.53\n",
            "Epoch 142, loss 18.67\n",
            "Epoch 143, loss 18.14\n",
            "Epoch 144, loss 18.10\n",
            "Epoch 145, loss 19.90\n",
            "Epoch 146, loss 18.40\n",
            "Epoch 147, loss 18.45\n",
            "Epoch 148, loss 18.50\n",
            "Epoch 149, loss 17.35\n",
            "Epoch 150, loss 18.05\n",
            "Epoch 151, loss 18.71\n",
            "Epoch 152, loss 17.46\n",
            "Epoch 153, loss 17.90\n",
            "Epoch 154, loss 17.89\n",
            "Epoch 155, loss 17.93\n",
            "Epoch 156, loss 19.44\n",
            "Epoch 157, loss 17.87\n",
            "Epoch 158, loss 17.78\n",
            "Epoch 159, loss 18.64\n",
            "Epoch 160, loss 18.41\n",
            "Epoch 161, loss 17.84\n",
            "Epoch 162, loss 20.54\n",
            "Epoch 163, loss 19.65\n",
            "Epoch 164, loss 17.90\n",
            "Epoch 165, loss 17.57\n",
            "Epoch 166, loss 17.34\n",
            "Epoch 167, loss 18.55\n",
            "Epoch 168, loss 19.28\n",
            "Epoch 169, loss 17.42\n",
            "Epoch 170, loss 17.61\n",
            "Epoch 171, loss 17.74\n",
            "Epoch 172, loss 18.71\n",
            "Epoch 173, loss 16.52\n",
            "Epoch 174, loss 17.85\n",
            "Epoch 175, loss 18.14\n",
            "Epoch 176, loss 17.67\n",
            "Epoch 177, loss 18.33\n",
            "Epoch 178, loss 17.58\n",
            "Epoch 179, loss 18.03\n",
            "Epoch 180, loss 16.91\n",
            "Epoch 181, loss 17.13\n",
            "Epoch 182, loss 16.79\n",
            "Epoch 183, loss 15.61\n",
            "Epoch 184, loss 17.92\n",
            "Epoch 185, loss 17.79\n",
            "Epoch 186, loss 18.55\n",
            "Epoch 187, loss 17.12\n",
            "Epoch 188, loss 15.90\n",
            "Epoch 189, loss 16.22\n",
            "Epoch 190, loss 17.09\n",
            "Epoch 191, loss 17.40\n",
            "Epoch 192, loss 17.97\n",
            "Epoch 193, loss 16.50\n",
            "Epoch 194, loss 15.69\n",
            "Epoch 195, loss 18.09\n",
            "Epoch 196, loss 15.85\n",
            "Epoch 197, loss 17.43\n",
            "Epoch 198, loss 16.87\n",
            "Epoch 199, loss 17.57\n",
            "Epoch 200, loss 16.54\n",
            "Epoch 201, loss 15.40\n",
            "Epoch 202, loss 16.42\n",
            "Epoch 203, loss 17.62\n",
            "Epoch 204, loss 18.29\n",
            "Epoch 205, loss 17.48\n",
            "Epoch 206, loss 17.68\n",
            "Epoch 207, loss 16.41\n",
            "Epoch 208, loss 15.59\n",
            "Epoch 209, loss 15.55\n",
            "Epoch 210, loss 16.44\n",
            "Epoch 211, loss 18.03\n",
            "Epoch 212, loss 17.17\n",
            "Epoch 213, loss 16.54\n",
            "Epoch 214, loss 17.77\n",
            "Epoch 215, loss 18.56\n",
            "Epoch 216, loss 16.64\n",
            "Epoch 217, loss 15.45\n",
            "Epoch 218, loss 18.58\n",
            "Epoch 219, loss 18.23\n",
            "Epoch 220, loss 16.10\n",
            "Epoch 221, loss 16.95\n",
            "Epoch 222, loss 16.53\n",
            "Epoch 223, loss 17.58\n",
            "Epoch 224, loss 17.45\n",
            "Epoch 225, loss 15.70\n",
            "Epoch 226, loss 15.94\n",
            "Epoch 227, loss 15.52\n",
            "Epoch 228, loss 16.94\n",
            "Epoch 229, loss 16.83\n",
            "Epoch 230, loss 15.97\n",
            "Epoch 231, loss 16.86\n",
            "Epoch 232, loss 16.43\n",
            "Epoch 233, loss 15.51\n",
            "Epoch 234, loss 15.07\n",
            "Epoch 235, loss 15.69\n",
            "Epoch 236, loss 15.96\n",
            "Epoch 237, loss 16.40\n",
            "Epoch 238, loss 16.09\n",
            "Epoch 239, loss 15.39\n",
            "Epoch 240, loss 17.09\n",
            "Epoch 241, loss 16.72\n",
            "Epoch 242, loss 15.47\n",
            "Epoch 243, loss 15.92\n",
            "Epoch 244, loss 16.81\n",
            "Epoch 245, loss 16.03\n",
            "Epoch 246, loss 16.04\n",
            "Epoch 247, loss 15.94\n",
            "Epoch 248, loss 15.71\n",
            "Epoch 249, loss 16.00\n",
            "Epoch 250, loss 16.44\n",
            "Epoch 251, loss 15.86\n",
            "Epoch 252, loss 16.86\n",
            "Epoch 253, loss 15.67\n",
            "Epoch 254, loss 15.66\n",
            "Epoch 255, loss 15.64\n",
            "Epoch 256, loss 15.85\n",
            "Epoch 257, loss 14.94\n",
            "Epoch 258, loss 15.77\n",
            "Epoch 259, loss 14.74\n",
            "Epoch 260, loss 16.87\n",
            "Epoch 261, loss 16.89\n",
            "Epoch 262, loss 15.69\n",
            "Epoch 263, loss 15.72\n",
            "Epoch 264, loss 17.95\n",
            "Epoch 265, loss 16.31\n",
            "Epoch 266, loss 15.32\n",
            "Epoch 267, loss 15.03\n",
            "Epoch 268, loss 16.31\n",
            "Epoch 269, loss 16.14\n",
            "Epoch 270, loss 14.80\n",
            "Epoch 271, loss 16.05\n",
            "Epoch 272, loss 16.50\n",
            "Epoch 273, loss 15.66\n",
            "Epoch 274, loss 17.33\n",
            "Epoch 275, loss 14.65\n",
            "Epoch 276, loss 14.80\n",
            "Epoch 277, loss 15.17\n",
            "Epoch 278, loss 14.92\n",
            "Epoch 279, loss 15.20\n",
            "Epoch 280, loss 15.21\n",
            "Epoch 281, loss 16.15\n",
            "Epoch 282, loss 14.67\n",
            "Epoch 283, loss 16.13\n",
            "Epoch 284, loss 15.53\n",
            "Epoch 285, loss 15.35\n",
            "Epoch 286, loss 15.04\n",
            "Epoch 287, loss 14.95\n",
            "Epoch 288, loss 15.44\n",
            "Epoch 289, loss 15.31\n",
            "Epoch 290, loss 16.23\n",
            "Epoch 291, loss 14.97\n",
            "Epoch 292, loss 14.56\n",
            "Epoch 293, loss 14.84\n",
            "Epoch 294, loss 14.36\n",
            "Epoch 295, loss 15.96\n",
            "Epoch 296, loss 14.47\n",
            "Epoch 297, loss 15.79\n",
            "Epoch 298, loss 15.54\n",
            "Epoch 299, loss 16.20\n",
            "Epoch 300, loss 14.38\n",
            "Epoch 301, loss 14.55\n",
            "Epoch 302, loss 14.82\n",
            "Epoch 303, loss 16.72\n",
            "Epoch 304, loss 15.73\n",
            "Epoch 305, loss 14.58\n",
            "Epoch 306, loss 14.40\n",
            "Epoch 307, loss 13.86\n",
            "Epoch 308, loss 15.86\n",
            "Epoch 309, loss 14.62\n",
            "Epoch 310, loss 16.75\n",
            "Epoch 311, loss 16.79\n",
            "Epoch 312, loss 15.17\n",
            "Epoch 313, loss 14.61\n",
            "Epoch 314, loss 14.76\n",
            "Epoch 315, loss 15.17\n",
            "Epoch 316, loss 14.68\n",
            "Epoch 317, loss 14.24\n",
            "Epoch 318, loss 15.98\n",
            "Epoch 319, loss 15.58\n",
            "Epoch 320, loss 15.54\n",
            "Epoch 321, loss 16.19\n",
            "Epoch 322, loss 15.70\n",
            "Epoch 323, loss 15.24\n",
            "Epoch 324, loss 15.99\n",
            "Epoch 325, loss 15.00\n",
            "Epoch 326, loss 15.00\n",
            "Epoch 327, loss 15.25\n",
            "Epoch 328, loss 15.01\n",
            "Epoch 329, loss 15.91\n",
            "Epoch 330, loss 15.47\n",
            "Epoch 331, loss 15.68\n",
            "Epoch 332, loss 14.97\n",
            "Epoch 333, loss 15.90\n",
            "Epoch 334, loss 15.04\n",
            "Epoch 335, loss 16.47\n",
            "Epoch 336, loss 15.78\n",
            "Epoch 337, loss 14.49\n",
            "Epoch 338, loss 15.24\n",
            "Epoch 339, loss 14.20\n",
            "Epoch 340, loss 15.30\n",
            "Epoch 341, loss 14.55\n",
            "Epoch 342, loss 13.97\n",
            "Epoch 343, loss 15.22\n",
            "Epoch 344, loss 15.67\n",
            "Epoch 345, loss 15.03\n",
            "Epoch 346, loss 16.11\n",
            "Epoch 347, loss 14.50\n",
            "Epoch 348, loss 15.14\n",
            "Epoch 349, loss 14.72\n",
            "Epoch 350, loss 15.15\n",
            "Epoch 351, loss 13.97\n",
            "Epoch 352, loss 14.40\n",
            "Epoch 353, loss 15.05\n",
            "Epoch 354, loss 16.21\n",
            "Epoch 355, loss 17.06\n",
            "Epoch 356, loss 14.32\n",
            "Epoch 357, loss 16.06\n",
            "Epoch 358, loss 15.47\n",
            "Epoch 359, loss 14.32\n",
            "Epoch 360, loss 16.04\n",
            "Epoch 361, loss 15.81\n",
            "Epoch 362, loss 14.91\n",
            "Epoch 363, loss 14.39\n",
            "Epoch 364, loss 14.75\n",
            "Epoch 365, loss 14.40\n",
            "Epoch 366, loss 15.51\n",
            "Epoch 367, loss 15.62\n",
            "Epoch 368, loss 15.00\n",
            "Epoch 369, loss 14.08\n",
            "Epoch 370, loss 15.55\n",
            "Epoch 371, loss 15.82\n",
            "Epoch 372, loss 14.92\n",
            "Epoch 373, loss 14.60\n",
            "Epoch 374, loss 15.57\n",
            "Epoch 375, loss 14.52\n",
            "Epoch 376, loss 15.08\n",
            "Epoch 377, loss 15.12\n",
            "Epoch 378, loss 16.42\n",
            "Epoch 379, loss 13.85\n",
            "Epoch 380, loss 14.01\n",
            "Epoch 381, loss 14.24\n",
            "Epoch 382, loss 13.97\n",
            "Epoch 383, loss 14.30\n",
            "Epoch 384, loss 15.18\n",
            "Epoch 385, loss 15.56\n",
            "Epoch 386, loss 15.15\n",
            "Epoch 387, loss 14.65\n",
            "Epoch 388, loss 14.41\n",
            "Epoch 389, loss 14.32\n",
            "Epoch 390, loss 14.02\n",
            "Epoch 391, loss 15.11\n",
            "Epoch 392, loss 13.76\n",
            "Epoch 393, loss 14.35\n",
            "Epoch 394, loss 15.40\n",
            "Epoch 395, loss 13.71\n",
            "Epoch 396, loss 14.45\n",
            "Epoch 397, loss 14.57\n",
            "Epoch 398, loss 14.54\n",
            "Epoch 399, loss 14.69\n",
            "Epoch 400, loss 13.95\n",
            "Epoch 401, loss 16.00\n",
            "Epoch 402, loss 15.14\n",
            "Epoch 403, loss 14.07\n",
            "Epoch 404, loss 14.92\n",
            "Epoch 405, loss 14.68\n",
            "Epoch 406, loss 13.24\n",
            "Epoch 407, loss 14.15\n",
            "Epoch 408, loss 15.19\n",
            "Epoch 409, loss 14.52\n",
            "Epoch 410, loss 13.98\n",
            "Epoch 411, loss 14.66\n",
            "Epoch 412, loss 14.89\n",
            "Epoch 413, loss 15.95\n",
            "Epoch 414, loss 14.75\n",
            "Epoch 415, loss 14.84\n",
            "Epoch 416, loss 14.06\n",
            "Epoch 417, loss 14.35\n",
            "Epoch 418, loss 14.72\n",
            "Epoch 419, loss 15.05\n",
            "Epoch 420, loss 14.33\n",
            "Epoch 421, loss 14.50\n",
            "Epoch 422, loss 13.96\n",
            "Epoch 423, loss 15.20\n",
            "Epoch 424, loss 15.49\n",
            "Epoch 425, loss 15.29\n",
            "Epoch 426, loss 15.13\n",
            "Epoch 427, loss 14.52\n",
            "Epoch 428, loss 13.76\n",
            "Epoch 429, loss 14.12\n",
            "Epoch 430, loss 15.44\n",
            "Epoch 431, loss 15.46\n",
            "Epoch 432, loss 13.63\n",
            "Epoch 433, loss 14.06\n",
            "Epoch 434, loss 15.12\n",
            "Epoch 435, loss 15.51\n",
            "Epoch 436, loss 14.55\n",
            "Epoch 437, loss 14.60\n",
            "Epoch 438, loss 14.44\n",
            "Epoch 439, loss 14.14\n",
            "Epoch 440, loss 14.39\n",
            "Epoch 441, loss 14.32\n",
            "Epoch 442, loss 14.23\n",
            "Epoch 443, loss 15.00\n",
            "Epoch 444, loss 14.70\n",
            "Epoch 445, loss 13.70\n",
            "Epoch 446, loss 14.85\n",
            "Epoch 447, loss 14.00\n",
            "Epoch 448, loss 13.88\n",
            "Epoch 449, loss 16.14\n",
            "Epoch 450, loss 14.32\n",
            "Epoch 451, loss 13.96\n",
            "Epoch 452, loss 13.54\n",
            "Epoch 453, loss 14.40\n",
            "Epoch 454, loss 13.43\n",
            "Epoch 455, loss 16.92\n",
            "Epoch 456, loss 15.24\n",
            "Epoch 457, loss 14.93\n",
            "Epoch 458, loss 14.00\n",
            "Epoch 459, loss 14.12\n",
            "Epoch 460, loss 14.04\n",
            "Epoch 461, loss 14.63\n",
            "Epoch 462, loss 13.97\n",
            "Epoch 463, loss 13.89\n",
            "Epoch 464, loss 14.36\n",
            "Epoch 465, loss 14.34\n",
            "Epoch 466, loss 15.60\n",
            "Epoch 467, loss 13.75\n",
            "Epoch 468, loss 14.79\n",
            "Epoch 469, loss 14.67\n",
            "Epoch 470, loss 14.75\n",
            "Epoch 471, loss 13.90\n",
            "Epoch 472, loss 14.55\n",
            "Epoch 473, loss 14.76\n",
            "Epoch 474, loss 14.14\n",
            "Epoch 475, loss 13.73\n",
            "Epoch 476, loss 13.62\n",
            "Epoch 477, loss 13.98\n",
            "Epoch 478, loss 14.29\n",
            "Epoch 479, loss 14.36\n",
            "Epoch 480, loss 14.07\n",
            "Epoch 481, loss 14.48\n",
            "Epoch 482, loss 14.21\n",
            "Epoch 483, loss 14.40\n",
            "Epoch 484, loss 14.28\n",
            "Epoch 485, loss 15.43\n",
            "Epoch 486, loss 15.05\n",
            "Epoch 487, loss 14.19\n",
            "Epoch 488, loss 14.75\n",
            "Epoch 489, loss 14.72\n",
            "Epoch 490, loss 14.71\n",
            "Epoch 491, loss 14.03\n",
            "Epoch 492, loss 14.95\n",
            "Epoch 493, loss 15.60\n",
            "Epoch 494, loss 14.48\n",
            "Epoch 495, loss 13.68\n",
            "Epoch 496, loss 14.38\n",
            "Epoch 497, loss 13.91\n",
            "Epoch 498, loss 13.76\n",
            "Epoch 499, loss 13.90\n",
            "Epoch 500, loss 14.04\n",
            "Epoch 501, loss 14.98\n",
            "Epoch 502, loss 14.73\n",
            "Epoch 503, loss 13.82\n",
            "Epoch 504, loss 14.75\n",
            "Epoch 505, loss 14.49\n",
            "Epoch 506, loss 14.53\n",
            "Epoch 507, loss 14.96\n",
            "Epoch 508, loss 14.57\n",
            "Epoch 509, loss 14.81\n",
            "Epoch 510, loss 13.52\n",
            "Epoch 511, loss 15.52\n",
            "Epoch 512, loss 13.72\n",
            "Epoch 513, loss 13.26\n",
            "Epoch 514, loss 13.63\n",
            "Epoch 515, loss 13.57\n",
            "Epoch 516, loss 13.04\n",
            "Epoch 517, loss 14.52\n",
            "Epoch 518, loss 14.14\n",
            "Epoch 519, loss 13.08\n",
            "Epoch 520, loss 14.00\n",
            "Epoch 521, loss 13.22\n",
            "Epoch 522, loss 14.82\n",
            "Epoch 523, loss 14.01\n",
            "Epoch 524, loss 14.99\n",
            "Epoch 525, loss 16.66\n",
            "Epoch 526, loss 14.24\n",
            "Epoch 527, loss 13.93\n",
            "Epoch 528, loss 14.14\n",
            "Epoch 529, loss 14.02\n",
            "Epoch 530, loss 15.80\n",
            "Epoch 531, loss 15.05\n",
            "Epoch 532, loss 13.62\n",
            "Epoch 533, loss 13.60\n",
            "Epoch 534, loss 13.54\n",
            "Epoch 535, loss 13.01\n",
            "Epoch 536, loss 12.29\n",
            "Epoch 537, loss 13.84\n",
            "Epoch 538, loss 14.77\n",
            "Epoch 539, loss 14.54\n",
            "Epoch 540, loss 15.32\n",
            "Epoch 541, loss 14.11\n",
            "Epoch 542, loss 13.10\n",
            "Epoch 543, loss 13.81\n",
            "Epoch 544, loss 13.84\n",
            "Epoch 545, loss 14.92\n",
            "Epoch 546, loss 13.94\n",
            "Epoch 547, loss 14.88\n",
            "Epoch 548, loss 13.86\n",
            "Epoch 549, loss 13.37\n",
            "Epoch 550, loss 13.62\n",
            "Epoch 551, loss 14.19\n",
            "Epoch 552, loss 12.71\n",
            "Epoch 553, loss 13.62\n",
            "Epoch 554, loss 12.89\n",
            "Epoch 555, loss 12.98\n",
            "Epoch 556, loss 14.09\n",
            "Epoch 557, loss 13.96\n",
            "Epoch 558, loss 15.47\n",
            "Epoch 559, loss 13.40\n",
            "Epoch 560, loss 13.62\n",
            "Epoch 561, loss 13.71\n",
            "Epoch 562, loss 13.57\n",
            "Epoch 563, loss 14.28\n",
            "Epoch 564, loss 13.84\n",
            "Epoch 565, loss 13.30\n",
            "Epoch 566, loss 13.99\n",
            "Epoch 567, loss 13.96\n",
            "Epoch 568, loss 13.72\n",
            "Epoch 569, loss 13.57\n",
            "Epoch 570, loss 14.25\n",
            "Epoch 571, loss 13.96\n",
            "Epoch 572, loss 14.05\n",
            "Epoch 573, loss 13.64\n",
            "Epoch 574, loss 14.61\n",
            "Epoch 575, loss 13.63\n",
            "Epoch 576, loss 13.70\n",
            "Epoch 577, loss 14.93\n",
            "Epoch 578, loss 13.23\n",
            "Epoch 579, loss 14.44\n",
            "Epoch 580, loss 13.51\n",
            "Epoch 581, loss 13.05\n",
            "Epoch 582, loss 15.08\n",
            "Epoch 583, loss 13.72\n",
            "Epoch 584, loss 13.69\n",
            "Epoch 585, loss 13.78\n",
            "Epoch 586, loss 14.83\n",
            "Epoch 587, loss 13.70\n",
            "Epoch 588, loss 14.84\n",
            "Epoch 589, loss 15.37\n",
            "Epoch 590, loss 14.20\n",
            "Epoch 591, loss 13.64\n",
            "Epoch 592, loss 14.74\n",
            "Epoch 593, loss 14.40\n",
            "Epoch 594, loss 13.80\n",
            "Epoch 595, loss 14.34\n",
            "Epoch 596, loss 14.29\n",
            "Epoch 597, loss 13.23\n",
            "Epoch 598, loss 13.88\n",
            "Epoch 599, loss 15.08\n",
            "Epoch 600, loss 13.37\n",
            "Epoch 601, loss 13.48\n",
            "Epoch 602, loss 13.41\n",
            "Epoch 603, loss 13.53\n",
            "Epoch 604, loss 13.77\n",
            "Epoch 605, loss 14.17\n",
            "Epoch 606, loss 14.20\n",
            "Epoch 607, loss 14.29\n",
            "Epoch 608, loss 15.25\n",
            "Epoch 609, loss 13.66\n",
            "Epoch 610, loss 14.03\n",
            "Epoch 611, loss 13.28\n",
            "Epoch 612, loss 13.40\n",
            "Epoch 613, loss 13.35\n",
            "Epoch 614, loss 14.23\n",
            "Epoch 615, loss 13.54\n",
            "Epoch 616, loss 13.69\n",
            "Epoch 617, loss 13.76\n",
            "Epoch 618, loss 14.67\n",
            "Epoch 619, loss 14.45\n",
            "Epoch 620, loss 14.96\n",
            "Epoch 621, loss 14.05\n",
            "Epoch 622, loss 13.68\n",
            "Epoch 623, loss 13.42\n",
            "Epoch 624, loss 13.62\n",
            "Epoch 625, loss 14.17\n",
            "Epoch 626, loss 13.41\n",
            "Epoch 627, loss 14.20\n",
            "Epoch 628, loss 13.00\n",
            "Epoch 629, loss 12.74\n",
            "Epoch 630, loss 13.15\n",
            "Epoch 631, loss 14.45\n",
            "Epoch 632, loss 13.14\n",
            "Epoch 633, loss 14.75\n",
            "Epoch 634, loss 13.98\n",
            "Epoch 635, loss 14.20\n",
            "Epoch 636, loss 13.30\n",
            "Epoch 637, loss 13.31\n",
            "Epoch 638, loss 13.87\n",
            "Epoch 639, loss 13.36\n",
            "Epoch 640, loss 12.88\n",
            "Epoch 641, loss 14.04\n",
            "Epoch 642, loss 13.86\n",
            "Epoch 643, loss 14.10\n",
            "Epoch 644, loss 13.41\n",
            "Epoch 645, loss 13.45\n",
            "Epoch 646, loss 13.01\n",
            "Epoch 647, loss 13.73\n",
            "Epoch 648, loss 14.10\n",
            "Epoch 649, loss 13.64\n",
            "Epoch 650, loss 14.94\n",
            "Epoch 651, loss 13.41\n",
            "Epoch 652, loss 13.38\n",
            "Epoch 653, loss 14.11\n",
            "Epoch 654, loss 13.30\n",
            "Epoch 655, loss 13.07\n",
            "Epoch 656, loss 13.91\n",
            "Epoch 657, loss 13.54\n",
            "Epoch 658, loss 14.81\n",
            "Epoch 659, loss 13.23\n",
            "Epoch 660, loss 12.63\n",
            "Epoch 661, loss 13.81\n",
            "Epoch 662, loss 13.82\n",
            "Epoch 663, loss 14.30\n",
            "Epoch 664, loss 13.29\n",
            "Epoch 665, loss 14.15\n",
            "Epoch 666, loss 13.84\n",
            "Epoch 667, loss 14.38\n",
            "Epoch 668, loss 13.49\n",
            "Epoch 669, loss 13.23\n",
            "Epoch 670, loss 13.03\n",
            "Epoch 671, loss 14.04\n",
            "Epoch 672, loss 13.28\n",
            "Epoch 673, loss 13.90\n",
            "Epoch 674, loss 13.50\n",
            "Epoch 675, loss 13.11\n",
            "Epoch 676, loss 14.26\n",
            "Epoch 677, loss 13.35\n",
            "Epoch 678, loss 13.60\n",
            "Epoch 679, loss 12.79\n",
            "Epoch 680, loss 13.53\n",
            "Epoch 681, loss 12.98\n",
            "Epoch 682, loss 14.11\n",
            "Epoch 683, loss 16.07\n",
            "Epoch 684, loss 13.81\n",
            "Epoch 685, loss 13.46\n",
            "Epoch 686, loss 13.06\n",
            "Epoch 687, loss 14.10\n",
            "Epoch 688, loss 13.22\n",
            "Epoch 689, loss 13.16\n",
            "Epoch 690, loss 13.40\n",
            "Epoch 691, loss 13.81\n",
            "Epoch 692, loss 14.28\n",
            "Epoch 693, loss 13.27\n",
            "Epoch 694, loss 12.86\n",
            "Epoch 695, loss 13.90\n",
            "Epoch 696, loss 14.72\n",
            "Epoch 697, loss 13.43\n",
            "Epoch 698, loss 12.82\n",
            "Epoch 699, loss 13.46\n",
            "Epoch 700, loss 13.54\n",
            "Epoch 701, loss 13.84\n",
            "Epoch 702, loss 13.12\n",
            "Epoch 703, loss 13.82\n",
            "Epoch 704, loss 13.17\n",
            "Epoch 705, loss 13.86\n",
            "Epoch 706, loss 13.60\n",
            "Epoch 707, loss 13.92\n",
            "Epoch 708, loss 13.97\n",
            "Epoch 709, loss 13.42\n",
            "Epoch 710, loss 14.14\n",
            "Epoch 711, loss 13.02\n",
            "Epoch 712, loss 14.05\n",
            "Epoch 713, loss 13.07\n",
            "Epoch 714, loss 14.02\n",
            "Epoch 715, loss 13.68\n",
            "Epoch 716, loss 13.83\n",
            "Epoch 717, loss 13.99\n",
            "Epoch 718, loss 14.19\n",
            "Epoch 719, loss 13.33\n",
            "Epoch 720, loss 13.74\n",
            "Epoch 721, loss 13.79\n",
            "Epoch 722, loss 13.06\n",
            "Epoch 723, loss 13.55\n",
            "Epoch 724, loss 13.75\n",
            "Epoch 725, loss 13.63\n",
            "Epoch 726, loss 13.37\n",
            "Epoch 727, loss 13.58\n",
            "Epoch 728, loss 13.49\n",
            "Epoch 729, loss 13.14\n",
            "Epoch 730, loss 13.84\n",
            "Epoch 731, loss 13.38\n",
            "Epoch 732, loss 13.47\n",
            "Epoch 733, loss 13.14\n",
            "Epoch 734, loss 13.51\n",
            "Epoch 735, loss 14.63\n",
            "Epoch 736, loss 12.92\n",
            "Epoch 737, loss 13.62\n",
            "Epoch 738, loss 13.51\n",
            "Epoch 739, loss 14.33\n",
            "Epoch 740, loss 14.97\n",
            "Epoch 741, loss 14.71\n",
            "Epoch 742, loss 14.10\n",
            "Epoch 743, loss 13.69\n",
            "Epoch 744, loss 14.11\n",
            "Epoch 745, loss 13.60\n",
            "Epoch 746, loss 13.67\n",
            "Epoch 747, loss 13.35\n",
            "Epoch 748, loss 13.51\n",
            "Epoch 749, loss 13.27\n",
            "Epoch 750, loss 13.39\n",
            "Epoch 751, loss 13.68\n",
            "Epoch 752, loss 14.35\n",
            "Epoch 753, loss 12.99\n",
            "Epoch 754, loss 13.27\n",
            "Epoch 755, loss 13.66\n",
            "Epoch 756, loss 12.81\n",
            "Epoch 757, loss 13.16\n",
            "Epoch 758, loss 13.14\n",
            "Epoch 759, loss 13.66\n",
            "Epoch 760, loss 14.43\n",
            "Epoch 761, loss 13.88\n",
            "Epoch 762, loss 12.65\n",
            "Epoch 763, loss 13.19\n",
            "Epoch 764, loss 13.56\n",
            "Epoch 765, loss 13.72\n",
            "Epoch 766, loss 15.48\n",
            "Epoch 767, loss 13.13\n",
            "Epoch 768, loss 13.07\n",
            "Epoch 769, loss 13.92\n",
            "Epoch 770, loss 14.44\n",
            "Epoch 771, loss 13.86\n",
            "Epoch 772, loss 12.72\n",
            "Epoch 773, loss 13.12\n",
            "Epoch 774, loss 13.55\n",
            "Epoch 775, loss 13.45\n",
            "Epoch 776, loss 13.16\n",
            "Epoch 777, loss 13.01\n",
            "Epoch 778, loss 13.63\n",
            "Epoch 779, loss 13.63\n",
            "Epoch 780, loss 13.34\n",
            "Epoch 781, loss 13.61\n",
            "Epoch 782, loss 13.90\n",
            "Epoch 783, loss 14.22\n",
            "Epoch 784, loss 13.59\n",
            "Epoch 785, loss 12.84\n",
            "Epoch 786, loss 14.05\n",
            "Epoch 787, loss 14.12\n",
            "Epoch 788, loss 13.74\n",
            "Epoch 789, loss 15.06\n",
            "Epoch 790, loss 13.50\n",
            "Epoch 791, loss 14.00\n",
            "Epoch 792, loss 12.85\n",
            "Epoch 793, loss 13.01\n",
            "Epoch 794, loss 14.14\n",
            "Epoch 795, loss 13.74\n",
            "Epoch 796, loss 13.48\n",
            "Epoch 797, loss 12.88\n",
            "Epoch 798, loss 13.60\n",
            "Epoch 799, loss 13.79\n",
            "Epoch 800, loss 13.50\n",
            "Epoch 801, loss 13.60\n",
            "Epoch 802, loss 13.33\n",
            "Epoch 803, loss 14.61\n",
            "Epoch 804, loss 13.64\n",
            "Epoch 805, loss 12.76\n",
            "Epoch 806, loss 12.98\n",
            "Epoch 807, loss 14.14\n",
            "Epoch 808, loss 14.20\n",
            "Epoch 809, loss 13.74\n",
            "Epoch 810, loss 14.09\n",
            "Epoch 811, loss 13.39\n",
            "Epoch 812, loss 12.90\n",
            "Epoch 813, loss 13.72\n",
            "Epoch 814, loss 13.27\n",
            "Epoch 815, loss 13.83\n",
            "Epoch 816, loss 13.03\n",
            "Epoch 817, loss 14.14\n",
            "Epoch 818, loss 13.83\n",
            "Epoch 819, loss 14.41\n",
            "Epoch 820, loss 13.19\n",
            "Epoch 821, loss 14.09\n",
            "Epoch 822, loss 12.83\n",
            "Epoch 823, loss 12.58\n",
            "Epoch 824, loss 13.17\n",
            "Epoch 825, loss 13.27\n",
            "Epoch 826, loss 13.35\n",
            "Epoch 827, loss 13.26\n",
            "Epoch 828, loss 13.26\n",
            "Epoch 829, loss 14.35\n",
            "Epoch 830, loss 13.62\n",
            "Epoch 831, loss 13.40\n",
            "Epoch 832, loss 13.30\n",
            "Epoch 833, loss 12.48\n",
            "Epoch 834, loss 13.42\n",
            "Epoch 835, loss 13.63\n",
            "Epoch 836, loss 13.48\n",
            "Epoch 837, loss 13.43\n",
            "Epoch 838, loss 13.75\n",
            "Epoch 839, loss 13.91\n",
            "Epoch 840, loss 12.86\n",
            "Epoch 841, loss 12.76\n",
            "Epoch 842, loss 12.98\n",
            "Epoch 843, loss 13.16\n",
            "Epoch 844, loss 12.80\n",
            "Epoch 845, loss 12.38\n",
            "Epoch 846, loss 14.74\n",
            "Epoch 847, loss 12.62\n",
            "Epoch 848, loss 13.79\n",
            "Epoch 849, loss 14.01\n",
            "Epoch 850, loss 13.54\n",
            "Epoch 851, loss 13.49\n",
            "Epoch 852, loss 13.18\n",
            "Epoch 853, loss 13.38\n",
            "Epoch 854, loss 13.48\n",
            "Epoch 855, loss 12.67\n",
            "Epoch 856, loss 12.80\n",
            "Epoch 857, loss 13.05\n",
            "Epoch 858, loss 13.60\n",
            "Epoch 859, loss 13.66\n",
            "Epoch 860, loss 12.32\n",
            "Epoch 861, loss 13.27\n",
            "Epoch 862, loss 12.62\n",
            "Epoch 863, loss 13.52\n",
            "Epoch 864, loss 13.22\n",
            "Epoch 865, loss 13.03\n",
            "Epoch 866, loss 12.99\n",
            "Epoch 867, loss 13.56\n",
            "Epoch 868, loss 12.87\n",
            "Epoch 869, loss 13.43\n",
            "Epoch 870, loss 13.05\n",
            "Epoch 871, loss 12.98\n",
            "Epoch 872, loss 13.33\n",
            "Epoch 873, loss 12.53\n",
            "Epoch 874, loss 13.91\n",
            "Epoch 875, loss 12.70\n",
            "Epoch 876, loss 13.54\n",
            "Epoch 877, loss 13.33\n",
            "Epoch 878, loss 12.82\n",
            "Epoch 879, loss 13.08\n",
            "Epoch 880, loss 13.31\n",
            "Epoch 881, loss 12.83\n",
            "Epoch 882, loss 13.26\n",
            "Epoch 883, loss 12.56\n",
            "Epoch 884, loss 13.64\n",
            "Epoch 885, loss 13.57\n",
            "Epoch 886, loss 12.56\n",
            "Epoch 887, loss 12.03\n",
            "Epoch 888, loss 12.88\n",
            "Epoch 889, loss 12.71\n",
            "Epoch 890, loss 11.66\n",
            "Epoch 891, loss 13.72\n",
            "Epoch 892, loss 13.82\n",
            "Epoch 893, loss 13.39\n",
            "Epoch 894, loss 12.75\n",
            "Epoch 895, loss 14.65\n",
            "Epoch 896, loss 12.97\n",
            "Epoch 897, loss 13.45\n",
            "Epoch 898, loss 13.21\n",
            "Epoch 899, loss 13.01\n",
            "Epoch 900, loss 12.62\n",
            "Epoch 901, loss 13.33\n",
            "Epoch 902, loss 12.36\n",
            "Epoch 903, loss 12.73\n",
            "Epoch 904, loss 13.93\n",
            "Epoch 905, loss 13.11\n",
            "Epoch 906, loss 12.68\n",
            "Epoch 907, loss 12.43\n",
            "Epoch 908, loss 12.55\n",
            "Epoch 909, loss 13.49\n",
            "Epoch 910, loss 13.78\n",
            "Epoch 911, loss 13.74\n",
            "Epoch 912, loss 12.32\n",
            "Epoch 913, loss 12.92\n",
            "Epoch 914, loss 12.75\n",
            "Epoch 915, loss 14.05\n",
            "Epoch 916, loss 13.03\n",
            "Epoch 917, loss 12.76\n",
            "Epoch 918, loss 12.91\n",
            "Epoch 919, loss 15.15\n",
            "Epoch 920, loss 14.32\n",
            "Epoch 921, loss 12.78\n",
            "Epoch 922, loss 14.17\n",
            "Epoch 923, loss 13.41\n",
            "Epoch 924, loss 12.24\n",
            "Epoch 925, loss 13.67\n",
            "Epoch 926, loss 12.95\n",
            "Epoch 927, loss 12.82\n",
            "Epoch 928, loss 14.02\n",
            "Epoch 929, loss 13.18\n",
            "Epoch 930, loss 13.31\n",
            "Epoch 931, loss 13.81\n",
            "Epoch 932, loss 14.33\n",
            "Epoch 933, loss 12.65\n",
            "Epoch 934, loss 12.91\n",
            "Epoch 935, loss 12.74\n",
            "Epoch 936, loss 12.82\n",
            "Epoch 937, loss 13.14\n",
            "Epoch 938, loss 14.97\n",
            "Epoch 939, loss 14.36\n",
            "Epoch 940, loss 13.19\n",
            "Epoch 941, loss 14.11\n",
            "Epoch 942, loss 14.54\n",
            "Epoch 943, loss 13.10\n",
            "Epoch 944, loss 12.13\n",
            "Epoch 945, loss 12.07\n",
            "Epoch 946, loss 13.47\n",
            "Epoch 947, loss 12.83\n",
            "Epoch 948, loss 12.70\n",
            "Epoch 949, loss 13.05\n",
            "Epoch 950, loss 12.84\n",
            "Epoch 951, loss 13.41\n",
            "Epoch 952, loss 12.88\n",
            "Epoch 953, loss 13.35\n",
            "Epoch 954, loss 13.70\n",
            "Epoch 955, loss 14.22\n",
            "Epoch 956, loss 14.21\n",
            "Epoch 957, loss 13.45\n",
            "Epoch 958, loss 12.76\n",
            "Epoch 959, loss 12.94\n",
            "Epoch 960, loss 13.01\n",
            "Epoch 961, loss 12.82\n",
            "Epoch 962, loss 14.06\n",
            "Epoch 963, loss 12.94\n",
            "Epoch 964, loss 13.21\n",
            "Epoch 965, loss 12.63\n",
            "Epoch 966, loss 12.55\n",
            "Epoch 967, loss 12.84\n",
            "Epoch 968, loss 14.74\n",
            "Epoch 969, loss 12.77\n",
            "Epoch 970, loss 12.83\n",
            "Epoch 971, loss 12.66\n",
            "Epoch 972, loss 12.60\n",
            "Epoch 973, loss 12.49\n",
            "Epoch 974, loss 12.51\n",
            "Epoch 975, loss 13.52\n",
            "Epoch 976, loss 12.99\n",
            "Epoch 977, loss 13.74\n",
            "Epoch 978, loss 14.96\n",
            "Epoch 979, loss 13.10\n",
            "Epoch 980, loss 13.80\n",
            "Epoch 981, loss 12.73\n",
            "Epoch 982, loss 12.18\n",
            "Epoch 983, loss 13.22\n",
            "Epoch 984, loss 13.90\n",
            "Epoch 985, loss 13.30\n",
            "Epoch 986, loss 14.28\n",
            "Epoch 987, loss 13.68\n",
            "Epoch 988, loss 12.44\n",
            "Epoch 989, loss 12.82\n",
            "Epoch 990, loss 13.82\n",
            "Epoch 991, loss 13.45\n",
            "Epoch 992, loss 12.80\n",
            "Epoch 993, loss 13.31\n",
            "Epoch 994, loss 13.57\n",
            "Epoch 995, loss 12.47\n",
            "Epoch 996, loss 13.04\n",
            "Epoch 997, loss 13.58\n",
            "Epoch 998, loss 13.39\n",
            "Epoch 999, loss 13.45\n",
            "**** Finished Training ****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoUdDSLu-URD",
        "outputId": "66a967e5-7ec4-4fdb-a86e-87d43917624a"
      },
      "source": [
        "# Compute the model accuracy on the test set\n",
        "class_correct = torch.zeros(10)\n",
        "class_total = torch.zeros(10)\n",
        "model.eval()\n",
        "for (images, labels) in testloader:\n",
        "\n",
        "  for label in labels:\n",
        "    class_total[label] += 1\n",
        "  \n",
        "  outputs = model(images, 0.0001)\n",
        "  for i in range(outputs.shape[0]):\n",
        "    _, prediction = outputs[i].max(0)\n",
        "    if prediction == labels[i]:\n",
        "      class_correct[labels[i]] += 1\n",
        "\n",
        "for i in range(10):\n",
        "    print('Class %d accuracy: %2.2f %%' % (i, 100.0*class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 0 accuracy: 98.06 %\n",
            "Class 1 accuracy: 98.94 %\n",
            "Class 2 accuracy: 94.09 %\n",
            "Class 3 accuracy: 97.92 %\n",
            "Class 4 accuracy: 94.50 %\n",
            "Class 5 accuracy: 93.61 %\n",
            "Class 6 accuracy: 96.03 %\n",
            "Class 7 accuracy: 96.60 %\n",
            "Class 8 accuracy: 94.05 %\n",
            "Class 9 accuracy: 93.46 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUy02Zrc_kH3",
        "outputId": "d8a63bc3-3e08-4cab-d4dc-2e69b86b17ef"
      },
      "source": [
        "max(0.001, math.exp(-3 * math.pow(10, -5) * epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9704746473512195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKlGE6UGFBrJ"
      },
      "source": [
        "def softmax_temperature(logits, temperature=0.001):\n",
        "  pro = F.softmax(logits / temperature, dim=-1)\n",
        "  one_hot_code = pro @ torch.FloatTensor(range(1, pro.shape[1]+1))\n",
        "  return one_hot_code.long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VNIRx08FMkG",
        "outputId": "832abb42-3cd2-40d5-e5b2-444d99fa78cb"
      },
      "source": [
        "logits = torch.FloatTensor([[1,2,4,5],[4,3,2,1]])\n",
        "# softmax_temperature(logits, )\n",
        "F.softmax(logits / 0.001, dim=-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWe-bmJzFPFm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}