{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-demo-benchmark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtcCfLC0EgegyYOzA6TxLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WHU-Peter/COMP6200-Project/blob/main/project_demo_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKDeAnF07V6x"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.optim import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZL2hmRM8TMQ",
        "outputId": "aaa40842-4918-41a3-dd13-cedce172cee1"
      },
      "source": [
        "# flatten 28*28 images to a 784 vector for each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # convert to tensor\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # flatten into vector\n",
        "])\n",
        "\n",
        "trainset = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "testset = MNIST(\".\", train=False, download=True, transform=transform)\n",
        "\n",
        "# create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=True)\n",
        "\n",
        "class_counts = torch.zeros(10, dtype=torch.int32)\n",
        "\n",
        "for (images, labels) in trainloader:\n",
        "  for label in labels:\n",
        "    class_counts[label] += 1\n",
        "\n",
        "assert class_counts.sum()==60000"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLpsbgjy6f14"
      },
      "source": [
        "Baseline model, two linear layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgmENJ6YgR3K"
      },
      "source": [
        "# define baseline model\n",
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "            out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju7wMIq7gYGh"
      },
      "source": [
        "# # build the model \n",
        "# baseline_model = BaselineModel(784, 784, 10).to(device)\n",
        "\n",
        "# # define the loss function and the optimiser\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# optimiser = optim.Adam(baseline_model.parameters())\n",
        "\n",
        "# Loss_Baseline_Model = []\n",
        "# # the epoch loop\n",
        "# for epoch in range(50):\n",
        "#     running_loss = 0.0\n",
        "#     for data in trainloader:\n",
        "#         # get the inputs\n",
        "#         inputs, labels = data\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimiser.zero_grad()\n",
        "\n",
        "#         # forward + loss + backward + optimise (update weights)\n",
        "#         outputs = baseline_model(inputs.to(device))\n",
        "#         loss = loss_function(outputs, labels.to(device))\n",
        "#         loss.backward()\n",
        "#         optimiser.step()\n",
        "\n",
        "#         # keep track of the loss this epoch\n",
        "#         running_loss += loss.item()\n",
        "#     print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
        "#     Loss_Baseline_Model.append(running_loss)\n",
        "# print('**** Finished Training ****')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni0Oh50GTfYX",
        "outputId": "f721a876-3c28-4eb9-da28-5ac89647cf64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build the model \n",
        "baseline_model = BaselineModel(784, 784, 10).to(device)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(baseline_model.parameters())\n",
        "\n",
        "def baseline_model_train(inputs, labels):\n",
        "  baseline_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimiser.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = baseline_model(inputs)\n",
        "  loss = loss_function(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "def baseline_model_inference(inputs):\n",
        "  baseline_model.eval()\n",
        "  outputs = baseline_model(inputs)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_baseline_train = timeit.Timer(\n",
        "    stmt='baseline_model_train(x, y)', \n",
        "    setup='from __main__ import baseline_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_baseline_inference = timeit.Timer(\n",
        "    stmt='baseline_model_inference(x)', \n",
        "    setup='from __main__ import baseline_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'baseline_model_train(x, y):  {t_baseline_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'baseline_model_inference(x):  {t_baseline_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "baseline_model_train(x, y):   13.0 ms\n",
            "baseline_model_inference(x):    2.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sK0zJMA8K25"
      },
      "source": [
        "learnable temperature parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgIQZdemuXW3"
      },
      "source": [
        "class SoftMaxWithTemperature(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SoftMaxWithTemperature, self).__init__()\n",
        "        self.temperature = nn.Parameter(torch.tensor(0.0001))\n",
        "    def forward(self, x):\n",
        "        # print(self.temperature)\n",
        "        return F.softmax(x / self.temperature, dim=-1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv90aYQ_8Yv5"
      },
      "source": [
        "def softmax_temperature(logits, temperature=0.00001):\n",
        "  pro = F.softmax(logits / temperature, dim=-1)\n",
        "  # pro = torch.matmul(pro, torch.FloatTensor(range(0, pro.shape[1])));\n",
        "  return pro;\n",
        "  # return one_hot_code"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqKL-4RvlVWx"
      },
      "source": [
        "# define LUTModel\n",
        "class LUTModelWithLearnableTemperature(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LUTModelWithLearnableTemperature, self).__init__()\n",
        "        self.softmax_temperature = SoftMaxWithTemperature();\n",
        "        self.emb = nn.Embedding(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # if self.training:\n",
        "        #   out = self.softmax_temperature(x)\n",
        "        #   out = out @ self.emb.weight\n",
        "        # else:\n",
        "        #   out = self.softmax_temperature(x)\n",
        "        #   out = mapping_onehot_vector(out).round().long()\n",
        "        #   out = self.emb(out)\n",
        "        \n",
        "        out = self.softmax_temperature(x)\n",
        "        out = out @ self.emb.weight\n",
        "        # out = self.emb(out.long())\n",
        "        # out = self.emb(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "            out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lGu1mINKhAZ"
      },
      "source": [
        "# # build the model \n",
        "# learnable_lut_model = LUTModelWithLearnableTemperature(784, 784, 10).to(device)\n",
        "\n",
        "# # define the loss function and the optimiser\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# optimiser = optim.Adam(learnable_lut_model.parameters(), lr=1e-2)\n",
        "\n",
        "# Loss_Learnable_Model = []\n",
        "# Temperature = []\n",
        "# # the epoch loop\n",
        "# for epoch in range(50):\n",
        "#     running_loss = 0.0\n",
        "#     for data in trainloader:\n",
        "#         # get the inputs\n",
        "#         inputs, labels = data\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimiser.zero_grad()\n",
        "\n",
        "#         # forward + loss + backward + optimise (update weights)\n",
        "#         outputs = learnable_lut_model(inputs.to(device))\n",
        "#         loss = loss_function(outputs, labels.to(device))\n",
        "#         loss.backward()\n",
        "#         optimiser.step()\n",
        "#         # print(optimiser.param_groups[0]['params'][2])\n",
        "#         # print(np.all(optimiser.param_groups[0]['params'][0].grad.numpy() == 0))\n",
        "#         # keep track of the loss this epoch\n",
        "#         running_loss += loss.item()\n",
        "#         # break\n",
        "#     print(optimiser.param_groups[0]['params'][0])\n",
        "#     print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
        "#     Loss_Learnable_Model.append(running_loss)\n",
        "#     Temperature.append(optimiser.param_groups[0]['params'][0].detach().cpu().numpy())\n",
        "# print('**** Finished Training ****')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccNHXUmNS-2H"
      },
      "source": [
        "# # Compute the model accuracy on the test set\n",
        "# class_correct = torch.zeros(10)\n",
        "# class_total = torch.zeros(10)\n",
        "# learnable_lut_model.eval()\n",
        "# for (images, labels) in testloader:\n",
        "\n",
        "#   for label in labels:\n",
        "#     class_total[label] += 1\n",
        "  \n",
        "#   outputs = learnable_lut_model(images.to(device))\n",
        "#   outputs = outputs.to(\"cpu\")\n",
        "#   for i in range(outputs.shape[0]):\n",
        "#     _, prediction = outputs[i].max(0)\n",
        "#     if prediction == labels[i]:\n",
        "#       class_correct[labels[i]] += 1\n",
        "\n",
        "# for i in range(10):\n",
        "#     print('Class %d accuracy: %2.2f %%' % (i, 100.0*class_correct[i] / class_total[i]))\n",
        "# print(\"accuracy: %2.2f %%\" % (100.0*sum(class_correct)/sum(class_total)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSrXXOBEZpBG",
        "outputId": "53f46ab5-a633-441b-dcf2-a1eeddd10038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build the model \n",
        "learnable_lut_model = LUTModelWithLearnableTemperature(784, 784, 10).to(device)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(learnable_lut_model.parameters(), lr=1e-2)\n",
        "\n",
        "def learnable_model_train(inputs, labels):\n",
        "  learnable_lut_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimiser.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = learnable_lut_model(inputs)\n",
        "  loss = loss_function(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "def learnable_model_inference(inputs):\n",
        "  learnable_lut_model.eval()\n",
        "  outputs = learnable_lut_model(inputs)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_learnable_train = timeit.Timer(\n",
        "    stmt='learnable_model_train(x, y)', \n",
        "    setup='from __main__ import learnable_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_learnable_inference = timeit.Timer(\n",
        "    stmt='learnable_model_inference(x)', \n",
        "    setup='from __main__ import learnable_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'learnable_model_train(x, y):  {t_learnable_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'learnable_model_inference(x):  {t_learnable_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learnable_model_train(x, y):   29.8 ms\n",
            "learnable_model_inference(x):   11.6 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdJqT4Fu93Sz"
      },
      "source": [
        "class LUTModelWithAnnealTemperature(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LUTModelWithAnnealTemperature, self).__init__()\n",
        "        self.emb = nn.Embedding(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x, temperature):\n",
        "        # out = self.fc1(x)\n",
        "        # out = F.relu(out)\n",
        "        if self.training:\n",
        "          # print(x.shape)\n",
        "          # print(self.emb.weight.shape)\n",
        "          out = softmax_temperature(x, temperature)\n",
        "          out = out @ self.emb.weight\n",
        "        else:\n",
        "          out = softmax_temperature(x, temperature)\n",
        "          # out = mapping_onehot_vector(out).round().long()\n",
        "          # out = self.emb(out)\n",
        "\n",
        "          nozero = torch.nonzero(out);\n",
        "          # print(np.array(nozero).shape[1])\n",
        "          for i in range(out.shape[0]):\n",
        "            idx = torch.where(nozero[:,0]==i)[0]\n",
        "            rows = nozero[idx, 1].long()\n",
        "            out[i] = torch.mean(self.emb(rows), axis=0)\n",
        "          \n",
        "        # out = self.emb(out.long())\n",
        "        # out = self.emb(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "            out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1StIRCg-CeI"
      },
      "source": [
        "# # build the model \n",
        "# Annealing_lut_model = LUTModelWithAnnealTemperature(784, 784, 10).to(device)\n",
        "# Annealing_lut_model.train()\n",
        "# # define the loss function and the optimiser\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "# learning_rate = 1e-2  #Learning rate\n",
        "# Annealing_lut_optimiser = optim.Adam(Annealing_lut_model.parameters(), lr=learning_rate)\n",
        "# scheduler = lr_scheduler.MultiStepLR(Annealing_lut_optimiser,milestones=[2500],gamma = 0.2)\n",
        "# Loss_Annealing_Model = []\n",
        "# # the epoch loop\n",
        "# i = 0;\n",
        "# for epoch in range(50):\n",
        "#     running_loss = 0.0\n",
        "#     print(\"Epoch %d, lr %4.5f\" % (epoch, Annealing_lut_optimiser.state_dict()['param_groups'][0]['lr']))\n",
        "#     for data in trainloader:\n",
        "#         # get the inputs\n",
        "#         i = i+1\n",
        "#         inputs, labels = data\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         Annealing_lut_optimiser.zero_grad()\n",
        "#         # forward + loss + backward + optimise (update weights)\n",
        "#         outputs = Annealing_lut_model(inputs.to(device), max(0.0001, math.exp(-15 * math.pow(10, -5) * (i))))\n",
        "#         loss = loss_function(outputs, labels.to(device))\n",
        "#         loss.backward()\n",
        "#         Annealing_lut_optimiser.step()\n",
        "#         scheduler.step()\n",
        "#         # print(optimiser.param_groups[0]['params'][2])\n",
        "#         # print(np.all(optimiser.param_groups[0]['params'][0].grad.numpy() == 0))\n",
        "#         # keep track of the loss this epoch\n",
        "#         running_loss += loss.item()\n",
        "#         # break\n",
        "#     print(\"Epoch %d, loss %4.2f\" % (epoch, running_loss))\n",
        "#     print(\"Epoch %d, temperature %4.8f\" % (epoch, max(0.0001, math.exp(-20 * math.pow(10, -5) * (i)))))\n",
        "#     Loss_Annealing_Model.append(running_loss)\n",
        "# print('**** Finished Training ****')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoUdDSLu-URD"
      },
      "source": [
        "# # Compute the model accuracy on the test set\n",
        "# class_correct = torch.zeros(10)\n",
        "# class_total = torch.zeros(10)\n",
        "# Annealing_lut_model.eval()\n",
        "# for (images, labels) in testloader:\n",
        "\n",
        "#   for label in labels:\n",
        "#     class_total[label] += 1\n",
        "#   outputs = Annealing_lut_model(images.to(device), 0.001)\n",
        "#   outputs = outputs.to(\"cpu\")\n",
        "#   for i in range(outputs.shape[0]):\n",
        "#     _, prediction = outputs[i].max(0)\n",
        "#     if prediction == labels[i]:\n",
        "#       class_correct[labels[i]] += 1\n",
        "\n",
        "# for i in range(10):\n",
        "#     print('Class %d accuracy: %2.2f %%' % (i, 100.0*class_correct[i] / class_total[i]))\n",
        "# print(\"accuracy: %2.2f %%\" % (100.0*sum(class_correct)/sum(class_total)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-7Q5YeGaviz",
        "outputId": "e6e3eb9e-f09c-4510-ffd5-9671efb83429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# build the model \n",
        "Annealing_lut_model = LUTModelWithAnnealTemperature(784, 784, 10).to(device)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(Annealing_lut_model.parameters(), lr=1e-2)\n",
        "\n",
        "def annealing_model_train(inputs, labels):\n",
        "  Annealing_lut_model.train()\n",
        "  # zero the parameter gradients\n",
        "  optimiser.zero_grad()\n",
        "  # forward + loss + backward + optimise (update weights)\n",
        "  outputs = Annealing_lut_model(inputs, 0.001)\n",
        "  loss = loss_function(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "def annealing_model_inference(inputs):\n",
        "  Annealing_lut_model.eval()\n",
        "  outputs = Annealing_lut_model(inputs, 0.001)\n",
        "\n",
        "x = 0\n",
        "y = 0\n",
        "for data in testloader:\n",
        "  inputs, labels = data\n",
        "  x = inputs.to(device)\n",
        "  y = labels.to(device)\n",
        "  break\n",
        "\n",
        "t_annealing_train = timeit.Timer(\n",
        "    stmt='annealing_model_train(x, y)', \n",
        "    setup='from __main__ import annealing_model_train',\n",
        "    globals={'x': x, 'y' : y}\n",
        ")\n",
        "\n",
        "t_annealing_inference = timeit.Timer(\n",
        "    stmt='annealing_model_inference(x)', \n",
        "    setup='from __main__ import annealing_model_inference',\n",
        "    globals={'x': x}\n",
        ")\n",
        "\n",
        "print(f'annealing_model_train(x, y):  {t_annealing_train.timeit(100) / 100 * 1e3:>5.1f} ms')\n",
        "print(f'annealing_model_inference(x):  {t_annealing_inference.timeit(100) / 100 * 1e3:>5.1f} ms')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "annealing_model_train(x, y):   18.0 ms\n",
            "annealing_model_inference(x):   26.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}